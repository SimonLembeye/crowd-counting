{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch\n",
    "from torch.nn import functional as F2\n",
    "from PIL import Image\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "from glob import glob\n",
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision import transforms, models\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch import optim\n",
    "from torch.nn import Module\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orginal model of the paper\n",
    "\n",
    "A pretrain vvg19 nn with a downsampling of 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = ['vgg19']\n",
    "model_urls = {\n",
    "    'vgg19': 'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth',\n",
    "}\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "        self.reg_layer = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = F2.upsample_bilinear(x, scale_factor=2)\n",
    "        x = self.reg_layer(x)\n",
    "        return torch.abs(x)\n",
    "\n",
    "\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "cfg = {\n",
    "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512]\n",
    "}\n",
    "\n",
    "def vgg19():\n",
    "    \"\"\"VGG 19-layer model (configuration \"E\")\n",
    "        model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = VGG(make_layers(cfg['E']))\n",
    "    model.load_state_dict(model_zoo.load_url(model_urls['vgg19']), strict=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSRNet \n",
    "Downsampling = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSRNet(nn.Module):\n",
    "    def __init__(self, load_weights=False):\n",
    "        super(CSRNet, self).__init__()\n",
    "        self.seen = 0\n",
    "        self.frontend_feat = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512]\n",
    "        self.backend_feat  = [512, 512, 512,256,128,64]\n",
    "        self.frontend = make_layers(self.frontend_feat)\n",
    "        self.backend = make_layers(self.backend_feat,in_channels = 512,dilation = True)\n",
    "        self.output_layer = nn.Conv2d(64, 1, kernel_size=1)\n",
    "        if not load_weights:\n",
    "            mod = models.vgg16(pretrained = True)\n",
    "            self._initialize_weights()\n",
    "            self.frontend.load_state_dict(mod.features[0:23].state_dict())\n",
    "            \n",
    "    def forward(self,x):\n",
    "        size = x.size()\n",
    "        x = self.frontend(x)\n",
    "        x = self.backend(x)\n",
    "        x = self.output_layer(x)\n",
    "        x = F2.upsample(x, size = size[2:])\n",
    "        return x\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight, std=0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            \n",
    "                \n",
    "def make_layers(cfg, in_channels = 3,batch_norm=False,dilation = False):\n",
    "    if dilation:\n",
    "        d_rate = 2\n",
    "    else:\n",
    "        d_rate = 1\n",
    "    layers = []\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=d_rate,dilation = d_rate)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_crop(im_h, im_w, crop_h, crop_w):\n",
    "    res_h = im_h - crop_h\n",
    "    res_w = im_w - crop_w\n",
    "    i = random.randint(0, res_h)\n",
    "    j = random.randint(0, res_w)\n",
    "    return i, j, crop_h, crop_w\n",
    "\n",
    "\n",
    "def cal_innner_area(c_left, c_up, c_right, c_down, bbox):\n",
    "    inner_left = np.maximum(c_left, bbox[:, 0])\n",
    "    inner_up = np.maximum(c_up, bbox[:, 1])\n",
    "    inner_right = np.minimum(c_right, bbox[:, 2])\n",
    "    inner_down = np.minimum(c_down, bbox[:, 3])\n",
    "    inner_area = np.maximum(inner_right-inner_left, 0.0) * np.maximum(inner_down-inner_up, 0.0)\n",
    "    return inner_area\n",
    "\n",
    "\n",
    "\n",
    "class Crowd(data.Dataset):\n",
    "    def __init__(self, root_path, crop_size,\n",
    "                 downsample_ratio, is_gray=False,\n",
    "                 method='train'):\n",
    "\n",
    "        self.root_path = root_path\n",
    "        self.im_list = sorted(glob(os.path.join(self.root_path, '*.jpg')))\n",
    "        if method not in ['train', 'val']:\n",
    "            raise Exception(\"not implement\")\n",
    "        self.method = method\n",
    "\n",
    "        self.c_size = crop_size\n",
    "        self.d_ratio = downsample_ratio\n",
    "        assert self.c_size % self.d_ratio == 0\n",
    "        self.dc_size = self.c_size // self.d_ratio\n",
    "\n",
    "        if is_gray:\n",
    "            self.trans = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "            ])\n",
    "        else:\n",
    "            self.trans = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # Pour CSRNet à vérifier si besoin de modif\n",
    "            ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.im_list)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        img_path = self.im_list[item]\n",
    "        gd_path = img_path.replace('jpg', 'npy')\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        if self.method == 'train':\n",
    "            keypoints = np.load(gd_path)\n",
    "            return self.train_transform(img, keypoints)\n",
    "        elif self.method == 'val':\n",
    "            keypoints = np.load(gd_path)\n",
    "            img = self.trans(img)\n",
    "            name = os.path.basename(img_path).split('.')[0]\n",
    "            return img, len(keypoints), name\n",
    "\n",
    "    def train_transform(self, img, keypoints):\n",
    "        \"\"\"random crop image patch and find people in it\"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        Les keypoints correspondent aux coordonnées des têtes\n",
    "        MAIS une troisième coordonnée a été calculée lors du preprocessing des données,\n",
    "        elle correspont à \"dis\" et semble important pour calculer pas mal de choses\n",
    "        \"\"\"\n",
    "        \n",
    "        wd, ht = img.size\n",
    "        st_size = min(wd, ht)\n",
    "        assert st_size >= self.c_size\n",
    "        assert len(keypoints) > 0\n",
    "        i, j, h, w = random_crop(ht, wd, self.c_size, self.c_size)\n",
    "        img = F.crop(img, i, j, h, w)\n",
    "        \n",
    "        nearest_dis = np.clip(keypoints[:, 2], 4.0, 128.0)\n",
    "       \n",
    "        points_left_up = keypoints[:, :2] - nearest_dis[:, None] / 2.0\n",
    "        points_right_down = keypoints[:, :2] + nearest_dis[:, None] / 2.0\n",
    "        bbox = np.concatenate((points_left_up, points_right_down), axis=1)\n",
    "        inner_area = cal_innner_area(j, i, j+w, i+h, bbox)\n",
    "        origin_area = nearest_dis * nearest_dis\n",
    "        ratio = np.clip(1.0 * inner_area / origin_area, 0.0, 1.0)\n",
    "        mask = (ratio >= 0.3)\n",
    "\n",
    "        target = ratio[mask]\n",
    "        keypoints = keypoints[mask]\n",
    "        keypoints = keypoints[:, :2] - [j, i]  # change coodinate\n",
    "        if len(keypoints) > 0:\n",
    "            if random.random() > 0.5:\n",
    "                img = F.hflip(img)\n",
    "                keypoints[:, 0] = w - keypoints[:, 0]\n",
    "        else:\n",
    "            if random.random() > 0.5:\n",
    "                img = F.hflip(img)\n",
    "        return self.trans(img), torch.from_numpy(keypoints.copy()).float(), \\\n",
    "               torch.from_numpy(target.copy()).float(), st_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation of p(yn|xm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post_Prob(Module):\n",
    "    def __init__(self, sigma, c_size, stride, background_ratio, use_background, device):\n",
    "        super(Post_Prob, self).__init__()\n",
    "        assert c_size % stride == 0\n",
    "\n",
    "        self.sigma = sigma\n",
    "        self.bg_ratio = background_ratio\n",
    "        self.device = device\n",
    "        # coordinate is same to image space, set to constant since crop size is same\n",
    "        self.cood = torch.arange(0, c_size, step=stride,\n",
    "                                 dtype=torch.float32, device=device) + stride / 2\n",
    "        self.cood.unsqueeze_(0)\n",
    "        self.softmax = torch.nn.Softmax(dim=0)\n",
    "        self.use_bg = use_background\n",
    "\n",
    "    def forward(self, points, st_sizes):\n",
    "        num_points_per_image = [len(points_per_image) for points_per_image in points]\n",
    "        all_points = torch.cat(points, dim=0)\n",
    "\n",
    "        if len(all_points) > 0:\n",
    "            x = all_points[:, 0].unsqueeze_(1)\n",
    "            y = all_points[:, 1].unsqueeze_(1)\n",
    "            x_dis = -2 * torch.matmul(x, self.cood) + x * x + self.cood * self.cood\n",
    "            y_dis = -2 * torch.matmul(y, self.cood) + y * y + self.cood * self.cood\n",
    "            y_dis.unsqueeze_(2)\n",
    "            x_dis.unsqueeze_(1)\n",
    "            dis = y_dis + x_dis\n",
    "            dis = dis.view((dis.size(0), -1))\n",
    "\n",
    "            dis_list = torch.split(dis, num_points_per_image)\n",
    "            prob_list = []\n",
    "            for dis, st_size in zip(dis_list, st_sizes):\n",
    "                if len(dis) > 0:\n",
    "                    if self.use_bg:\n",
    "                        min_dis = torch.clamp(torch.min(dis, dim=0, keepdim=True)[0], min=0.0)\n",
    "                        d = st_size * self.bg_ratio\n",
    "                        bg_dis = (d - torch.sqrt(min_dis))**2\n",
    "                        dis = torch.cat([dis, bg_dis], 0)  # concatenate background distance to the last\n",
    "                    dis = -dis / (2.0 * self.sigma ** 2)\n",
    "                    prob = self.softmax(dis)\n",
    "                else:\n",
    "                    prob = None\n",
    "                prob_list.append(prob)\n",
    "        else:\n",
    "            prob_list = []\n",
    "            for _ in range(len(points)):\n",
    "                prob_list.append(None)\n",
    "        return prob_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Loss definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bay_Loss(Module):\n",
    "    def __init__(self, use_background, device):\n",
    "        super(Bay_Loss, self).__init__()\n",
    "        self.device = device\n",
    "        self.use_bg = use_background\n",
    "\n",
    "    def forward(self, prob_list, target_list, pre_density):\n",
    "        # print(\"problist\", prob_list[0].size())\n",
    "        # print(\"pre dense\", pre_density.size())\n",
    "        # print(\"target_list\", target_list)\n",
    "        loss = 0\n",
    "        \n",
    "        \"\"\"\n",
    "            - prob list semble être la listes des p(yn|xm) ie la contribution du pixel xm sur la n-ieme tête\n",
    "            (les lignes de cette matrice sont de taille 4096 = 64*64)\n",
    "            - pre density est la prédiction de la densité (sortie du réseau) - de taille 64x64 ici\n",
    "            - target list a pour longueur le nombre de têtes - correspond aux E[cn] \"réel\" (le calcul reste un mystère)\n",
    "            - On obtient les E[cn] estimées grâce à un produit terme à terme de prob_list et pre_density\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        for idx, prob in enumerate(prob_list):  # iterative through each sample\n",
    "            if prob is None:  # image contains no annotation points\n",
    "                pre_count = torch.sum(pre_density[idx])\n",
    "                target = torch.zeros((1,), dtype=torch.float32, device=self.device)\n",
    "            else:\n",
    "                N = len(prob)\n",
    "                if self.use_bg:\n",
    "                    target = torch.zeros((N,), dtype=torch.float32, device=self.device)\n",
    "                    target[:-1] = target_list[idx]\n",
    "                else:\n",
    "                    target = target_list[idx]\n",
    "                pre_count = torch.sum(pre_density[idx].view((1, -1)) * prob, dim=1)  # flatten into vector\n",
    "\n",
    "            loss += torch.sum(torch.abs(target - pre_count))\n",
    "        loss = loss / len(prob_list)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = 1.0 * self.sum / self.count\n",
    "\n",
    "    def get_avg(self):\n",
    "        return self.avg\n",
    "\n",
    "    def get_count(self):\n",
    "        return self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using gpu: False \n"
     ]
    }
   ],
   "source": [
    "downsample_ratio = 1 # Mettre à 8 pour le réseau du répo (à 1 pour CSRNet puisque on ne modifie pas la dim avec le réseau)\n",
    "# data_dir = \"/home/simon/Bureau/framework-crowd-counting/processed_data_bcc/UCF\"\n",
    "data_dir = \"/home/simon/Bureau/framework-crowd-counting/processed_data_bcc/SHHA\"\n",
    "crop_size = 512\n",
    "is_gray = False\n",
    "num_workers = 1\n",
    "batch_size = 2\n",
    "\n",
    "def train_collate(batch):\n",
    "    transposed_batch = list(zip(*batch))\n",
    "    # print(transposed_batch)\n",
    "    images = torch.stack(transposed_batch[0], 0)\n",
    "    points = transposed_batch[1]  # the number of points is not fixed, keep it as a list of tensor\n",
    "    targets = transposed_batch[2]\n",
    "    st_sizes = torch.FloatTensor(transposed_batch[3])\n",
    "    return images, points, targets, st_sizes\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Using gpu: %s ' % torch.cuda.is_available())\n",
    "\n",
    "datasets = {x: Crowd(os.path.join(data_dir, x),\n",
    "                          crop_size,\n",
    "                          downsample_ratio,\n",
    "                          is_gray, x) for x in ['train', 'val']}\n",
    "\n",
    "dataloaders = {x: DataLoader(datasets[x],\n",
    "                            collate_fn=(train_collate if x == 'train' else default_collate),\n",
    "                            batch_size=(batch_size if x == 'train' else 1),\n",
    "                            shuffle=(True if x == 'train' else False),\n",
    "                            num_workers=num_workers,\n",
    "                            pin_memory=(True if x == 'train' else False))\n",
    "                            for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model / Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.00001\n",
    "weight_decay = 0.9\n",
    "sigma = 0.1\n",
    "use_background = False\n",
    "background_ratio = 1\n",
    "\n",
    "model = CSRNet()\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "post_prob = Post_Prob(sigma,\n",
    "                           crop_size,\n",
    "                           downsample_ratio,\n",
    "                           background_ratio,\n",
    "                           use_background,\n",
    "                           device)\n",
    "criterion = Bay_Loss(use_background, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "max_epoch = 10\n",
    "val_epoch = 2\n",
    "val_start = 0\n",
    "\n",
    "def train():\n",
    "    for epoch in range(start_epoch, max_epoch):\n",
    "        print('-'*5 + 'Epoch {}/{}'.format(epoch, max_epoch - 1) + '-'*5)\n",
    "        train_epoch(epoch)\n",
    "        if epoch % val_epoch == 0 and epoch >= val_start:\n",
    "            val_epoch(epoch)\n",
    "\n",
    "def train_epoch(epoch):\n",
    "    epoch_loss = AverageMeter()\n",
    "    epoch_mae = AverageMeter()\n",
    "    epoch_mse = AverageMeter()\n",
    "    epoch_start = time.time()\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    # Iterate over data.\n",
    "    for step, (inputs, points, targets, st_sizes) in enumerate(dataloaders['train']):\n",
    "        inputs = inputs.to(device)\n",
    "        st_sizes = st_sizes.to(device)\n",
    "        gd_count = np.array([len(p) for p in points], dtype=np.float32)\n",
    "        points = [p.to(device) for p in points]\n",
    "        targets = [t.to(device) for t in targets]\n",
    "        \n",
    "        # print(inputs.size())\n",
    "        # print(gd_count)\n",
    "        # print(points)\n",
    "        # print(targets)\n",
    "        # print(st_sizes)\n",
    "        \n",
    "        # inputs = image size(m, 3, crop_size, crop_size)\n",
    "        # gd_counts = [nb_of_head_img1, num_of_head_img2, ...]\n",
    "        # point = array of positions\n",
    "        # targets = ?\n",
    "        \n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            outputs = model(inputs)\n",
    "            prob_list = post_prob(points, st_sizes)\n",
    "            loss = criterion(prob_list, targets, outputs)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "            N = inputs.size(0) # batch size\n",
    "            pre_count = torch.sum(outputs.view(N, -1), dim=1).detach().cpu().numpy()\n",
    "            res = pre_count - gd_count\n",
    "            \n",
    "            print(f'step: {step} | gd_count: {gd_count} | prediction: {pre_count} | loss: {loss}')\n",
    "            \n",
    "            \n",
    "            # epoch_loss.update(loss.item(), N)\n",
    "            # epoch_mse.update(np.mean(res * res), N)\n",
    "            # epoch_mae.update(np.mean(abs(res)), N)\n",
    "\n",
    "    # print('Epoch {} Train, Loss: {:.2f}, MSE: {:.2f} MAE: {:.2f}, Cost {:.1f} sec'\n",
    "    #             .format(epoch, epoch_loss.get_avg(), np.sqrt(epoch_mse.get_avg()), epoch_mae.get_avg(),\n",
    "    #                     time.time()-epoch_start))\n",
    "\n",
    "def val_epoch(epoch):\n",
    "    epoch_start = time.time()\n",
    "    self.model.eval()  # Set model to evaluate mode\n",
    "    epoch_res = []\n",
    "    # Iterate over data.\n",
    "    for inputs, count, name in self.dataloaders['val']:\n",
    "        inputs = inputs.to(self.device)\n",
    "        # inputs are images with different sizes\n",
    "        assert inputs.size(0) == 1, 'the batch size should equal to 1 in validation mode'\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = self.model(inputs)\n",
    "            res = count[0].item() - torch.sum(outputs).item()\n",
    "            epoch_res.append(res)\n",
    "\n",
    "    epoch_res = np.array(epoch_res)\n",
    "    mse = np.sqrt(np.mean(np.square(epoch_res)))\n",
    "    mae = np.mean(np.abs(epoch_res))\n",
    "    logging.info('Epoch {} Val, MSE: {:.2f} MAE: {:.2f}, Cost {:.1f} sec'\n",
    "                 .format(epoch, mse, mae, time.time()-epoch_start))\n",
    "\n",
    "    model_state_dic = self.model.state_dict()\n",
    "    if (2.0 * mse + mae) < (2.0 * self.best_mse + self.best_mae):\n",
    "        self.best_mse = mse\n",
    "        self.best_mae = mae\n",
    "        logging.info(\"save best mse {:.2f} mae {:.2f} model epoch {}\".format(self.best_mse,\n",
    "                                                                             self.best_mae,\n",
    "                                                                             epoch))\n",
    "        torch.save(model_state_dic, os.path.join(self.save_dir, 'best_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Epoch 0/9-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simon/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:2404: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 | gd_count: [418. 174.] | prediction: [-13.321764 -21.62857 ] | loss: 306.58087158203125\n",
      "step: 1 | gd_count: [106. 200.] | prediction: [140.86218 156.17812] | loss: 91.14297485351562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/simon/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/simon/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/simon/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/simon/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-131-754274ad8cf2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m5\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'Epoch {}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epoch\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mval_epoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mval_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-131-754274ad8cf2>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
