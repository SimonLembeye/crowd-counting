{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import models,transforms,datasets\n",
    "from torch.utils import data\n",
    "%matplotlib inline\n",
    "import torchvision.transforms as standard_transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from PIL import Image, ImageOps, ImageFilter\n",
    "import numbers\n",
    "import pdb\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as F2\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from glob import glob\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch import optim\n",
    "from torch.nn import Module\n",
    "import time\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code can be run both on laptop and on GCP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using gpu: False \n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Using gpu: %s ' % torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "We use 2 nn, a CSRNet and a VGG19 extented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG19 extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = ['vgg19']\n",
    "model_urls = {\n",
    "    'vgg19': 'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth',\n",
    "}\n",
    "\n",
    "class VGGExtended(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super(VGGExtended, self).__init__()\n",
    "        self.features = features\n",
    "        self.reg_layer = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = F.upsample_bilinear(x, scale_factor=2)\n",
    "        x = self.reg_layer(x)\n",
    "        return torch.abs(x)\n",
    "\n",
    "\n",
    "def make_layers(cfg, in_channels = 3,batch_norm=False,dilation = False):\n",
    "    if dilation:\n",
    "        d_rate = 2\n",
    "    else:\n",
    "        d_rate = 1\n",
    "    layers = []\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=d_rate,dilation = d_rate)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "cfg = {\n",
    "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512]\n",
    "}\n",
    "\n",
    "def vgg19():\n",
    "    \"\"\"VGG 19-layer model (configuration \"E\")\n",
    "        model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = VGGExtended(make_layers(cfg['E']))\n",
    "    model.load_state_dict(model_zoo.load_url(model_urls['vgg19']), strict=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSRNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSRNet(nn.Module):\n",
    "    def __init__(self, load_weights=False):\n",
    "        super(CSRNet, self).__init__()\n",
    "        self.seen = 0\n",
    "        self.frontend_feat = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512]\n",
    "        self.backend_feat  = [512, 512, 512,256,128,64]\n",
    "        self.frontend = make_layers(self.frontend_feat)\n",
    "        self.backend = make_layers(self.backend_feat,in_channels = 512,dilation = True)\n",
    "        self.output_layer = nn.Conv2d(64, 1, kernel_size=1)\n",
    "        if not load_weights:\n",
    "            mod = models.vgg16(pretrained = True)\n",
    "            self._initialize_weights()\n",
    "            self.frontend.load_state_dict(mod.features[0:23].state_dict())\n",
    "            \n",
    "    def forward(self,x):\n",
    "        size = x.size()\n",
    "        x = self.frontend(x)\n",
    "        x = self.backend(x)\n",
    "        x = self.output_layer(x)\n",
    "        x = F.upsample(x, size = size[2:])\n",
    "        return x\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight, std=0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_cropBayes(im_h, im_w, crop_h, crop_w):\n",
    "    res_h = im_h - crop_h\n",
    "    res_w = im_w - crop_w\n",
    "    i = random.randint(0, res_h)\n",
    "    j = random.randint(0, res_w)\n",
    "    return i, j, crop_h, crop_w\n",
    "\n",
    "\n",
    "def cal_innner_area(c_left, c_up, c_right, c_down, bbox):\n",
    "    inner_left = np.maximum(c_left, bbox[:, 0])\n",
    "    inner_up = np.maximum(c_up, bbox[:, 1])\n",
    "    inner_right = np.minimum(c_right, bbox[:, 2])\n",
    "    inner_down = np.minimum(c_down, bbox[:, 3])\n",
    "    inner_area = np.maximum(inner_right-inner_left, 0.0) * np.maximum(inner_down-inner_up, 0.0)\n",
    "    return inner_area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ground Truth dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTDataset(data.Dataset):\n",
    "    def __init__(self, data_path, mode, main_transform=None, img_transform=None, gt_transform=None):\n",
    "        self.img_path = data_path + '/img'\n",
    "        self.gt_path = data_path + '/den'\n",
    "        self.data_files = [filename for filename in os.listdir(self.img_path) \\\n",
    "                           if os.path.isfile(os.path.join(self.img_path,filename))]\n",
    "        self.num_samples = len(self.data_files) \n",
    "        self.main_transform=main_transform  \n",
    "        self.img_transform = img_transform\n",
    "        self.gt_transform = gt_transform     \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        fname = self.data_files[index]\n",
    "        img, den = self.read_image_and_gt(fname)      \n",
    "        if self.main_transform is not None:\n",
    "            img, den = self.main_transform(img,den) \n",
    "        if self.img_transform is not None:\n",
    "            img = self.img_transform(img)         \n",
    "        if self.gt_transform is not None:\n",
    "            den = self.gt_transform(den)               \n",
    "        return img, den\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def read_image_and_gt(self,fname):\n",
    "        img = Image.open(os.path.join(self.img_path,fname))\n",
    "        if img.mode == 'L':\n",
    "            img = img.convert('RGB')\n",
    "\n",
    "        den = pd.read_csv(os.path.join(self.gt_path,os.path.splitext(fname)[0] + '.csv'), sep=',',header=None).values\n",
    "        \n",
    "        den = den.astype(np.float32, copy=False)    \n",
    "        den = Image.fromarray(den)  \n",
    "        return img, den    \n",
    "\n",
    "    def get_num_samples(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayes method Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesDataset(data.Dataset):\n",
    "    def __init__(self, root_path, crop_size,\n",
    "                 downsample_ratio, is_gray=False,\n",
    "                 method='train'):\n",
    "\n",
    "        self.root_path = root_path\n",
    "        self.im_list = sorted(glob(os.path.join(self.root_path, '*.jpg')))\n",
    "        if method not in ['train', 'val']:\n",
    "            raise Exception(\"not implement\")\n",
    "        self.method = method\n",
    "\n",
    "        self.c_size = crop_size\n",
    "        self.d_ratio = downsample_ratio\n",
    "        assert self.c_size % self.d_ratio == 0\n",
    "        self.dc_size = self.c_size // self.d_ratio\n",
    "\n",
    "        if is_gray:\n",
    "            self.trans = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "            ])\n",
    "        else:\n",
    "            self.trans = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # Pour CSRNet à vérifier si besoin de modif\n",
    "            ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.im_list)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        img_path = self.im_list[item]\n",
    "        gd_path = img_path.replace('jpg', 'npy')\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        if self.method == 'train':\n",
    "            keypoints = np.load(gd_path)\n",
    "            return self.train_transform(img, keypoints)\n",
    "        elif self.method == 'val':\n",
    "            keypoints = np.load(gd_path)\n",
    "            img = self.trans(img)\n",
    "            name = os.path.basename(img_path).split('.')[0]\n",
    "            return img, len(keypoints), name\n",
    "\n",
    "    def train_transform(self, img, keypoints):\n",
    "        \"\"\"random crop image patch and find people in it\"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        Les keypoints correspondent aux coordonnées des têtes\n",
    "        MAIS une troisième coordonnée a été calculée lors du preprocessing des données,\n",
    "        elle correspont à \"dis\" et semble important pour calculer pas mal de choses\n",
    "        \"\"\"\n",
    "        \n",
    "        wd, ht = img.size\n",
    "        st_size = min(wd, ht)\n",
    "        assert st_size >= self.c_size\n",
    "        assert len(keypoints) > 0\n",
    "        i, j, h, w = random_cropBayes(ht, wd, self.c_size, self.c_size)\n",
    "        img = F2.crop(img, i, j, h, w)\n",
    "        \n",
    "        nearest_dis = np.clip(keypoints[:, 2], 4.0, 128.0)\n",
    "       \n",
    "        points_left_up = keypoints[:, :2] - nearest_dis[:, None] / 2.0\n",
    "        points_right_down = keypoints[:, :2] + nearest_dis[:, None] / 2.0\n",
    "        bbox = np.concatenate((points_left_up, points_right_down), axis=1)\n",
    "        inner_area = cal_innner_area(j, i, j+w, i+h, bbox)\n",
    "        origin_area = nearest_dis * nearest_dis\n",
    "        ratio = np.clip(1.0 * inner_area / origin_area, 0.0, 1.0)\n",
    "        mask = (ratio >= 0.3)\n",
    "\n",
    "        target = ratio[mask]\n",
    "        keypoints = keypoints[mask]\n",
    "        keypoints = keypoints[:, :2] - [j, i]  # change coodinate\n",
    "        if len(keypoints) > 0:\n",
    "            if random.random() > 0.5:\n",
    "                img = F2.hflip(img)\n",
    "                keypoints[:, 0] = w - keypoints[:, 0]\n",
    "        else:\n",
    "            if random.random() > 0.5:\n",
    "                img = F2.hflip(img)\n",
    "        return self.trans(img), torch.from_numpy(keypoints.copy()).float(), \\\n",
    "               torch.from_numpy(target.copy()).float(), st_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CSRNet\n",
    "TRAIN_SIZE = (576,768)\n",
    "LABEL_FACTOR = 1\n",
    "TRAIN_BATCH_SIZE = 1\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "def get_min_size(batch):\n",
    "\n",
    "    min_ht = TRAIN_SIZE[0]\n",
    "    min_wd = TRAIN_SIZE[1]\n",
    "\n",
    "    for i_sample in batch:\n",
    "        \n",
    "        _,ht,wd = i_sample.shape\n",
    "        if ht<min_ht:\n",
    "            min_ht = ht\n",
    "        if wd<min_wd:\n",
    "            min_wd = wd\n",
    "    return min_ht,min_wd\n",
    "\n",
    "def random_crop_GT(img,den,dst_size):\n",
    "    # dst_size: ht, wd\n",
    "\n",
    "    _,ts_hd,ts_wd = img.shape\n",
    "\n",
    "    x1 = random.randint(0, ts_wd - dst_size[1])//LABEL_FACTOR*LABEL_FACTOR\n",
    "    y1 = random.randint(0, ts_hd - dst_size[0])//LABEL_FACTOR*LABEL_FACTOR\n",
    "    x2 = x1 + dst_size[1]\n",
    "    y2 = y1 + dst_size[0]\n",
    "\n",
    "    label_x1 = x1//LABEL_FACTOR\n",
    "    label_y1 = y1//LABEL_FACTOR\n",
    "    label_x2 = x2//LABEL_FACTOR\n",
    "    label_y2 = y2//LABEL_FACTOR\n",
    "\n",
    "    return img[:,y1:y2,x1:x2], den[label_y1:label_y2,label_x1:label_x2]\n",
    "\n",
    "\n",
    "def GT_collate(batch):\n",
    "    # @GJY \n",
    "    r\"\"\"Puts each data field into a tensor with outer dimension batch size\"\"\"\n",
    "\n",
    "    transposed = list(zip(*batch)) # imgs and dens\n",
    "    imgs, dens = [transposed[0],transposed[1]]\n",
    "\n",
    "\n",
    "    error_msg = \"batch must contain tensors; found {}\"\n",
    "    if isinstance(imgs[0], torch.Tensor) and isinstance(dens[0], torch.Tensor):\n",
    "        \n",
    "        min_ht, min_wd = get_min_size(imgs)\n",
    "\n",
    "        # print min_ht, min_wd\n",
    "\n",
    "        # pdb.set_trace()\n",
    "        \n",
    "        cropped_imgs = []\n",
    "        cropped_dens = []\n",
    "        for i_sample in range(len(batch)):\n",
    "            _img, _den = random_crop_GT(imgs[i_sample],dens[i_sample],[min_ht,min_wd])\n",
    "            cropped_imgs.append(_img)\n",
    "            cropped_dens.append(_den)\n",
    "\n",
    "\n",
    "        cropped_imgs = torch.stack(cropped_imgs, 0, out=share_memory(cropped_imgs))\n",
    "        cropped_dens = torch.stack(cropped_dens, 0, out=share_memory(cropped_dens))\n",
    "\n",
    "        return [cropped_imgs,cropped_dens]\n",
    "\n",
    "    raise TypeError((error_msg.format(type(batch[0]))))\n",
    "\n",
    "\n",
    "def loading_data_GT():\n",
    "    mean_std = ([0.410824894905, 0.370634973049, 0.359682112932], [0.278580576181, 0.26925137639, 0.27156367898])\n",
    "    log_para = 100.\n",
    "    factor = 1\n",
    "    TRAIN_BATCH_SIZE = 1\n",
    "    DATA_PATH = \"/home/simon/Bureau/framework-crowd-counting/ProcessedData/shanghaitech_part_A\"\n",
    "    VAL_BATCH_SIZE = 3\n",
    "    \n",
    "    \n",
    "    train_main_transform = Compose([\n",
    "        RandomHorizontallyFlip()\n",
    "    ])\n",
    "    img_transform = standard_transforms.Compose([\n",
    "        standard_transforms.ToTensor(),\n",
    "        standard_transforms.Normalize(*mean_std)\n",
    "    ])\n",
    "    gt_transform = standard_transforms.Compose([\n",
    "        GTScaleDown(factor),\n",
    "        LabelNormalize(log_para)\n",
    "    ])\n",
    "\n",
    "    train_set = GTDataset(DATA_PATH+'/train', 'train',main_transform=train_main_transform, img_transform=img_transform, gt_transform=gt_transform)\n",
    "    train_loader =None\n",
    "    if TRAIN_BATCH_SIZE==1:\n",
    "        train_loader = DataLoader(train_set, batch_size=1, shuffle=True, drop_last=True)\n",
    "    elif TRAIN_BATCH_SIZE>1:\n",
    "        train_loader = DataLoader(train_set, batch_size=TRAIN_BATCH_SIZE, num_workers=NUM_WORKERS, collate_fn=GT_collate, shuffle=True, drop_last=True)\n",
    "    \n",
    "    \n",
    "    # il va falloir nous faire un val pour les données GT.\n",
    "    # val_set = GTDataset(DATA_PATH+'/val', 'val', main_transform=None, img_transform=img_transform, gt_transform=gt_transform)\n",
    "    # val_loader = DataLoader(val_set, batch_size=VAL_BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True, drop_last=False)\n",
    "    val_loader = None\n",
    "    \n",
    "    test_set = GTDataset(DATA_PATH+'/test', 'test', main_transform=None, img_transform=img_transform, gt_transform=gt_transform)\n",
    "    test_loader = DataLoader(test_set, batch_size=VAL_BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True, drop_last=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bayes\n",
    "downsample_ratio = 8 # Mettre à 8 pour le réseau du répo (à 1 pour CSRNet puisque on ne modifie pas la dim avec le réseau)\n",
    "data_dir = \"SHHA\"\n",
    "data_dir = \"/home/simon/Bureau/framework-crowd-counting/processed_data_bcc/SHHA\"\n",
    "#data_dir = \"/Users/VictoRambaud/dev/crowd_counting2/ProcessedData/SHHA\"\n",
    "crop_size = 256\n",
    "is_gray = False\n",
    "num_workers = 8\n",
    "batch_size = 4\n",
    "\n",
    "def train_collate(batch):\n",
    "    transposed_batch = list(zip(*batch))\n",
    "    # print(transposed_batch)\n",
    "    images = torch.stack(transposed_batch[0], 0)\n",
    "    points = transposed_batch[1]  # the number of points is not fixed, keep it as a list of tensor\n",
    "    targets = transposed_batch[2]\n",
    "    st_sizes = torch.FloatTensor(transposed_batch[3])\n",
    "    return images, points, targets, st_sizes\n",
    "\n",
    "\n",
    "def loading_data_Bayes():\n",
    "    datasets_bayes = {x: BayesDataset(os.path.join(data_dir, x),\n",
    "                              crop_size,\n",
    "                              downsample_ratio,\n",
    "                              is_gray, x) for x in ['train', 'val']}\n",
    "\n",
    "    dataloaders_bayes = {x: DataLoader(datasets_bayes[x],\n",
    "                                collate_fn=(train_collate if x == 'train' else default_collate),\n",
    "                                batch_size=(batch_size if x == 'train' else 1),\n",
    "                                shuffle=(True if x == 'train' else False),\n",
    "                                num_workers=num_workers,\n",
    "                                pin_memory=(True if x == 'train' else False))\n",
    "                                for x in ['train', 'val']}\n",
    "    \n",
    "    dataloaders_bayes_test = \"To do\"\n",
    "    \n",
    "    return dataloaders_bayes[\"train\"], dataloaders_bayes[\"val\"], dataloaders_bayes_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x7f531c4ded30>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7f531c5020f0>,\n",
       " 'To do')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loading_data_Bayes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes : computing losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post_Prob(Module):\n",
    "    def __init__(self, sigma, c_size, stride, background_ratio, use_background, device):\n",
    "        super(Post_Prob, self).__init__()\n",
    "        assert c_size % stride == 0\n",
    "\n",
    "        self.sigma = sigma\n",
    "        self.bg_ratio = background_ratio\n",
    "        self.device = device\n",
    "        # coordinate is same to image space, set to constant since crop size is same\n",
    "        self.cood = torch.arange(0, c_size, step=stride,\n",
    "                                 dtype=torch.float32, device=device) + stride / 2\n",
    "        self.cood.unsqueeze_(0)\n",
    "        self.softmax = torch.nn.Softmax(dim=0)\n",
    "        self.use_bg = use_background\n",
    "\n",
    "    def forward(self, points, st_sizes):\n",
    "        num_points_per_image = [len(points_per_image) for points_per_image in points]\n",
    "        all_points = torch.cat(points, dim=0)\n",
    "\n",
    "        if len(all_points) > 0:\n",
    "            x = all_points[:, 0].unsqueeze_(1)\n",
    "            y = all_points[:, 1].unsqueeze_(1)\n",
    "            x_dis = -2 * torch.matmul(x, self.cood) + x * x + self.cood * self.cood\n",
    "            y_dis = -2 * torch.matmul(y, self.cood) + y * y + self.cood * self.cood\n",
    "            y_dis.unsqueeze_(2)\n",
    "            x_dis.unsqueeze_(1)\n",
    "            dis = y_dis + x_dis\n",
    "            dis = dis.view((dis.size(0), -1))\n",
    "\n",
    "            dis_list = torch.split(dis, num_points_per_image)\n",
    "            prob_list = []\n",
    "            for dis, st_size in zip(dis_list, st_sizes):\n",
    "                if len(dis) > 0:\n",
    "                    if self.use_bg:\n",
    "                        min_dis = torch.clamp(torch.min(dis, dim=0, keepdim=True)[0], min=0.0)\n",
    "                        d = st_size * self.bg_ratio\n",
    "                        bg_dis = (d - torch.sqrt(min_dis))**2\n",
    "                        dis = torch.cat([dis, bg_dis], 0)  # concatenate background distance to the last\n",
    "                    dis = -dis / (2.0 * self.sigma ** 2)\n",
    "                    prob = self.softmax(dis)\n",
    "                else:\n",
    "                    prob = None\n",
    "                prob_list.append(prob)\n",
    "        else:\n",
    "            prob_list = []\n",
    "            for _ in range(len(points)):\n",
    "                prob_list.append(None)\n",
    "        return prob_list\n",
    "    \n",
    "    \n",
    "class Bay_Loss(Module):\n",
    "    def __init__(self, use_background, device):\n",
    "        super(Bay_Loss, self).__init__()\n",
    "        self.device = device\n",
    "        self.use_bg = use_background\n",
    "\n",
    "    def forward(self, prob_list, target_list, pre_density):\n",
    "        loss = 0\n",
    "        \n",
    "        \"\"\"\n",
    "            - prob list semble être la listes des p(yn|xm) ie la contribution du pixel xm sur la n-ieme tête\n",
    "            (les lignes de cette matrice sont de taille 4096 = 64*64)\n",
    "            - pre density est la prédiction de la densité (sortie du réseau) - de taille 64x64 ici\n",
    "            - target list a pour longueur le nombre de têtes - correspond aux E[cn] \"réel\" (le calcul reste un mystère)\n",
    "            - On obtient les E[cn] estimées grâce à un produit terme à terme de prob_list et pre_density\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        for idx, prob in enumerate(prob_list):  # iterative through each sample\n",
    "            if prob is None:  # image contains no annotation points\n",
    "                pre_count = torch.sum(pre_density[idx])\n",
    "                target = torch.zeros((1,), dtype=torch.float32, device=self.device)\n",
    "            else:\n",
    "                N = len(prob)\n",
    "                if self.use_bg:\n",
    "                    target = torch.zeros((N,), dtype=torch.float32, device=self.device)\n",
    "                    target[:-1] = target_list[idx]\n",
    "                else:\n",
    "                    target = target_list[idx]\n",
    "                pre_count = torch.sum(pre_density[idx].view((1, -1)) * prob, dim=1)  # flatten into vector\n",
    "            print(target.shape)\n",
    "            print(pre_count.shape)\n",
    "            \n",
    "            loss += torch.sum(torch.abs(target - pre_count))\n",
    "        loss = loss / len(prob_list)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this one should work for both architectures\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = 1.0 * self.sum / self.count\n",
    "\n",
    "    def get_avg(self):\n",
    "        return self.avg\n",
    "\n",
    "    def get_count(self):\n",
    "        return self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================img tranforms============================\n",
    "\n",
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, mask, bbx=None):\n",
    "        if bbx is None:\n",
    "            for t in self.transforms:\n",
    "                img, mask = t(img, mask)\n",
    "            return img, mask\n",
    "        for t in self.transforms:\n",
    "            img, mask, bbx = t(img, mask, bbx)\n",
    "        return img, mask, bbx\n",
    "\n",
    "class RandomHorizontallyFlip(object):\n",
    "    def __call__(self, img, mask, bbx=None):\n",
    "        if random.random() < 0.5:\n",
    "            if bbx is None:\n",
    "                return img.transpose(Image.FLIP_LEFT_RIGHT), mask.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            w, h = img.size\n",
    "            xmin = w - bbx[:,3]\n",
    "            xmax = w - bbx[:,1]\n",
    "            bbx[:,1] = xmin\n",
    "            bbx[:,3] = xmax\n",
    "            return img.transpose(Image.FLIP_LEFT_RIGHT), mask.transpose(Image.FLIP_LEFT_RIGHT), bbx\n",
    "        if bbx is None:\n",
    "            return img, mask\n",
    "        return img, mask, bbx\n",
    "\n",
    "\n",
    "\n",
    "# ===============================label tranforms============================\n",
    "\n",
    "class DeNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "        return tensor\n",
    "\n",
    "\n",
    "class LabelNormalize(object):\n",
    "    def __init__(self, para):\n",
    "        self.para = para\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        # tensor = 1./(tensor+self.para).log()\n",
    "        tensor = torch.from_numpy(np.array(tensor))\n",
    "        tensor = tensor*self.para\n",
    "        return tensor\n",
    "\n",
    "    \n",
    "class GTScaleDown(object):\n",
    "    def __init__(self, factor=8):\n",
    "        self.factor = factor\n",
    "\n",
    "    def __call__(self, img):\n",
    "        w, h = img.size\n",
    "        if self.factor==1:\n",
    "            return img\n",
    "        tmp = np.array(img.resize((w//self.factor, h//self.factor), Image.BICUBIC))*self.factor*self.factor\n",
    "        img = Image.fromarray(tmp)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer CSRNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCH = 200\n",
    "PRINT_FREQ = 1\n",
    "LOG_PARA = 100. # C'est quoi ce LOG_PARA ??\n",
    "\n",
    "seed = 1\n",
    "\n",
    "\n",
    "class Trainer_GT():\n",
    "    def __init__(self, dataloader, net, loss, optimizer):\n",
    "        self.train_loader, self.val_loader, self.test_loader = dataloader()\n",
    "        self.net = net\n",
    "        self.loss = loss \n",
    "        self.optimizer = optimizer\n",
    "        self.train_record = {'best_mae': 1e20, 'best_mse':1e20, 'best_model_name': ''}\n",
    "        self.epoch = 0\n",
    "\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epoch, MAX_EPOCH):\n",
    "            self.epoch = epoch\n",
    "            # si on veut un lr sheduler il faut le mettre là\n",
    "                \n",
    "            # training    \n",
    "            self.train_epoch()\n",
    "\n",
    "            # validation\n",
    "            if epoch%VAL_FREQ==0 or epoch>VAL_DENSE_START:\n",
    "                self.validate()\n",
    "                torch.save(self.net.state_dict(), \"my_model_weigths_GT.pth\")\n",
    "\n",
    "\n",
    "    def train_epoch(self): # training for all datasets\n",
    "        self.net.train()\n",
    "        \n",
    "        for i, data in enumerate(self.train_loader, 0):\n",
    "            img, gt_map = data\n",
    "            img = Variable(img).to(device)\n",
    "            gt_map = Variable(gt_map).to(device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            # pred_map = self.net(img, gt_map)\n",
    "            \n",
    "            pred_density_map = self.net(img) \n",
    "            loss = self.loss(pred_density_map, gt_map)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if (i + 1) % PRINT_FREQ == 0:\n",
    "                # self.writer.add_scalar('train_loss', loss.item(), self.i_tb)\n",
    "                print( '[ep %d][it %d][loss %.4f][lr %.4f]' % \\\n",
    "                        (self.epoch + 1, i + 1, loss.item(), self.optimizer.param_groups[0]['lr']*10000) )\n",
    "                print( '        [cnt: gt: %.1f pred: %.2f]' % (gt_map[0].sum().data/LOG_PARA, pred_density_map[0].sum().data/LOG_PARA) )   \n",
    "\n",
    "\n",
    "    def validate(self):\n",
    "\n",
    "        self.net.eval()\n",
    "        \n",
    "        losses = AverageMeter()\n",
    "        maes = AverageMeter()\n",
    "        mses = AverageMeter()\n",
    "\n",
    "        for vi, data in enumerate(self.val_loader, 0):\n",
    "            img, gt_map = data\n",
    "\n",
    "            with torch.no_grad():\n",
    "                img = Variable(img).to(device)\n",
    "                gt_map = Variable(gt_map).to(device)\n",
    "\n",
    "                pred_map = self.net.forward(img,gt_map)\n",
    "\n",
    "                pred_map = pred_map.data.cpu().numpy()\n",
    "                gt_map = gt_map.data.cpu().numpy()\n",
    "\n",
    "                for i_img in range(pred_map.shape[0]):\n",
    "                \n",
    "                    pred_cnt = np.sum(pred_map[i_img])/LOG_PARA\n",
    "                    gt_count = np.sum(gt_map[i_img])/LOG_PARA\n",
    "\n",
    "                    \n",
    "                    losses.update(self.net.loss.item())\n",
    "                    maes.update(abs(gt_count-pred_cnt))\n",
    "                    mses.update((gt_count-pred_cnt)*(gt_count-pred_cnt))\n",
    "            \n",
    "        mae = maes.avg\n",
    "        mse = np.sqrt(mses.avg)\n",
    "        loss = losses.avg\n",
    "\n",
    "        self.writer.add_scalar('val_loss', loss, self.epoch + 1)\n",
    "        self.writer.add_scalar('mae', mae, self.epoch + 1)\n",
    "        self.writer.add_scalar('mse', mse, self.epoch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simon/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:2404: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/home/simon/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([1, 598, 1024])) that is different to the input size (torch.Size([1, 1, 598, 1024])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ep 1][it 1][loss 0.0680][lr 0.1000]\n",
      "        [cnt: gt: 624.0 pred: 0.31]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-0979bfe9007a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mgt_trainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer_GT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloading_data_GT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mgt_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-d982b808efcb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m# training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m# validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-d982b808efcb>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# pred_map = self.net(img, gt_map)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mpred_density_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_density_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-9655267abf97>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrontend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 342\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Launch GT Train !\n",
    "\n",
    "lr = 1e-5 \n",
    "\n",
    "net = CSRNet().to(device)\n",
    "loss = nn.MSELoss().to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=1e-4)\n",
    "# optimizer = optim.SGD(self.net.parameters(), cfg.LR, momentum=0.95,weight_decay=5e-4)  \n",
    "\n",
    "gt_trainer = Trainer_GT(loading_data_GT, net, loss, optimizer)\n",
    "gt_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models parameters for Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# image = Image.open('../ProcessedData/SHHA/train/IMG_1.jpg')\n",
    "# trans1 = transforms.ToTensor()\n",
    "# img = trans1(image).to(device)\n",
    "# img = img.unsqueeze(0)\n",
    "\n",
    "# write to tensorboard\n",
    "\n",
    "# writer.add_graph(model, img)\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "max_epoch = 20\n",
    "validation_epoch = 1\n",
    "val_start = 0\n",
    "save_dir = \"\"\n",
    "\n",
    "class Trainer_Bayes():\n",
    "    def __init__(self, dataloader, net, loss, optimizer):\n",
    "        self.train_loader, self.val_loader, self.test_loader = dataloader()\n",
    "        self.net = net\n",
    "        self.loss = loss \n",
    "        self.optimizer = optimizer\n",
    "        self.train_record = {'best_mae': 1e20, 'best_mse':1e20, 'best_model_name': ''}\n",
    "        self.epoch = 0\n",
    "        \n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epoch, MAX_EPOCH):\n",
    "            self.epoch = epoch\n",
    "\n",
    "            # training    \n",
    "            self.train_epoch()\n",
    "\n",
    "            # validation\n",
    "            if epoch%VAL_FREQ==0 or epoch>VAL_DENSE_START:\n",
    "                self.validate()\n",
    "                torch.save(self.net.state_dict(), \"my_model_weigths.pth\")\n",
    "                \n",
    "\n",
    "    def train_epoch(self):\n",
    "        epoch_loss = AverageMeter()\n",
    "        epoch_mae = AverageMeter()\n",
    "        epoch_mse = AverageMeter()\n",
    "        epoch_start = time.time()\n",
    "        self.net.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        # Iterate over data.\n",
    "        for step, (inputs, points, targets, st_sizes) in enumerate(self.train_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            st_sizes = st_sizes.to(device)\n",
    "            gd_count = np.array([len(p) for p in points], dtype=np.float32)\n",
    "            points = [p.to(device) for p in points]\n",
    "            targets = [t.to(device) for t in targets]\n",
    "\n",
    "            # print(inputs.size())\n",
    "            # print(gd_count)\n",
    "            # print(points)\n",
    "            # print(targets)\n",
    "            # print(st_sizes)\n",
    "\n",
    "            # inputs = image size(m, 3, crop_size, crop_size)\n",
    "            # gd_counts = [nb_of_head_img1, num_of_head_img2, ...]\n",
    "            # point = array of positions\n",
    "            # targets = ?\n",
    "\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = self.net(inputs)\n",
    "                prob_list = post_prob(points, st_sizes)\n",
    "                loss = self.loss(prob_list, targets, outputs)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "\n",
    "                N = inputs.size(0) # batch size\n",
    "                pre_count = torch.sum(outputs.view(N, -1), dim=1).detach().cpu().numpy()\n",
    "                res = pre_count - gd_count\n",
    "\n",
    "                print(f'step: {step} | gd_count: {gd_count} | prediction: {pre_count} | loss: {loss}')\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                if step % 2 == 1:\n",
    "                    writer.add_scalar('training loss vgg',\n",
    "                                running_loss / 2,\n",
    "                                self.epoch * len(self.train_loader) + step)\n",
    "                    running_loss = 0.0\n",
    "\n",
    "    def validate(self):\n",
    "        epoch_start = time.time()\n",
    "        self.net.eval()  # Set model to evaluate mode\n",
    "        epoch_res = []\n",
    "        running_loss = 0.0\n",
    "        best_mse = 100000000000000\n",
    "        best_mae = 100000000000000\n",
    "\n",
    "        # Iterate over data.\n",
    "        for inputs, count, name in self.val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            # inputs are images with different sizes\n",
    "            assert inputs.size(0) == 1, 'the batch size should equal to 1 in validation mode'\n",
    "            with torch.set_grad_enabled(False):\n",
    "                outputs = self.net(inputs)\n",
    "                res = count[0].item() - torch.sum(outputs).item()\n",
    "                epoch_res.append(res)\n",
    "\n",
    "\n",
    "        running_loss = 0.0\n",
    "        epoch_res = np.array(epoch_res)\n",
    "        mse = np.sqrt(np.mean(np.square(epoch_res)))\n",
    "        mae = np.mean(np.abs(epoch_res))\n",
    "\n",
    "        # ...log the running loss\n",
    "        writer.add_scalar('val MAE vgg',\n",
    "                            mae,\n",
    "                            self.epoch * len(self.val_loader))\n",
    "        writer.add_scalar('val MSE vgg',\n",
    "                        mse,\n",
    "                        self.epoch * len(self.val_loader))\n",
    "\n",
    "        print('Epoch {} Val, MSE: {:.2f} MAE: {:.2f}, Cost {:.1f} sec'\n",
    "                     .format(self.epoch, mse, mae, time.time()-epoch_start))\n",
    "\n",
    "        model_state_dic = self.net.state_dict()\n",
    "        if (2.0 * mse + mae) < (2.0 * best_mse + best_mae):\n",
    "            best_mse = mse\n",
    "            best_mae = mae\n",
    "            print(\"save best mse {:.2f} mae {:.2f} model epoch {}\".format(best_mse,\n",
    "                                                                                 best_mae,\n",
    "                                                                                 self.epoch))\n",
    "            torch.save(model_state_dic, os.path.join(save_dir, 'best_model_vgg.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-4c95b04022bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvgg19\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m post_prob = Post_Prob(sigma,\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "lr = 0.00001\n",
    "weight_decay = 0.9\n",
    "sigma = 0.1\n",
    "use_background = False\n",
    "background_ratio = 1\n",
    "\n",
    "net = vgg19()\n",
    "net.to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "post_prob = Post_Prob(sigma,\n",
    "                           crop_size,\n",
    "                           downsample_ratio,\n",
    "                           background_ratio,\n",
    "                           use_background,\n",
    "                           device)\n",
    "optimizer = Bay_Loss(use_background, device)\n",
    "\n",
    "bayes_trainer = Trainer_Bayes(loading_data_Bayes, net, loss, optimizer)\n",
    "bayes_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
