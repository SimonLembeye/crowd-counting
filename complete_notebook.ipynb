{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import models,transforms,datasets\n",
    "from torch.utils import data\n",
    "%matplotlib inline\n",
    "import torchvision.transforms as standard_transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from PIL import Image, ImageOps, ImageFilter\n",
    "import numbers\n",
    "import pdb\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as F2\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from glob import glob\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch import optim\n",
    "from torch.nn import Module\n",
    "import time\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code can be run both on laptop and on GCP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using gpu: True \n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Using gpu: %s ' % torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unzip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data.zip\n",
      "replace data/bayes/train/IMG_290.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "We use 2 nn, a CSRNet and a VGG19 extented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG19 extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = ['vgg19']\n",
    "model_urls = {\n",
    "    'vgg19': 'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth',\n",
    "}\n",
    "\n",
    "class VGGExtended(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super(VGGExtended, self).__init__()\n",
    "        self.features = features\n",
    "        self.reg_layer = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = F.upsample_bilinear(x, scale_factor=2)\n",
    "        x = self.reg_layer(x)\n",
    "        return torch.abs(x)\n",
    "\n",
    "\n",
    "def make_layers(cfg, in_channels = 3,batch_norm=False,dilation = False):\n",
    "    if dilation:\n",
    "        d_rate = 2\n",
    "    else:\n",
    "        d_rate = 1\n",
    "    layers = []\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=d_rate,dilation = d_rate)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "cfg = {\n",
    "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512]\n",
    "}\n",
    "\n",
    "def vgg19():\n",
    "    \"\"\"VGG 19-layer model (configuration \"E\")\n",
    "        model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = VGGExtended(make_layers(cfg['E']))\n",
    "    model.load_state_dict(model_zoo.load_url(model_urls['vgg19']), strict=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSRNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSRNet(nn.Module):\n",
    "    def __init__(self, load_weights=False):\n",
    "        super(CSRNet, self).__init__()\n",
    "        self.seen = 0\n",
    "        self.frontend_feat = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512]\n",
    "        self.backend_feat  = [512, 512, 512,256,128,64]\n",
    "        self.frontend = make_layers(self.frontend_feat)\n",
    "        self.backend = make_layers(self.backend_feat,in_channels = 512,dilation = True)\n",
    "        self.output_layer = nn.Conv2d(64, 1, kernel_size=1)\n",
    "        if not load_weights:\n",
    "            mod = models.vgg16(pretrained = True)\n",
    "            self._initialize_weights()\n",
    "            self.frontend.load_state_dict(mod.features[0:23].state_dict())\n",
    "            \n",
    "    def forward(self,x):\n",
    "        size = x.size()\n",
    "        x = self.frontend(x)\n",
    "        x = self.backend(x)\n",
    "        x = self.output_layer(x)\n",
    "        x = F.upsample(x, size = size[2:])\n",
    "        return x\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight, std=0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_cropBayes(im_h, im_w, crop_h, crop_w):\n",
    "    res_h = im_h - crop_h\n",
    "    res_w = im_w - crop_w\n",
    "    i = random.randint(0, res_h)\n",
    "    j = random.randint(0, res_w)\n",
    "    return i, j, crop_h, crop_w\n",
    "\n",
    "\n",
    "def cal_innner_area(c_left, c_up, c_right, c_down, bbox):\n",
    "    inner_left = np.maximum(c_left, bbox[:, 0])\n",
    "    inner_up = np.maximum(c_up, bbox[:, 1])\n",
    "    inner_right = np.minimum(c_right, bbox[:, 2])\n",
    "    inner_down = np.minimum(c_down, bbox[:, 3])\n",
    "    inner_area = np.maximum(inner_right-inner_left, 0.0) * np.maximum(inner_down-inner_up, 0.0)\n",
    "    return inner_area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ground Truth dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTDataset(data.Dataset):\n",
    "    def __init__(self, data_path, mode, main_transform=None, img_transform=None, gt_transform=None):\n",
    "        self.img_path = data_path + '/img'\n",
    "        self.gt_path = data_path + '/den'\n",
    "        self.data_files = [filename for filename in os.listdir(self.img_path) \\\n",
    "                           if os.path.isfile(os.path.join(self.img_path,filename))]\n",
    "        self.num_samples = len(self.data_files) \n",
    "        self.main_transform=main_transform  \n",
    "        self.img_transform = img_transform\n",
    "        self.gt_transform = gt_transform     \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        fname = self.data_files[index]\n",
    "        img, den = self.read_image_and_gt(fname)      \n",
    "        if self.main_transform is not None:\n",
    "            img, den = self.main_transform(img,den) \n",
    "        if self.img_transform is not None:\n",
    "            img = self.img_transform(img)         \n",
    "        if self.gt_transform is not None:\n",
    "            den = self.gt_transform(den)               \n",
    "        return img, den\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def read_image_and_gt(self,fname):\n",
    "        img = Image.open(os.path.join(self.img_path,fname))\n",
    "        if img.mode == 'L':\n",
    "            img = img.convert('RGB')\n",
    "\n",
    "        den = pd.read_csv(os.path.join(self.gt_path,os.path.splitext(fname)[0] + '.csv'), sep=',',header=None).values\n",
    "        \n",
    "        den = den.astype(np.float32, copy=False)    \n",
    "        den = Image.fromarray(den)  \n",
    "        return img, den    \n",
    "\n",
    "    def get_num_samples(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayes method Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesDataset(data.Dataset):\n",
    "    def __init__(self, root_path, crop_size,\n",
    "                 downsample_ratio, is_gray=False,\n",
    "                 method='train'):\n",
    "\n",
    "        self.root_path = root_path\n",
    "        self.im_list = sorted(glob(os.path.join(self.root_path, '*.jpg')))\n",
    "        if method not in ['train', 'val', 'test']:\n",
    "            raise Exception(\"not implement\")\n",
    "        self.method = method\n",
    "\n",
    "        self.c_size = crop_size\n",
    "        self.d_ratio = downsample_ratio\n",
    "        assert self.c_size % self.d_ratio == 0\n",
    "        self.dc_size = self.c_size // self.d_ratio\n",
    "\n",
    "        if is_gray:\n",
    "            self.trans = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "            ])\n",
    "        else:\n",
    "            self.trans = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # Pour CSRNet à vérifier si besoin de modif\n",
    "            ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.im_list)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        img_path = self.im_list[item]\n",
    "        gd_path = img_path.replace('jpg', 'npy')\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        if self.method == 'train':\n",
    "            keypoints = np.load(gd_path)\n",
    "            return self.train_transform(img, keypoints)\n",
    "        else:\n",
    "            keypoints = np.load(gd_path)\n",
    "            img = self.trans(img)\n",
    "            name = os.path.basename(img_path).split('.')[0]\n",
    "            return img, len(keypoints), name\n",
    "\n",
    "    def train_transform(self, img, keypoints):\n",
    "        \"\"\"random crop image patch and find people in it\"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        Les keypoints correspondent aux coordonnées des têtes\n",
    "        MAIS une troisième coordonnée a été calculée lors du preprocessing des données,\n",
    "        elle correspont à \"dis\" et semble important pour calculer pas mal de choses\n",
    "        \"\"\"\n",
    "        \n",
    "        wd, ht = img.size\n",
    "        st_size = min(wd, ht)\n",
    "        assert st_size >= self.c_size\n",
    "        assert len(keypoints) > 0\n",
    "        i, j, h, w = random_cropBayes(ht, wd, self.c_size, self.c_size)\n",
    "        img = F2.crop(img, i, j, h, w)\n",
    "        \n",
    "        nearest_dis = np.clip(keypoints[:, 2], 4.0, 128.0)\n",
    "       \n",
    "        points_left_up = keypoints[:, :2] - nearest_dis[:, None] / 2.0\n",
    "        points_right_down = keypoints[:, :2] + nearest_dis[:, None] / 2.0\n",
    "        bbox = np.concatenate((points_left_up, points_right_down), axis=1)\n",
    "        inner_area = cal_innner_area(j, i, j+w, i+h, bbox)\n",
    "        origin_area = nearest_dis * nearest_dis\n",
    "        ratio = np.clip(1.0 * inner_area / origin_area, 0.0, 1.0)\n",
    "        mask = (ratio >= 0.3)\n",
    "\n",
    "        target = ratio[mask]\n",
    "        keypoints = keypoints[mask]\n",
    "        keypoints = keypoints[:, :2] - [j, i]  # change coodinate\n",
    "        if len(keypoints) > 0:\n",
    "            if random.random() > 0.5:\n",
    "                img = F2.hflip(img)\n",
    "                keypoints[:, 0] = w - keypoints[:, 0]\n",
    "        else:\n",
    "            if random.random() > 0.5:\n",
    "                img = F2.hflip(img)\n",
    "        return self.trans(img), torch.from_numpy(keypoints.copy()).float(), \\\n",
    "               torch.from_numpy(target.copy()).float(), st_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CSRNet\n",
    "LABEL_FACTOR = 1\n",
    "\n",
    "\n",
    "def random_crop_GT(img,den,dst_size):\n",
    "    # dst_size: ht, wd\n",
    "\n",
    "    _,ts_hd,ts_wd = img.shape\n",
    "\n",
    "    x1 = random.randint(0, ts_wd - dst_size[1])//LABEL_FACTOR*LABEL_FACTOR\n",
    "    y1 = random.randint(0, ts_hd - dst_size[0])//LABEL_FACTOR*LABEL_FACTOR\n",
    "    x2 = x1 + dst_size[1]\n",
    "    y2 = y1 + dst_size[0]\n",
    "\n",
    "    label_x1 = x1//LABEL_FACTOR\n",
    "    label_y1 = y1//LABEL_FACTOR\n",
    "    label_x2 = x2//LABEL_FACTOR\n",
    "    label_y2 = y2//LABEL_FACTOR\n",
    "\n",
    "    return img[:,y1:y2,x1:x2], den[label_y1:label_y2,label_x1:label_x2]\n",
    "\n",
    "\n",
    "\n",
    "def share_memory(batch):\n",
    "    out = None\n",
    "    if False:\n",
    "        # If we're in a background process, concatenate directly into a\n",
    "        # shared memory tensor to avoid an extra copy\n",
    "        numel = sum([x.numel() for x in batch])\n",
    "        storage = batch[0].storage()._new_shared(numel)\n",
    "        out = batch[0].new(storage)\n",
    "    return out\n",
    "\n",
    "crop_size = 256\n",
    "\n",
    "def GT_collate(batch):\n",
    "    # @GJY \n",
    "    r\"\"\"Puts each data field into a tensor with outer dimension batch size\"\"\"\n",
    "\n",
    "    transposed = list(zip(*batch)) # imgs and dens\n",
    "    imgs, dens = [transposed[0],transposed[1]]\n",
    "\n",
    "\n",
    "    error_msg = \"batch must contain tensors; found {}\"\n",
    "    if isinstance(imgs[0], torch.Tensor) and isinstance(dens[0], torch.Tensor):\n",
    "        \n",
    "        cropped_imgs = []\n",
    "        cropped_dens = []\n",
    "        for i_sample in range(len(batch)):\n",
    "            _img, _den = random_crop_GT(imgs[i_sample],dens[i_sample],[crop_size,crop_size])\n",
    "            cropped_imgs.append(_img)\n",
    "            cropped_dens.append(_den)\n",
    "\n",
    "\n",
    "        cropped_imgs = torch.stack(cropped_imgs, 0, out=share_memory(cropped_imgs))\n",
    "        cropped_dens = torch.stack(cropped_dens, 0, out=share_memory(cropped_dens))\n",
    "\n",
    "        return [cropped_imgs,cropped_dens]\n",
    "\n",
    "    raise TypeError((error_msg.format(type(batch[0]))))\n",
    "\n",
    "\n",
    "def loading_data_GT(batch_size=5, num_workers=8):\n",
    "    mean_std = ([0.410824894905, 0.370634973049, 0.359682112932], [0.278580576181, 0.26925137639, 0.27156367898])\n",
    "    log_para = 100.\n",
    "    factor = 1\n",
    "    # DATA_PATH = \"/home/simon/Bureau/framework-crowd-counting/ProcessedData/shanghaitech_part_A\"\n",
    "    DATA_PATH = \"data/gt\"\n",
    "    \n",
    "    \n",
    "    train_main_transform = Compose([\n",
    "        RandomHorizontallyFlip()\n",
    "    ])\n",
    "    img_transform = standard_transforms.Compose([\n",
    "        standard_transforms.ToTensor(),\n",
    "        standard_transforms.Normalize(*mean_std)\n",
    "    ])\n",
    "    gt_transform = standard_transforms.Compose([\n",
    "        GTScaleDown(factor),\n",
    "        LabelNormalize(log_para)\n",
    "    ])\n",
    "\n",
    "    train_set = GTDataset(DATA_PATH+'/train', 'train',main_transform=train_main_transform, img_transform=img_transform, gt_transform=gt_transform)\n",
    "    train_loader =None\n",
    "    if batch_size == 1:\n",
    "        train_loader = DataLoader(train_set, batch_size=1, shuffle=True, drop_last=True)\n",
    "    elif batch_size > 1:\n",
    "        train_loader = DataLoader(train_set, batch_size=batch_size, num_workers=num_workers, collate_fn=GT_collate, shuffle=True, drop_last=True)\n",
    "    \n",
    "    val_set = GTDataset(DATA_PATH+'/val', 'val', main_transform=None, img_transform=img_transform, gt_transform=gt_transform)\n",
    "    val_loader = DataLoader(val_set, batch_size=1, num_workers=num_workers, shuffle=True, drop_last=False)\n",
    "    \n",
    "    test_set = GTDataset(DATA_PATH+'/test', 'test', main_transform=None, img_transform=img_transform, gt_transform=gt_transform)\n",
    "    test_loader = DataLoader(test_set, batch_size=1, num_workers=num_workers, shuffle=True, drop_last=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bayes\n",
    "downsample_ratio = 8 # Mettre à 8 pour le réseau du répo (à 1 pour CSRNet puisque on ne modifie pas la dim avec le réseau)\n",
    "data_dir = \"data/bayes\"\n",
    "#data_dir = \"/home/simon/Bureau/framework-crowd-counting/processed_data_bcc/SHHA\"\n",
    "#data_dir = \"/Users/VictoRambaud/dev/crowd_counting2/ProcessedData/SHHA\"\n",
    "crop_size = 256\n",
    "is_gray = False\n",
    "\n",
    "def train_collate(batch):\n",
    "    transposed_batch = list(zip(*batch))\n",
    "    images = torch.stack(transposed_batch[0], 0)\n",
    "    points = transposed_batch[1]  # the number of points is not fixed, keep it as a list of tensor\n",
    "    targets = transposed_batch[2]\n",
    "    st_sizes = torch.FloatTensor(transposed_batch[3])\n",
    "    return images, points, targets, st_sizes\n",
    "\n",
    "\n",
    "def loading_data_Bayes(batch_size = 5, num_workers = 8):\n",
    "    datasets_bayes = {x: BayesDataset(os.path.join(data_dir, x),\n",
    "                              crop_size,\n",
    "                              downsample_ratio,\n",
    "                              is_gray, x) for x in ['train', 'val', 'test']}\n",
    "\n",
    "    dataloaders_bayes = {x: DataLoader(datasets_bayes[x],\n",
    "                                collate_fn=(train_collate if x == 'train' else default_collate),\n",
    "                                batch_size=(batch_size if x == 'train' else 1),\n",
    "                                shuffle=(True if x == 'train' else False),\n",
    "                                num_workers=num_workers,\n",
    "                                pin_memory=(True if x == 'train' else False))\n",
    "                                for x in ['train', 'val', 'test']}\n",
    "    \n",
    "    dataloaders_bayes_test = \"To do\"\n",
    "    \n",
    "    return dataloaders_bayes[\"train\"], dataloaders_bayes[\"val\"], dataloaders_bayes[\"test\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x7fe65d43f250>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7fe65d2b7c50>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7fe65d2b7f90>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loading_data_Bayes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes : computing losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post_Prob(Module):\n",
    "    def __init__(self, sigma, c_size, stride, background_ratio, use_background, device):\n",
    "        super(Post_Prob, self).__init__()\n",
    "        assert c_size % stride == 0\n",
    "\n",
    "        self.sigma = sigma\n",
    "        self.bg_ratio = background_ratio\n",
    "        self.device = device\n",
    "        # coordinate is same to image space, set to constant since crop size is same\n",
    "        self.cood = torch.arange(0, c_size, step=stride,\n",
    "                                 dtype=torch.float32, device=device) + stride / 2\n",
    "        self.cood.unsqueeze_(0)\n",
    "        self.softmax = torch.nn.Softmax(dim=0)\n",
    "        self.use_bg = use_background\n",
    "\n",
    "    def forward(self, points, st_sizes):\n",
    "        num_points_per_image = [len(points_per_image) for points_per_image in points]\n",
    "        all_points = torch.cat(points, dim=0)\n",
    "\n",
    "        if len(all_points) > 0:\n",
    "            x = all_points[:, 0].unsqueeze_(1)\n",
    "            y = all_points[:, 1].unsqueeze_(1)\n",
    "            x_dis = -2 * torch.matmul(x, self.cood) + x * x + self.cood * self.cood\n",
    "            y_dis = -2 * torch.matmul(y, self.cood) + y * y + self.cood * self.cood\n",
    "            y_dis.unsqueeze_(2)\n",
    "            x_dis.unsqueeze_(1)\n",
    "            dis = y_dis + x_dis\n",
    "            dis = dis.view((dis.size(0), -1))\n",
    "\n",
    "            dis_list = torch.split(dis, num_points_per_image)\n",
    "            prob_list = []\n",
    "            for dis, st_size in zip(dis_list, st_sizes):\n",
    "                if len(dis) > 0:\n",
    "                    if self.use_bg:\n",
    "                        min_dis = torch.clamp(torch.min(dis, dim=0, keepdim=True)[0], min=0.0)\n",
    "                        d = st_size * self.bg_ratio\n",
    "                        bg_dis = (d - torch.sqrt(min_dis))**2\n",
    "                        dis = torch.cat([dis, bg_dis], 0)  # concatenate background distance to the last\n",
    "                    dis = -dis / (2.0 * self.sigma ** 2)\n",
    "                    prob = self.softmax(dis)\n",
    "                else:\n",
    "                    prob = None\n",
    "                prob_list.append(prob)\n",
    "        else:\n",
    "            prob_list = []\n",
    "            for _ in range(len(points)):\n",
    "                prob_list.append(None)\n",
    "        return prob_list\n",
    "    \n",
    "    \n",
    "class Bay_Loss(Module):\n",
    "    def __init__(self, use_background, device):\n",
    "        super(Bay_Loss, self).__init__()\n",
    "        self.device = device\n",
    "        self.use_bg = use_background\n",
    "\n",
    "    def forward(self, prob_list, target_list, pre_density):\n",
    "        loss = 0\n",
    "        \n",
    "        \"\"\"\n",
    "            - prob list semble être la listes des p(yn|xm) ie la contribution du pixel xm sur la n-ieme tête\n",
    "            (les lignes de cette matrice sont de taille 4096 = 64*64)\n",
    "            - pre density est la prédiction de la densité (sortie du réseau) - de taille 64x64 ici\n",
    "            - target list a pour longueur le nombre de têtes - correspond aux E[cn] \"réel\" (le calcul reste un mystère)\n",
    "            - On obtient les E[cn] estimées grâce à un produit terme à terme de prob_list et pre_density\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        for idx, prob in enumerate(prob_list):  # iterative through each sample\n",
    "            if prob is None:  # image contains no annotation points\n",
    "                pre_count = torch.sum(pre_density[idx])\n",
    "                target = torch.zeros((1,), dtype=torch.float32, device=self.device)\n",
    "            else:\n",
    "                N = len(prob)\n",
    "                if self.use_bg:\n",
    "                    target = torch.zeros((N,), dtype=torch.float32, device=self.device)\n",
    "                    target[:-1] = target_list[idx]\n",
    "                else:\n",
    "                    target = target_list[idx]\n",
    "                pre_count = torch.sum(pre_density[idx].view((1, -1)) * prob, dim=1)  # flatten into vector\n",
    "            \n",
    "            loss += torch.sum(torch.abs(target - pre_count))\n",
    "        loss = loss / len(prob_list)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================img tranforms============================\n",
    "\n",
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, mask, bbx=None):\n",
    "        if bbx is None:\n",
    "            for t in self.transforms:\n",
    "                img, mask = t(img, mask)\n",
    "            return img, mask\n",
    "        for t in self.transforms:\n",
    "            img, mask, bbx = t(img, mask, bbx)\n",
    "        return img, mask, bbx\n",
    "\n",
    "class RandomHorizontallyFlip(object):\n",
    "    def __call__(self, img, mask, bbx=None):\n",
    "        if random.random() < 0.5:\n",
    "            if bbx is None:\n",
    "                return img.transpose(Image.FLIP_LEFT_RIGHT), mask.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            w, h = img.size\n",
    "            xmin = w - bbx[:,3]\n",
    "            xmax = w - bbx[:,1]\n",
    "            bbx[:,1] = xmin\n",
    "            bbx[:,3] = xmax\n",
    "            return img.transpose(Image.FLIP_LEFT_RIGHT), mask.transpose(Image.FLIP_LEFT_RIGHT), bbx\n",
    "        if bbx is None:\n",
    "            return img, mask\n",
    "        return img, mask, bbx\n",
    "\n",
    "\n",
    "\n",
    "# ===============================label tranforms============================\n",
    "\n",
    "class DeNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "        return tensor\n",
    "\n",
    "\n",
    "class LabelNormalize(object):\n",
    "    def __init__(self, para):\n",
    "        self.para = para\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        # tensor = 1./(tensor+self.para).log()\n",
    "        tensor = torch.from_numpy(np.array(tensor))\n",
    "        tensor = tensor*self.para\n",
    "        return tensor\n",
    "\n",
    "    \n",
    "class GTScaleDown(object):\n",
    "    def __init__(self, factor=8):\n",
    "        self.factor = factor\n",
    "\n",
    "    def __call__(self, img):\n",
    "        w, h = img.size\n",
    "        if self.factor==1:\n",
    "            return img\n",
    "        tmp = np.array(img.resize((w//self.factor, h//self.factor), Image.BICUBIC))*self.factor*self.factor\n",
    "        img = Image.fromarray(tmp)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dossier à créer sur Google Cloud !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"best_model_weights\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainer GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRINT_FREQ = 1\n",
    "LOG_PARA = 100. # C'est quoi ce LOG_PARA ??\n",
    "seed = 1\n",
    "\n",
    "\n",
    "class Trainer_GT():\n",
    "    def __init__(self, dataloader, net, loss, optimizer, validation_frequency=1, max_epoch=100):\n",
    "        self.train_loader, self.val_loader, self.test_loader = dataloader()\n",
    "        self.net = net\n",
    "        self.loss = loss \n",
    "        self.optimizer = optimizer\n",
    "        self.best_mae = 1e20\n",
    "        self.best_mse = 1e20\n",
    "        self.epoch = 0\n",
    "        self.validation_frequency = validation_frequency\n",
    "        self.max_epoch = max_epoch\n",
    "\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(0, self.max_epoch):\n",
    "            self.epoch = epoch\n",
    "            # si on veut un lr sheduler il faut le mettre là\n",
    "                \n",
    "            # training    \n",
    "            self.train_epoch()\n",
    "\n",
    "            # validation\n",
    "            if epoch%self.validation_frequency==0:\n",
    "                self.validate()\n",
    "                \n",
    "        print(f'Train finished | best_mse: {self.best_mse} | best_mae: {self.best_mae}')\n",
    "\n",
    "\n",
    "    def train_epoch(self): # training for all datasets\n",
    "        self.net.train()\n",
    "        \n",
    "        for step, data in enumerate(self.train_loader, 0):\n",
    "            img, gt_map = data\n",
    "            img = Variable(img).to(device)\n",
    "            print(img.size())\n",
    "            gt_map = Variable(gt_map).to(device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            # pred_map = self.net(img, gt_map)\n",
    "            \n",
    "            pred_density_map = self.net(img)\n",
    "            loss = self.loss(pred_density_map, gt_map)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            gt_count = [int(gt_map[i].sum().data / LOG_PARA) for i in range(gt_map.size()[0])]\n",
    "            pre_count = [int(pred_density_map[i].sum().data/LOG_PARA) for i in range(pred_density_map.size()[0])]\n",
    "            \n",
    "            print(f'epoch: {self.epoch} | step: {step} | count: {gt_count} | prediction: {pre_count} | loss: {loss}') \n",
    "\n",
    "\n",
    "    def validate(self):\n",
    "        epoch_start = time.time()\n",
    "        self.net.eval()\n",
    "        epoch_res = []\n",
    "\n",
    "        for vi, data in enumerate(self.val_loader, 0):\n",
    "            img, gt_map = data\n",
    "\n",
    "            with torch.no_grad():\n",
    "                img = Variable(img).to(device)\n",
    "                assert img.size(0) == 1\n",
    "                gt_map = Variable(gt_map).to(device)\n",
    "                pred_density_map = self.net(img)\n",
    "                \n",
    "                pred_cnt = int(gt_map[0].sum().data / LOG_PARA)\n",
    "                gt_count = int(pred_density_map[0].sum().data/LOG_PARA)\n",
    "                res = gt_count - pred_cnt\n",
    "                    \n",
    "                epoch_res.append(res)\n",
    "\n",
    "\n",
    "        epoch_res = np.array(epoch_res)\n",
    "        mse = np.sqrt(np.mean(np.square(epoch_res)))\n",
    "        mae = np.mean(np.abs(epoch_res))\n",
    "        \n",
    "        print('Epoch {} Val, MSE: {:.2f} MAE: {:.2f}, Cost {:.1f} sec'\n",
    "                     .format(self.epoch, mse, mae, time.time()-epoch_start))\n",
    "\n",
    "        if (2.0 * mse + mae) < (2.0 * self.best_mse + self.best_mae):\n",
    "            self.best_mse = mse\n",
    "            self.best_mae = mae\n",
    "            print(\"save best mse {:.2f} mae {:.2f} model epoch {}\".format(self.best_mse,\n",
    "                                                                            self.best_mae,\n",
    "                                                                                 self.epoch))\n",
    "            torch.save(self.net.state_dict(), os.path.join(save_dir, 'best_model_gt.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([5, 256, 256])) that is different to the input size (torch.Size([5, 1, 256, 256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | step: 0 | count: [1, 449, 203, 33, 48] | prediction: [0, 0, 0, 0, 0] | loss: 0.3974553942680359\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 1 | count: [3, 5, 130, 185, 49] | prediction: [0, 0, 0, 0, 0] | loss: 0.08401802182197571\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 2 | count: [10, 0, 6, 120, 10] | prediction: [0, 0, 0, 0, 0] | loss: 0.033981602638959885\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 3 | count: [34, 56, 144, 64, 90] | prediction: [0, 0, 0, 0, 0] | loss: 0.15288524329662323\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 4 | count: [24, 97, 27, 30, 151] | prediction: [0, 0, 0, 0, 0] | loss: 0.07681484520435333\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 5 | count: [137, 44, 64, 137, 2] | prediction: [0, 0, 0, 0, 0] | loss: 0.0793282762169838\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 6 | count: [40, 11, 23, 5, 71] | prediction: [1, 0, 1, 0, 0] | loss: 0.029872078448534012\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 7 | count: [148, 199, 125, 46, 35] | prediction: [1, 1, 1, 1, 1] | loss: 0.13221091032028198\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 8 | count: [163, 159, 114, 349, 56] | prediction: [1, 1, 1, 0, 1] | loss: 0.22123120725154877\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 9 | count: [16, 112, 107, 19, 22] | prediction: [1, 1, 2, 1, 1] | loss: 0.05702589824795723\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 10 | count: [25, 3, 6, 27, 44] | prediction: [2, 2, 2, 2, 2] | loss: 0.020685382187366486\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 11 | count: [13, 21, 64, 15, 401] | prediction: [3, 3, 2, 2, 1] | loss: 0.24950575828552246\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 12 | count: [98, 54, 2, 85, 0] | prediction: [3, 2, 1, 2, 1] | loss: 0.07044017314910889\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 13 | count: [0, 101, 56, 17, 13] | prediction: [2, 4, 3, 4, 2] | loss: 0.04057732969522476\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 14 | count: [276, 0, 13, 61, 90] | prediction: [3, 3, 2, 3, 3] | loss: 0.14654898643493652\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 15 | count: [1, 35, 31, 157, 47] | prediction: [3, 4, 4, 6, 4] | loss: 0.06360625475645065\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 16 | count: [16, 93, 45, 71, 19] | prediction: [5, 3, 8, 3, 5] | loss: 0.05590170621871948\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 17 | count: [9, 18, 5, 29, 0] | prediction: [4, 7, 4, 7, 5] | loss: 0.01192071195691824\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 18 | count: [3, 31, 32, 8, 10] | prediction: [7, 8, 9, 7, 7] | loss: 0.015916740521788597\n",
      "torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-30c5dec6103e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mgt_trainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer_GT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloading_data_GT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mgt_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-100-7d57b2faa271>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m# training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m# validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-100-7d57b2faa271>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mgt_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mLOG_PARA\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mpre_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_density_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mLOG_PARA\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_density_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-100-7d57b2faa271>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mgt_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mLOG_PARA\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mpre_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_density_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mLOG_PARA\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_density_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Launch GT Train !\n",
    "lr = 1e-5 \n",
    "\n",
    "gt_net = CSRNet().to(device)\n",
    "loss = nn.MSELoss().to(device)\n",
    "optimizer = optim.Adam(gt_net.parameters(), lr=lr, weight_decay=1e-4)\n",
    "# optimizer = optim.SGD(self.net.parameters(), cfg.LR, momentum=0.95,weight_decay=5e-4)  \n",
    "\n",
    "gt_trainer = Trainer_GT(loading_data_GT, gt_net, loss, optimizer, max_epoch=2)\n",
    "gt_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainer Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# image = Image.open('../ProcessedData/SHHA/train/IMG_1.jpg')\n",
    "# trans1 = transforms.ToTensor()\n",
    "# img = trans1(image).to(device)\n",
    "# img = img.unsqueeze(0)\n",
    "\n",
    "# write to tensorboard\n",
    "\n",
    "# writer.add_graph(model, img)\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer_Bayes():\n",
    "    def __init__(self, dataloader, net, loss, optimizer,  validation_frequency=1, max_epoch=100):\n",
    "        self.train_loader, self.val_loader, _ = dataloader()\n",
    "        self.net = net\n",
    "        self.loss = loss \n",
    "        self.optimizer = optimizer\n",
    "        self.best_mae = 1e20\n",
    "        self.best_mse = 1e20\n",
    "        self.epoch = 0\n",
    "        self.validation_frequency = validation_frequency\n",
    "        self.max_epoch = max_epoch\n",
    "        \n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(0, self.max_epoch):\n",
    "            self.epoch = epoch\n",
    "\n",
    "            # training    \n",
    "            self.train_epoch()\n",
    "\n",
    "            # validation\n",
    "            if epoch % self.validation_frequency == 0:\n",
    "                self.validate()\n",
    "                \n",
    "        print(f'Train finished | best_mse: {self.best_mse} | best_mae: {self.best_mae}')\n",
    "                \n",
    "\n",
    "    def train_epoch(self):\n",
    "        self.net.train()  # Set model to training mode\n",
    "        # running_loss = 0.0\n",
    "        # Iterate over data.\n",
    "        for step, (inputs, points, targets, st_sizes) in enumerate(self.train_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            print(inputs.size())\n",
    "            st_sizes = st_sizes.to(device)\n",
    "            gd_count = np.array([len(p) for p in points], dtype=np.float32)\n",
    "            points = [p.to(device) for p in points]\n",
    "            targets = [t.to(device) for t in targets]\n",
    "            \n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = self.net(inputs)\n",
    "                prob_list = post_prob(points, st_sizes)\n",
    "                loss = self.loss(prob_list, targets, outputs)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                N = inputs.size(0) # batch size\n",
    "                pre_count = torch.sum(outputs.view(N, -1), dim=1).detach().cpu().numpy()\n",
    "                res = pre_count - gd_count\n",
    "\n",
    "                print(f'epoch: {self.epoch} | step: {step} | gd_count: {gd_count} | prediction: {pre_count} | loss: {loss}')\n",
    "\n",
    "                # running_loss += loss.item()\n",
    "                # if step % 2 == 1:\n",
    "                #   writer.add_scalar('training loss vgg',\n",
    "                #               running_loss / 2,\n",
    "                #               self.epoch * len(self.train_loader) + step)\n",
    "                #   running_loss = 0.0\n",
    "\n",
    "    def validate(self):\n",
    "        epoch_start = time.time()\n",
    "        self.net.eval()  # Set model to evaluate mode\n",
    "        epoch_res = []\n",
    "\n",
    "        # Iterate over data.\n",
    "        for inputs, count, name in self.val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            # inputs are images with different sizes\n",
    "            assert inputs.size(0) == 1 # 'the batch size should equal to 1 in validation mode'\n",
    "            with torch.set_grad_enabled(False):\n",
    "                outputs = self.net(inputs)\n",
    "                res = count[0].item() - torch.sum(outputs).item()\n",
    "                epoch_res.append(res)\n",
    "\n",
    "\n",
    "        epoch_res = np.array(epoch_res)\n",
    "        mse = np.sqrt(np.mean(np.square(epoch_res)))\n",
    "        mae = np.mean(np.abs(epoch_res))\n",
    "\n",
    "        # ...log the running loss\n",
    "        writer.add_scalar('val MAE vgg',\n",
    "                            mae,\n",
    "                            self.epoch * len(self.val_loader))\n",
    "        writer.add_scalar('val MSE vgg',\n",
    "                        mse,\n",
    "                        self.epoch * len(self.val_loader))\n",
    "\n",
    "        print('Epoch {} Val, MSE: {:.2f} MAE: {:.2f}, Cost {:.1f} sec'\n",
    "                     .format(self.epoch, mse, mae, time.time()-epoch_start))\n",
    "\n",
    "        if (2.0 * mse + mae) < (2.0 * self.best_mse + self.best_mae):\n",
    "            self.best_mse = mse\n",
    "            self.best_mae = mae\n",
    "            print(\"save best mse {:.2f} mae {:.2f} model epoch {}\".format(self.best_mse,\n",
    "                                                                            self.best_mae,\n",
    "                                                                                 self.epoch))\n",
    "            torch.save(self.net.state_dict(), os.path.join(save_dir, 'best_model_bayes.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 0 | gd_count: [ 54.  75.  73. 336. 168.] | prediction: [40.376045 27.934978 27.685867 53.104065 34.23043 ] | loss: 105.5298843383789\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 1 | gd_count: [ 25.   3.  36.  32. 449.] | prediction: [37.186348 37.789803 53.837242 55.03681  52.229515] | loss: 107.7060546875\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 2 | gd_count: [ 23. 257.  51.  16.  57.] | prediction: [38.97393  56.050884 47.943253 32.612446 50.821888] | loss: 56.00762939453125\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 3 | gd_count: [ 47. 214.  23.  91.  54.] | prediction: [40.408295 63.39425  36.465775 68.987434 76.11583 ] | loss: 48.712764739990234\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 4 | gd_count: [ 33.   0. 204. 202.  35.] | prediction: [58.533924 21.874063 45.64112  59.16406  40.633938] | loss: 76.14788818359375\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 5 | gd_count: [469.   5. 240.  55.  35.] | prediction: [87.64728  39.30844  68.79186  47.824654 39.270927] | loss: 125.93971252441406\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 6 | gd_count: [ 58.  33.  46. 186.  98.] | prediction: [62.481808 48.213284 49.634583 77.8051   75.055145] | loss: 51.282711029052734\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 7 | gd_count: [ 4.  8. 22. 62. 57.] | prediction: [32.75579  34.696198 42.91473  64.72571  48.299057] | loss: 26.19962501525879\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 8 | gd_count: [465. 124.   6.   9. 272.] | prediction: [ 79.942764  89.08426   30.36123   34.296295 157.87875 ] | loss: 120.9142074584961\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 9 | gd_count: [ 77.  31.  67. 137.  75.] | prediction: [ 97.37155   35.334625  68.03128  153.09863   52.534584] | loss: 37.3581657409668\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 10 | gd_count: [30. 47. 83. 45. 22.] | prediction: [ 36.768486  35.83055  107.80233   45.652203  41.607376] | loss: 26.305334091186523\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 11 | gd_count: [ 63.  56. 110.  63. 128.] | prediction: [44.16808 49.52009 84.38148 74.05304 65.9112 ] | loss: 36.923439025878906\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 12 | gd_count: [ 96.  18.  48. 232.  10.] | prediction: [83.47271  26.916517 27.955273 72.22955  34.667816] | loss: 52.14546585083008\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 13 | gd_count: [ 80. 219.  60.  41.  86.] | prediction: [26.440811 68.88626  33.11035  31.973698 37.53154 ] | loss: 57.192481994628906\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 14 | gd_count: [121.  34. 138.  58. 255.] | prediction: [36.978767 28.098892 70.39856  39.789528 85.93776 ] | loss: 71.62104797363281\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 15 | gd_count: [191.  61. 159. 191.   0.] | prediction: [ 84.418304  38.78586  102.38092  106.328125  26.765759] | loss: 64.34855651855469\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 16 | gd_count: [15. 18. 54. 52. 26.] | prediction: [27.003704 27.984676 29.48886  62.153976 27.122656] | loss: 22.38471794128418\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 17 | gd_count: [38. 43. 84. 21. 29.] | prediction: [39.477337 52.626144 74.62141  28.373283 33.61207 ] | loss: 20.853466033935547\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 18 | gd_count: [ 4. 11. 64. 47. 76.] | prediction: [25.919529 25.840229 82.183556 37.575684 99.373024] | loss: 28.5400447845459\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 19 | gd_count: [ 61.  18.  96.  51. 229.] | prediction: [ 36.607773  22.0125    98.47675   68.37791  226.59496 ] | loss: 47.084564208984375\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 20 | gd_count: [65. 18. 74.  6. 54.] | prediction: [83.68703  41.640297 63.692154 17.086086 57.602398] | loss: 26.729028701782227\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 21 | gd_count: [195.  17.  32.  46.  16.] | prediction: [63.174255 23.527153 29.231281 25.922398 25.921837] | loss: 41.38660430908203\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 22 | gd_count: [47. 11. 61. 38. 50.] | prediction: [85.57301  23.856476 63.06997  54.771687 47.88408 ] | loss: 28.968908309936523\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 23 | gd_count: [ 3. 16. 55. 63. 89.] | prediction: [23.870651 21.817839 50.865177 36.344757 88.519424] | loss: 23.957075119018555\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 24 | gd_count: [ 50. 237.  49.  20.   9.] | prediction: [ 39.945652 111.2085    36.156197  31.799349  19.889189] | loss: 40.222835540771484\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 25 | gd_count: [113. 102.  23.  11.  76.] | prediction: [81.93585  71.54639  28.63698  17.140728 54.73874 ] | loss: 31.321502685546875\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 26 | gd_count: [ 97.  13.  65. 310.  36.] | prediction: [ 81.15312   20.986855  46.086594 121.35923   19.36461 ] | loss: 61.891902923583984\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 27 | gd_count: [119.  25. 109.  76.  93.] | prediction: [71.91603  22.902681 66.861404 60.172676 38.213623] | loss: 34.53290939331055\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 28 | gd_count: [ 33.  29.  49. 105.  43.] | prediction: [25.077778 25.17231  26.16542  63.526505 47.37095 ] | loss: 30.001325607299805\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 29 | gd_count: [ 87.  11.  59. 100. 144.] | prediction: [45.964027 21.050604 50.10206  56.448666 50.14264 ] | loss: 53.850929260253906\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 30 | gd_count: [ 25. 228.  36.  51.  28.] | prediction: [ 29.308147 148.8404    31.062721  32.50657   26.649406] | loss: 30.050878524780273\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 31 | gd_count: [624.   2.  39.  58. 236.] | prediction: [145.9335    21.186134  28.165604  40.303894 102.94691 ] | loss: 134.06935119628906\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 32 | gd_count: [ 30.  16. 119.   0.  71.] | prediction: [29.770214 25.574837 54.374283 21.125294 76.72525 ] | loss: 27.2884578704834\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 33 | gd_count: [  1.  67. 166.  57.  60.] | prediction: [27.485687 25.801552 72.002174 55.32184  32.40118 ] | loss: 46.656288146972656\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 34 | gd_count: [ 15.  71. 168.  25. 153.] | prediction: [ 21.58302   65.19381   87.59298  125.208694 152.82065 ] | loss: 58.619319915771484\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 35 | gd_count: [ 33.  20. 104.  32.  44.] | prediction: [ 37.25538   28.469952 145.61952   51.81501   36.83572 ] | loss: 29.37982177734375\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 36 | gd_count: [225.   9.  94.  17.  39.] | prediction: [100.64337   26.914246  97.62446   20.138584  35.469994] | loss: 44.4423942565918\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 37 | gd_count: [117.  14.  38.  17.  65.] | prediction: [114.47507   25.976734  37.86529   24.984766 150.40016 ] | loss: 37.12553787231445\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 38 | gd_count: [129. 429.  39.  87.  89.] | prediction: [ 91.312294 217.61864   34.396435  78.281235  87.25433 ] | loss: 68.48125457763672\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 39 | gd_count: [88. 31.  9. 80. 67.] | prediction: [93.31357  26.264868 27.016045 73.87196  71.23486 ] | loss: 29.2625675201416\n",
      "torch.Size([5, 3, 256, 256])\n",
      "epoch: 0 | step: 40 | gd_count: [79.  0. 77. 30. 74.] | prediction: [92.33977  14.719722 92.73224  36.039734 82.22084 ] | loss: 27.932600021362305\n",
      "torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-113456cec317>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mbayes_trainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer_Bayes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloading_data_Bayes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbayes_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mbayes_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-92-5e0d8af1bae4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;31m# training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m# validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-92-5e0d8af1bae4>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr = 0.00001\n",
    "weight_decay = 0.9\n",
    "sigma = 0.1\n",
    "use_background = False\n",
    "background_ratio = 1\n",
    "\n",
    "bayes_net = vgg19().to(device)\n",
    "optimizer = optim.Adam(bayes_net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "post_prob = Post_Prob(sigma,\n",
    "                           crop_size,\n",
    "                           downsample_ratio,\n",
    "                           background_ratio,\n",
    "                           use_background,\n",
    "                           device)\n",
    "loss = Bay_Loss(use_background, device)\n",
    "\n",
    "bayes_trainer = Trainer_Bayes(loading_data_Bayes, bayes_net, loss, optimizer, max_epoch=3)\n",
    "bayes_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -12 399 411\n",
      "1 223 291 68\n",
      "2 138 367 229\n",
      "3 -295 466 761\n",
      "4 -174 293 467\n",
      "5 -57 297 354\n",
      "6 -15 278 293\n",
      "7 -657 495 1152\n",
      "8 -7 477 484\n",
      "9 241 389 148\n",
      "10 168 366 198\n",
      "11 39 331 292\n",
      "12 56 556 500\n",
      "13 -752 404 1156\n",
      "14 -247 354 601\n",
      "15 224 338 114\n",
      "16 -298 418 716\n",
      "17 80 420 340\n",
      "18 94 394 300\n",
      "19 -11 369 380\n",
      "20 -74 509 583\n",
      "21 72 592 520\n",
      "22 -288 428 716\n",
      "23 -229 338 567\n",
      "24 127 261 134\n",
      "25 57 364 307\n",
      "26 -130 244 374\n",
      "27 -577 408 985\n",
      "28 -33 329 362\n",
      "29 345 462 117\n",
      "30 292 542 250\n",
      "31 235 375 140\n",
      "32 -28 524 552\n",
      "33 -859 372 1231\n",
      "34 195 260 65\n",
      "35 -207 359 566\n",
      "36 186 435 249\n",
      "37 -107 481 588\n",
      "38 208 437 229\n",
      "39 265 482 217\n",
      "40 -45 310 355\n",
      "41 -728 637 1365\n",
      "42 -80 317 397\n",
      "43 202 358 156\n",
      "44 -312 544 856\n",
      "45 -513 509 1022\n",
      "46 -292 236 528\n",
      "47 -34 569 603\n",
      "48 -111 402 513\n",
      "49 -1190 390 1580\n",
      "50 125 388 263\n",
      "51 -544 376 920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-db625c26f406>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mgt_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mpred_density_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgt_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mpred_cnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mLOG_PARA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mgt_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_density_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mLOG_PARA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgt_count\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpred_cnt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "_, _, test_dataloader = loading_data_GT()\n",
    "\n",
    "gt_net.load_state_dict(torch.load(os.path.join(save_dir, 'best_model_gt.pth'), device))\n",
    "gt_net.eval()\n",
    "errors = []\n",
    "\n",
    "\n",
    "for vi, data in enumerate(test_dataloader, 0):\n",
    "    img, gt_map = data\n",
    "\n",
    "    with torch.no_grad():\n",
    "        img = Variable(img).to(device)\n",
    "        assert img.size(0) == 1\n",
    "        gt_map = Variable(gt_map).to(device)\n",
    "        pred_density_map = gt_net(img)\n",
    "        pred_cnt = int(gt_map[0].sum().data / LOG_PARA)\n",
    "        gt_count = int(pred_density_map[0].sum().data/LOG_PARA)\n",
    "        error = gt_count - pred_cnt\n",
    "        print(vi, error, gt_count, pred_cnt)\n",
    "\n",
    "        errors.append(error)\n",
    "\n",
    "\n",
    "errors = np.array(errors)\n",
    "mse = np.sqrt(np.mean(np.square(errors)))\n",
    "mae = np.mean(np.abs(errors))\n",
    "\n",
    "log_str = 'Final Test: mae {}, mse {}'.format(mae, mse)\n",
    "print(log_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, test_dataloader = loading_data_Bayes()\n",
    "\n",
    "bayes_net.load_state_dict(torch.load(os.path.join(save_dir, 'best_model_bayes.pth'), device))\n",
    "errors = []\n",
    "\n",
    "for inputs, count, name in test_dataloader:\n",
    "    inputs = inputs.to(device)\n",
    "    assert inputs.size(0) == 1\n",
    "    with torch.set_grad_enabled(False):\n",
    "        outputs = bayes_net(inputs)\n",
    "        error = count[0].item() - torch.sum(outputs).item()\n",
    "        print(name, error, count[0].item(), torch.sum(outputs).item())\n",
    "        errors.append(error)\n",
    "\n",
    "errors = np.array(errors)\n",
    "mse = np.sqrt(np.mean(np.square(errors)))\n",
    "mae = np.mean(np.abs(errors))\n",
    "log_str = 'Final Test: mae {}, mse {}'.format(mae, mse)\n",
    "print(log_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fonction Victor - à garder pour le github final\n",
    "\n",
    "def test_bayes(net, test_data, has_loader=False):\n",
    "    \"\"\"\n",
    "    net : the trained network\n",
    "    has_loader : if false, we are just giving a single input and want to get its results, \n",
    "        else we are giving a dataloader\n",
    "    test_data : just input (np.array) and count if has_loader == False, data_loader if not\n",
    "    \"\"\"\n",
    "    \n",
    "    net.eval()  # Set model to evaluate mode\n",
    "    \n",
    "    if not has_loader:\n",
    "        img, count = test_data[0], len(test_data[1])\n",
    "        # img must be a np array\n",
    "        img = img.to(device)\n",
    "        img = np.asarray(img)\n",
    "        #the chanels must be in first position in order to work\n",
    "        if img.shape[0] != 3:\n",
    "            img = np.moveaxis(img, (0,1,2), (1,2,0))\n",
    "        img = torch.Tensor(img).unsqueeze(0)\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = net(img)\n",
    "            res = np.abs(count - torch.sum(outputs).item())\n",
    "        return outputs, res\n",
    "    \n",
    "    else:\n",
    "        full_res = []\n",
    "\n",
    "        # Iterate over data.\n",
    "        for inputs, count, name in test_data:\n",
    "            print(name)\n",
    "            inputs = inputs.to(device)\n",
    "            # inputs are images with different sizes\n",
    "            assert inputs.size(0) == 1, 'the batch size should equal to 1 in test mode'\n",
    "            with torch.set_grad_enabled(False):\n",
    "                outputs = net(inputs)\n",
    "                res = count[0].item() - torch.sum(outputs).item()\n",
    "                full_res.append(res)\n",
    "\n",
    "\n",
    "        res = np.array(full_resres)\n",
    "        mse = np.sqrt(np.mean(np.square(res)))\n",
    "        mae = np.mean(np.abs(res))\n",
    "\n",
    "        print('MSE: {:.2f} MAE: {:.2f}'\n",
    "                     .format(mse, mae))\n",
    "        return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
