{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import models,transforms,datasets\n",
    "from torch.utils import data\n",
    "%matplotlib inline\n",
    "import torchvision.transforms as standard_transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from PIL import Image, ImageOps, ImageFilter\n",
    "import numbers\n",
    "import pdb\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as F2\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from glob import glob\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch import optim\n",
    "from torch.nn import Module\n",
    "import time\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code can be run both on laptop and on GCP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using gpu: True \n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Using gpu: %s ' % torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "We use 2 nn, a CSRNet and a VGG19 extented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG19 extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = ['vgg19']\n",
    "model_urls = {\n",
    "    'vgg19': 'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth',\n",
    "}\n",
    "\n",
    "class VGGExtended(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super(VGGExtended, self).__init__()\n",
    "        self.features = features\n",
    "        self.reg_layer = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = F.upsample_bilinear(x, scale_factor=2)\n",
    "        x = self.reg_layer(x)\n",
    "        return torch.abs(x)\n",
    "\n",
    "\n",
    "def make_layers(cfg, in_channels = 3,batch_norm=False,dilation = False):\n",
    "    if dilation:\n",
    "        d_rate = 2\n",
    "    else:\n",
    "        d_rate = 1\n",
    "    layers = []\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=d_rate,dilation = d_rate)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "cfg = {\n",
    "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512]\n",
    "}\n",
    "\n",
    "def vgg19():\n",
    "    \"\"\"VGG 19-layer model (configuration \"E\")\n",
    "        model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = VGGExtended(make_layers(cfg['E']))\n",
    "    model.load_state_dict(model_zoo.load_url(model_urls['vgg19']), strict=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSRNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSRNet(nn.Module):\n",
    "    def __init__(self, load_weights=False):\n",
    "        super(CSRNet, self).__init__()\n",
    "        self.seen = 0\n",
    "        self.frontend_feat = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512]\n",
    "        self.backend_feat  = [512, 512, 512,256,128,64]\n",
    "        self.frontend = make_layers(self.frontend_feat)\n",
    "        self.backend = make_layers(self.backend_feat,in_channels = 512,dilation = True)\n",
    "        self.output_layer = nn.Conv2d(64, 1, kernel_size=1)\n",
    "        if not load_weights:\n",
    "            mod = models.vgg16(pretrained = True)\n",
    "            self._initialize_weights()\n",
    "            self.frontend.load_state_dict(mod.features[0:23].state_dict())\n",
    "            \n",
    "    def forward(self,x):\n",
    "        size = x.size()\n",
    "        x = self.frontend(x)\n",
    "        x = self.backend(x)\n",
    "        x = self.output_layer(x)\n",
    "        x = F.upsample(x, size = size[2:])\n",
    "        return x\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight, std=0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_cropBayes(im_h, im_w, crop_h, crop_w):\n",
    "    res_h = im_h - crop_h\n",
    "    res_w = im_w - crop_w\n",
    "    i = random.randint(0, res_h)\n",
    "    j = random.randint(0, res_w)\n",
    "    return i, j, crop_h, crop_w\n",
    "\n",
    "\n",
    "def cal_innner_area(c_left, c_up, c_right, c_down, bbox):\n",
    "    inner_left = np.maximum(c_left, bbox[:, 0])\n",
    "    inner_up = np.maximum(c_up, bbox[:, 1])\n",
    "    inner_right = np.minimum(c_right, bbox[:, 2])\n",
    "    inner_down = np.minimum(c_down, bbox[:, 3])\n",
    "    inner_area = np.maximum(inner_right-inner_left, 0.0) * np.maximum(inner_down-inner_up, 0.0)\n",
    "    return inner_area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ground Truth dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTDataset(data.Dataset):\n",
    "    def __init__(self, data_path, mode, main_transform=None, img_transform=None, gt_transform=None):\n",
    "        self.img_path = data_path + '/img'\n",
    "        self.gt_path = data_path + '/den'\n",
    "        self.data_files = [filename for filename in os.listdir(self.img_path) \\\n",
    "                           if os.path.isfile(os.path.join(self.img_path,filename))]\n",
    "        self.num_samples = len(self.data_files) \n",
    "        self.main_transform=main_transform  \n",
    "        self.img_transform = img_transform\n",
    "        self.gt_transform = gt_transform     \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        fname = self.data_files[index]\n",
    "        img, den = self.read_image_and_gt(fname)      \n",
    "        if self.main_transform is not None:\n",
    "            img, den = self.main_transform(img,den) \n",
    "        if self.img_transform is not None:\n",
    "            img = self.img_transform(img)         \n",
    "        if self.gt_transform is not None:\n",
    "            den = self.gt_transform(den)               \n",
    "        return img, den\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def read_image_and_gt(self,fname):\n",
    "        img = Image.open(os.path.join(self.img_path,fname))\n",
    "        if img.mode == 'L':\n",
    "            img = img.convert('RGB')\n",
    "\n",
    "        den = pd.read_csv(os.path.join(self.gt_path,os.path.splitext(fname)[0] + '.csv'), sep=',',header=None).values\n",
    "        \n",
    "        den = den.astype(np.float32, copy=False)    \n",
    "        den = Image.fromarray(den)  \n",
    "        return img, den    \n",
    "\n",
    "    def get_num_samples(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayes method Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesDataset(data.Dataset):\n",
    "    def __init__(self, root_path, crop_size,\n",
    "                 downsample_ratio, is_gray=False,\n",
    "                 method='train'):\n",
    "\n",
    "        self.root_path = root_path\n",
    "        self.im_list = sorted(glob(os.path.join(self.root_path, '*.jpg')))\n",
    "        if method not in ['train', 'val']:\n",
    "            raise Exception(\"not implement\")\n",
    "        self.method = method\n",
    "\n",
    "        self.c_size = crop_size\n",
    "        self.d_ratio = downsample_ratio\n",
    "        assert self.c_size % self.d_ratio == 0\n",
    "        self.dc_size = self.c_size // self.d_ratio\n",
    "\n",
    "        if is_gray:\n",
    "            self.trans = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "            ])\n",
    "        else:\n",
    "            self.trans = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # Pour CSRNet à vérifier si besoin de modif\n",
    "            ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.im_list)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        img_path = self.im_list[item]\n",
    "        gd_path = img_path.replace('jpg', 'npy')\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        if self.method == 'train':\n",
    "            keypoints = np.load(gd_path)\n",
    "            return self.train_transform(img, keypoints)\n",
    "        elif self.method == 'val':\n",
    "            keypoints = np.load(gd_path)\n",
    "            img = self.trans(img)\n",
    "            name = os.path.basename(img_path).split('.')[0]\n",
    "            return img, len(keypoints), name\n",
    "\n",
    "    def train_transform(self, img, keypoints):\n",
    "        \"\"\"random crop image patch and find people in it\"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        Les keypoints correspondent aux coordonnées des têtes\n",
    "        MAIS une troisième coordonnée a été calculée lors du preprocessing des données,\n",
    "        elle correspont à \"dis\" et semble important pour calculer pas mal de choses\n",
    "        \"\"\"\n",
    "        \n",
    "        wd, ht = img.size\n",
    "        st_size = min(wd, ht)\n",
    "        assert st_size >= self.c_size\n",
    "        assert len(keypoints) > 0\n",
    "        i, j, h, w = random_cropBayes(ht, wd, self.c_size, self.c_size)\n",
    "        img = F2.crop(img, i, j, h, w)\n",
    "        \n",
    "        nearest_dis = np.clip(keypoints[:, 2], 4.0, 128.0)\n",
    "       \n",
    "        points_left_up = keypoints[:, :2] - nearest_dis[:, None] / 2.0\n",
    "        points_right_down = keypoints[:, :2] + nearest_dis[:, None] / 2.0\n",
    "        bbox = np.concatenate((points_left_up, points_right_down), axis=1)\n",
    "        inner_area = cal_innner_area(j, i, j+w, i+h, bbox)\n",
    "        origin_area = nearest_dis * nearest_dis\n",
    "        ratio = np.clip(1.0 * inner_area / origin_area, 0.0, 1.0)\n",
    "        mask = (ratio >= 0.3)\n",
    "\n",
    "        target = ratio[mask]\n",
    "        keypoints = keypoints[mask]\n",
    "        keypoints = keypoints[:, :2] - [j, i]  # change coodinate\n",
    "        if len(keypoints) > 0:\n",
    "            if random.random() > 0.5:\n",
    "                img = F2.hflip(img)\n",
    "                keypoints[:, 0] = w - keypoints[:, 0]\n",
    "        else:\n",
    "            if random.random() > 0.5:\n",
    "                img = F2.hflip(img)\n",
    "        return self.trans(img), torch.from_numpy(keypoints.copy()).float(), \\\n",
    "               torch.from_numpy(target.copy()).float(), st_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CSRNet\n",
    "TRAIN_SIZE = (576,768)\n",
    "LABEL_FACTOR = 1\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "def get_min_size(batch):\n",
    "\n",
    "    min_ht = TRAIN_SIZE[0]\n",
    "    min_wd = TRAIN_SIZE[1]\n",
    "\n",
    "    for i_sample in batch:\n",
    "        \n",
    "        _,ht,wd = i_sample.shape\n",
    "        if ht<min_ht:\n",
    "            min_ht = ht\n",
    "        if wd<min_wd:\n",
    "            min_wd = wd\n",
    "    return min_ht,min_wd\n",
    "\n",
    "def random_crop_GT(img,den,dst_size):\n",
    "    # dst_size: ht, wd\n",
    "\n",
    "    _,ts_hd,ts_wd = img.shape\n",
    "\n",
    "    x1 = random.randint(0, ts_wd - dst_size[1])//LABEL_FACTOR*LABEL_FACTOR\n",
    "    y1 = random.randint(0, ts_hd - dst_size[0])//LABEL_FACTOR*LABEL_FACTOR\n",
    "    x2 = x1 + dst_size[1]\n",
    "    y2 = y1 + dst_size[0]\n",
    "\n",
    "    label_x1 = x1//LABEL_FACTOR\n",
    "    label_y1 = y1//LABEL_FACTOR\n",
    "    label_x2 = x2//LABEL_FACTOR\n",
    "    label_y2 = y2//LABEL_FACTOR\n",
    "\n",
    "    return img[:,y1:y2,x1:x2], den[label_y1:label_y2,label_x1:label_x2]\n",
    "\n",
    "\n",
    "\n",
    "def share_memory(batch):\n",
    "    out = None\n",
    "    if False:\n",
    "        # If we're in a background process, concatenate directly into a\n",
    "        # shared memory tensor to avoid an extra copy\n",
    "        numel = sum([x.numel() for x in batch])\n",
    "        storage = batch[0].storage()._new_shared(numel)\n",
    "        out = batch[0].new(storage)\n",
    "    return out\n",
    "\n",
    "\n",
    "def GT_collate(batch):\n",
    "    # @GJY \n",
    "    r\"\"\"Puts each data field into a tensor with outer dimension batch size\"\"\"\n",
    "\n",
    "    transposed = list(zip(*batch)) # imgs and dens\n",
    "    imgs, dens = [transposed[0],transposed[1]]\n",
    "\n",
    "\n",
    "    error_msg = \"batch must contain tensors; found {}\"\n",
    "    if isinstance(imgs[0], torch.Tensor) and isinstance(dens[0], torch.Tensor):\n",
    "        \n",
    "        min_ht, min_wd = get_min_size(imgs)\n",
    "\n",
    "        # print min_ht, min_wd\n",
    "\n",
    "        # pdb.set_trace()\n",
    "        \n",
    "        cropped_imgs = []\n",
    "        cropped_dens = []\n",
    "        for i_sample in range(len(batch)):\n",
    "            _img, _den = random_crop_GT(imgs[i_sample],dens[i_sample],[min_ht,min_wd])\n",
    "            cropped_imgs.append(_img)\n",
    "            cropped_dens.append(_den)\n",
    "\n",
    "\n",
    "        cropped_imgs = torch.stack(cropped_imgs, 0, out=share_memory(cropped_imgs))\n",
    "        cropped_dens = torch.stack(cropped_dens, 0, out=share_memory(cropped_dens))\n",
    "\n",
    "        return [cropped_imgs,cropped_dens]\n",
    "\n",
    "    raise TypeError((error_msg.format(type(batch[0]))))\n",
    "\n",
    "\n",
    "def loading_data_GT(batch_size=5, num_workers=8):\n",
    "    mean_std = ([0.410824894905, 0.370634973049, 0.359682112932], [0.278580576181, 0.26925137639, 0.27156367898])\n",
    "    log_para = 100.\n",
    "    factor = 1\n",
    "    # DATA_PATH = \"/home/simon/Bureau/framework-crowd-counting/ProcessedData/shanghaitech_part_A\"\n",
    "    DATA_PATH = \"ProcessedData/shanghaitech_part_A\"\n",
    "    \n",
    "    \n",
    "    train_main_transform = Compose([\n",
    "        RandomHorizontallyFlip()\n",
    "    ])\n",
    "    img_transform = standard_transforms.Compose([\n",
    "        standard_transforms.ToTensor(),\n",
    "        standard_transforms.Normalize(*mean_std)\n",
    "    ])\n",
    "    gt_transform = standard_transforms.Compose([\n",
    "        GTScaleDown(factor),\n",
    "        LabelNormalize(log_para)\n",
    "    ])\n",
    "\n",
    "    train_set = GTDataset(DATA_PATH+'/train', 'train',main_transform=train_main_transform, img_transform=img_transform, gt_transform=gt_transform)\n",
    "    train_loader =None\n",
    "    if batch_size == 1:\n",
    "        train_loader = DataLoader(train_set, batch_size=1, shuffle=True, drop_last=True)\n",
    "    elif batch_size > 1:\n",
    "        train_loader = DataLoader(train_set, batch_size=batch_size, num_workers=num_workers, collate_fn=GT_collate, shuffle=True, drop_last=True)\n",
    "    \n",
    "    \n",
    "    # il va falloir nous faire un val pour les données GT.\n",
    "    # val_set = GTDataset(DATA_PATH+'/val', 'val', main_transform=None, img_transform=img_transform, gt_transform=gt_transform)\n",
    "    # val_loader = DataLoader(val_set, batch_size=VAL_BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True, drop_last=False)\n",
    "    val_loader = None\n",
    "    \n",
    "    test_set = GTDataset(DATA_PATH+'/test', 'test', main_transform=None, img_transform=img_transform, gt_transform=gt_transform)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, num_workers=1, shuffle=True, drop_last=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bayes\n",
    "downsample_ratio = 8 # Mettre à 8 pour le réseau du répo (à 1 pour CSRNet puisque on ne modifie pas la dim avec le réseau)\n",
    "data_dir = \"SHHA\"\n",
    "#data_dir = \"/home/simon/Bureau/framework-crowd-counting/processed_data_bcc/SHHA\"\n",
    "#data_dir = \"/Users/VictoRambaud/dev/crowd_counting2/ProcessedData/SHHA\"\n",
    "crop_size = 256\n",
    "is_gray = False\n",
    "\n",
    "def train_collate(batch):\n",
    "    transposed_batch = list(zip(*batch))\n",
    "    images = torch.stack(transposed_batch[0], 0)\n",
    "    points = transposed_batch[1]  # the number of points is not fixed, keep it as a list of tensor\n",
    "    targets = transposed_batch[2]\n",
    "    st_sizes = torch.FloatTensor(transposed_batch[3])\n",
    "    return images, points, targets, st_sizes\n",
    "\n",
    "\n",
    "def loading_data_Bayes(batch_size = 5, num_workers = 8):\n",
    "    datasets_bayes = {x: BayesDataset(os.path.join(data_dir, x),\n",
    "                              crop_size,\n",
    "                              downsample_ratio,\n",
    "                              is_gray, x) for x in ['train', 'val']}\n",
    "\n",
    "    dataloaders_bayes = {x: DataLoader(datasets_bayes[x],\n",
    "                                collate_fn=(train_collate if x == 'train' else default_collate),\n",
    "                                batch_size=(batch_size if x == 'train' else 1),\n",
    "                                shuffle=(True if x == 'train' else False),\n",
    "                                num_workers=num_workers,\n",
    "                                pin_memory=(True if x == 'train' else False))\n",
    "                                for x in ['train', 'val']}\n",
    "    \n",
    "    dataloaders_bayes_test = \"To do\"\n",
    "    \n",
    "    return dataloaders_bayes[\"train\"], dataloaders_bayes[\"val\"], dataloaders_bayes_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x7f596193ded0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7f596193da10>,\n",
       " 'To do')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loading_data_Bayes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes : computing losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post_Prob(Module):\n",
    "    def __init__(self, sigma, c_size, stride, background_ratio, use_background, device):\n",
    "        super(Post_Prob, self).__init__()\n",
    "        assert c_size % stride == 0\n",
    "\n",
    "        self.sigma = sigma\n",
    "        self.bg_ratio = background_ratio\n",
    "        self.device = device\n",
    "        # coordinate is same to image space, set to constant since crop size is same\n",
    "        self.cood = torch.arange(0, c_size, step=stride,\n",
    "                                 dtype=torch.float32, device=device) + stride / 2\n",
    "        self.cood.unsqueeze_(0)\n",
    "        self.softmax = torch.nn.Softmax(dim=0)\n",
    "        self.use_bg = use_background\n",
    "\n",
    "    def forward(self, points, st_sizes):\n",
    "        num_points_per_image = [len(points_per_image) for points_per_image in points]\n",
    "        all_points = torch.cat(points, dim=0)\n",
    "\n",
    "        if len(all_points) > 0:\n",
    "            x = all_points[:, 0].unsqueeze_(1)\n",
    "            y = all_points[:, 1].unsqueeze_(1)\n",
    "            x_dis = -2 * torch.matmul(x, self.cood) + x * x + self.cood * self.cood\n",
    "            y_dis = -2 * torch.matmul(y, self.cood) + y * y + self.cood * self.cood\n",
    "            y_dis.unsqueeze_(2)\n",
    "            x_dis.unsqueeze_(1)\n",
    "            dis = y_dis + x_dis\n",
    "            dis = dis.view((dis.size(0), -1))\n",
    "\n",
    "            dis_list = torch.split(dis, num_points_per_image)\n",
    "            prob_list = []\n",
    "            for dis, st_size in zip(dis_list, st_sizes):\n",
    "                if len(dis) > 0:\n",
    "                    if self.use_bg:\n",
    "                        min_dis = torch.clamp(torch.min(dis, dim=0, keepdim=True)[0], min=0.0)\n",
    "                        d = st_size * self.bg_ratio\n",
    "                        bg_dis = (d - torch.sqrt(min_dis))**2\n",
    "                        dis = torch.cat([dis, bg_dis], 0)  # concatenate background distance to the last\n",
    "                    dis = -dis / (2.0 * self.sigma ** 2)\n",
    "                    prob = self.softmax(dis)\n",
    "                else:\n",
    "                    prob = None\n",
    "                prob_list.append(prob)\n",
    "        else:\n",
    "            prob_list = []\n",
    "            for _ in range(len(points)):\n",
    "                prob_list.append(None)\n",
    "        return prob_list\n",
    "    \n",
    "    \n",
    "class Bay_Loss(Module):\n",
    "    def __init__(self, use_background, device):\n",
    "        super(Bay_Loss, self).__init__()\n",
    "        self.device = device\n",
    "        self.use_bg = use_background\n",
    "\n",
    "    def forward(self, prob_list, target_list, pre_density):\n",
    "        loss = 0\n",
    "        \n",
    "        \"\"\"\n",
    "            - prob list semble être la listes des p(yn|xm) ie la contribution du pixel xm sur la n-ieme tête\n",
    "            (les lignes de cette matrice sont de taille 4096 = 64*64)\n",
    "            - pre density est la prédiction de la densité (sortie du réseau) - de taille 64x64 ici\n",
    "            - target list a pour longueur le nombre de têtes - correspond aux E[cn] \"réel\" (le calcul reste un mystère)\n",
    "            - On obtient les E[cn] estimées grâce à un produit terme à terme de prob_list et pre_density\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        for idx, prob in enumerate(prob_list):  # iterative through each sample\n",
    "            if prob is None:  # image contains no annotation points\n",
    "                pre_count = torch.sum(pre_density[idx])\n",
    "                target = torch.zeros((1,), dtype=torch.float32, device=self.device)\n",
    "            else:\n",
    "                N = len(prob)\n",
    "                if self.use_bg:\n",
    "                    target = torch.zeros((N,), dtype=torch.float32, device=self.device)\n",
    "                    target[:-1] = target_list[idx]\n",
    "                else:\n",
    "                    target = target_list[idx]\n",
    "                pre_count = torch.sum(pre_density[idx].view((1, -1)) * prob, dim=1)  # flatten into vector\n",
    "            \n",
    "            loss += torch.sum(torch.abs(target - pre_count))\n",
    "        loss = loss / len(prob_list)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this one should work for both architectures\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = 1.0 * self.sum / self.count\n",
    "\n",
    "    def get_avg(self):\n",
    "        return self.avg\n",
    "\n",
    "    def get_count(self):\n",
    "        return self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================img tranforms============================\n",
    "\n",
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, mask, bbx=None):\n",
    "        if bbx is None:\n",
    "            for t in self.transforms:\n",
    "                img, mask = t(img, mask)\n",
    "            return img, mask\n",
    "        for t in self.transforms:\n",
    "            img, mask, bbx = t(img, mask, bbx)\n",
    "        return img, mask, bbx\n",
    "\n",
    "class RandomHorizontallyFlip(object):\n",
    "    def __call__(self, img, mask, bbx=None):\n",
    "        if random.random() < 0.5:\n",
    "            if bbx is None:\n",
    "                return img.transpose(Image.FLIP_LEFT_RIGHT), mask.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            w, h = img.size\n",
    "            xmin = w - bbx[:,3]\n",
    "            xmax = w - bbx[:,1]\n",
    "            bbx[:,1] = xmin\n",
    "            bbx[:,3] = xmax\n",
    "            return img.transpose(Image.FLIP_LEFT_RIGHT), mask.transpose(Image.FLIP_LEFT_RIGHT), bbx\n",
    "        if bbx is None:\n",
    "            return img, mask\n",
    "        return img, mask, bbx\n",
    "\n",
    "\n",
    "\n",
    "# ===============================label tranforms============================\n",
    "\n",
    "class DeNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "        return tensor\n",
    "\n",
    "\n",
    "class LabelNormalize(object):\n",
    "    def __init__(self, para):\n",
    "        self.para = para\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        # tensor = 1./(tensor+self.para).log()\n",
    "        tensor = torch.from_numpy(np.array(tensor))\n",
    "        tensor = tensor*self.para\n",
    "        return tensor\n",
    "\n",
    "    \n",
    "class GTScaleDown(object):\n",
    "    def __init__(self, factor=8):\n",
    "        self.factor = factor\n",
    "\n",
    "    def __call__(self, img):\n",
    "        w, h = img.size\n",
    "        if self.factor==1:\n",
    "            return img\n",
    "        tmp = np.array(img.resize((w//self.factor, h//self.factor), Image.BICUBIC))*self.factor*self.factor\n",
    "        img = Image.fromarray(tmp)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer CSRNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRINT_FREQ = 1\n",
    "LOG_PARA = 100. # C'est quoi ce LOG_PARA ??\n",
    "seed = 1\n",
    "\n",
    "\n",
    "class Trainer_GT():\n",
    "    def __init__(self, dataloader, net, loss, optimizer, validation_frequency=1, max_epoch=100):\n",
    "        self.train_loader, self.val_loader, self.test_loader = dataloader()\n",
    "        self.net = net\n",
    "        self.loss = loss \n",
    "        self.optimizer = optimizer\n",
    "        self.best_mae = 1e20\n",
    "        self.best_mse = 1e20\n",
    "        self.epoch = 0\n",
    "        self.validation_frequency = validation_frequency\n",
    "        self.max_epoch = max_epoch\n",
    "\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(0, self.max_epoch):\n",
    "            self.epoch = epoch\n",
    "            # si on veut un lr sheduler il faut le mettre là\n",
    "                \n",
    "            # training    \n",
    "            self.train_epoch()\n",
    "\n",
    "            # validation\n",
    "            if epoch%self.validation_frequency==0:\n",
    "                self.validate()\n",
    "\n",
    "\n",
    "    def train_epoch(self): # training for all datasets\n",
    "        self.net.train()\n",
    "        \n",
    "        for step, data in enumerate(self.train_loader, 0):\n",
    "            img, gt_map = data\n",
    "            img = Variable(img).to(device)\n",
    "            gt_map = Variable(gt_map).to(device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            # pred_map = self.net(img, gt_map)\n",
    "            \n",
    "            pred_density_map = self.net(img)\n",
    "            loss = self.loss(pred_density_map, gt_map)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            gt_count = [int(gt_map[i].sum().data / LOG_PARA) for i in range(gt_map.size()[0])]\n",
    "            pre_count = [int(pred_density_map[i].sum().data/LOG_PARA) for i in range(pred_density_map.size()[0])]\n",
    "            \n",
    "            print(f'epoch: {self.epoch} | step: {step} | count: {gt_count} | prediction: {pre_count} | loss: {loss}') \n",
    "\n",
    "\n",
    "    def validate(self):\n",
    "\n",
    "        self.net.eval()\n",
    "        epoch_res = []\n",
    "\n",
    "        for vi, data in enumerate(self.val_loader, 0):\n",
    "            img, gt_map = data\n",
    "\n",
    "            with torch.no_grad():\n",
    "                img = Variable(img).to(device)\n",
    "                gt_map = Variable(gt_map).to(device)\n",
    "\n",
    "                pred_map = self.net.forward(img,gt_map)\n",
    "\n",
    "                pred_map = pred_map.data.cpu().numpy()\n",
    "                gt_map = gt_map.data.cpu().numpy()\n",
    "\n",
    "                for i_img in range(pred_map.shape[0]):\n",
    "                \n",
    "                    pred_cnt = np.sum(pred_map[i_img])/LOG_PARA\n",
    "                    gt_count = np.sum(gt_map[i_img])/LOG_PARA\n",
    "                    res = gt_count - pred_cnt\n",
    "                    \n",
    "                    epoch_res.append(res)\n",
    "\n",
    "\n",
    "        epoch_res = np.array(epoch_res)\n",
    "        mse = np.sqrt(np.mean(np.square(epoch_res)))\n",
    "        mae = np.mean(np.abs(epoch_res))\n",
    "        \n",
    "        print('Epoch {} Val, MSE: {:.2f} MAE: {:.2f}, Cost {:.1f} sec'\n",
    "                     .format(self.epoch, mse, mae, time.time()-epoch_start))\n",
    "\n",
    "        if (2.0 * mse + mae) < (2.0 * self.best_mse + self.best_mae):\n",
    "            self.best_mse = mse\n",
    "            self.best_mae = mae\n",
    "            print(\"save best mse {:.2f} mae {:.2f} model epoch {}\".format(self.best_mse,\n",
    "                                                                            self.best_mae,\n",
    "                                                                                 self.epoch))\n",
    "            torch.save(self.net.state_dict(), os.path.join(save_dir, 'best_model.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([5, 370, 768])) that is different to the input size (torch.Size([5, 1, 370, 768])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | step: 0 | count: [1008, 1735, 95, 283, 122] | prediction: [0, 0, 0, 0, 0] | loss: 0.24648582935333252\n",
      "epoch: 0 | step: 1 | count: [192, 103, 475, 140, 242] | prediction: [1, 1, 1, 1, 1] | loss: 0.05332471430301666\n",
      "epoch: 0 | step: 2 | count: [554, 95, 86, 329, 436] | prediction: [4, 4, 5, 3, 4] | loss: 0.05671855807304382\n",
      "epoch: 0 | step: 3 | count: [464, 398, 236, 316, 184] | prediction: [5, 5, 4, 6, 5] | loss: 0.07139339298009872\n",
      "epoch: 0 | step: 4 | count: [118, 175, 290, 608, 163] | prediction: [6, 8, 7, 6, 7] | loss: 0.041262730956077576\n",
      "epoch: 0 | step: 5 | count: [539, 249, 1600, 170, 266] | prediction: [5, 5, 4, 4, 7] | loss: 0.3592809736728668\n",
      "epoch: 0 | step: 6 | count: [527, 130, 255, 193, 243] | prediction: [10, 10, 13, 15, 12] | loss: 0.04474747180938721\n",
      "epoch: 0 | step: 7 | count: [334, 95, 868, 280, 346] | prediction: [14, 11, 13, 12, 11] | loss: 0.08001694083213806\n",
      "epoch: 0 | step: 8 | count: [1772, 18, 118, 407, 30] | prediction: [17, 12, 16, 16, 14] | loss: 0.09012211114168167\n",
      "epoch: 0 | step: 9 | count: [752, 212, 24, 175, 458] | prediction: [15, 21, 13, 19, 9] | loss: 0.05034290999174118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([5, 505, 768])) that is different to the input size (torch.Size([5, 1, 505, 768])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | step: 10 | count: [304, 103, 137, 150, 488] | prediction: [22, 17, 16, 16, 27] | loss: 0.045192405581474304\n",
      "epoch: 0 | step: 11 | count: [145, 236, 477, 536, 197] | prediction: [22, 31, 31, 28, 24] | loss: 0.08879166096448898\n",
      "epoch: 0 | step: 12 | count: [627, 157, 65, 142, 205] | prediction: [35, 19, 35, 30, 34] | loss: 0.055306170135736465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([5, 327, 768])) that is different to the input size (torch.Size([5, 1, 327, 768])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | step: 13 | count: [2034, 948, 191, 416, 73] | prediction: [21, 28, 20, 18, 13] | loss: 0.406492680311203\n",
      "epoch: 0 | step: 14 | count: [1020, 97, 94, 536, 533] | prediction: [41, 36, 50, 35, 44] | loss: 0.06993846595287323\n",
      "epoch: 0 | step: 15 | count: [347, 372, 390, 525, 156] | prediction: [70, 56, 61, 41, 71] | loss: 0.05186097323894501\n",
      "epoch: 0 | step: 16 | count: [976, 402, 210, 1841, 66] | prediction: [81, 71, 87, 73, 41] | loss: 0.298877477645874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([5, 545, 768])) that is different to the input size (torch.Size([5, 1, 545, 768])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | step: 17 | count: [785, 270, 170, 522, 292] | prediction: [35, 48, 55, 51, 67] | loss: 0.06866395473480225\n",
      "epoch: 0 | step: 18 | count: [153, 163, 973, 170, 314] | prediction: [108, 81, 65, 112, 93] | loss: 0.05780923366546631\n",
      "epoch: 0 | step: 19 | count: [176, 283, 146, 123, 325] | prediction: [113, 119, 111, 101, 123] | loss: 0.03081504814326763\n",
      "epoch: 0 | step: 20 | count: [459, 408, 130, 198, 153] | prediction: [72, 132, 88, 99, 120] | loss: 0.051570989191532135\n",
      "epoch: 0 | step: 21 | count: [523, 17, 202, 374, 149] | prediction: [133, 80, 167, 182, 145] | loss: 0.04956437647342682\n",
      "epoch: 0 | step: 22 | count: [1561, 1659, 334, 352, 181] | prediction: [115, 181, 192, 197, 140] | loss: 0.24997875094413757\n",
      "epoch: 0 | step: 23 | count: [274, 332, 156, 314, 160] | prediction: [143, 39, 125, 141, 138] | loss: 0.12013116478919983\n",
      "epoch: 0 | step: 24 | count: [132, 277, 184, 510, 935] | prediction: [225, 234, 197, 153, 252] | loss: 0.1000724583864212\n",
      "epoch: 0 | step: 25 | count: [126, 449, 288, 127, 343] | prediction: [208, 393, 234, 313, 288] | loss: 0.0487472303211689\n",
      "epoch: 0 | step: 26 | count: [115, 91, 212, 223, 400] | prediction: [262, 273, 281, 229, 333] | loss: 0.03458483889698982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([5, 475, 768])) that is different to the input size (torch.Size([5, 1, 475, 768])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | step: 27 | count: [780, 399, 135, 379, 35] | prediction: [384, 380, 270, 332, 241] | loss: 0.06135076656937599\n",
      "epoch: 0 | step: 28 | count: [377, 735, 479, 205, 150] | prediction: [392, 440, 374, 387, 411] | loss: 0.09669691324234009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([5, 444, 768])) that is different to the input size (torch.Size([5, 1, 444, 768])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | step: 29 | count: [266, 402, 458, 510, 45] | prediction: [303, 414, 408, 562, 247] | loss: 0.05672556534409523\n",
      "epoch: 0 | step: 30 | count: [44, 160, 81, 428, 181] | prediction: [405, 439, 366, 530, 463] | loss: 0.037848543375730515\n",
      "epoch: 0 | step: 31 | count: [225, 988, 259, 419, 300] | prediction: [328, 751, 488, 534, 272] | loss: 0.06590647995471954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([5, 530, 768])) that is different to the input size (torch.Size([5, 1, 530, 768])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | step: 32 | count: [88, 67, 376, 469, 1437] | prediction: [327, 381, 431, 437, 551] | loss: 0.0841480940580368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([5, 414, 768])) that is different to the input size (torch.Size([5, 1, 414, 768])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | step: 33 | count: [467, 99, 40, 19, 211] | prediction: [317, 252, 252, 165, 192] | loss: 0.036832548677921295\n",
      "epoch: 0 | step: 34 | count: [182, 1314, 418, 243, 373] | prediction: [261, 336, 378, 379, 309] | loss: 0.18635497987270355\n",
      "epoch: 0 | step: 35 | count: [620, 192, 392, 484, 209] | prediction: [335, 342, 345, 352, 274] | loss: 0.058924492448568344\n",
      "epoch: 0 | step: 36 | count: [864, 44, 587, 244, 38] | prediction: [268, 304, 290, 344, 217] | loss: 0.07973488420248032\n",
      "epoch: 0 | step: 37 | count: [225, 905, 1021, 103, 682] | prediction: [319, 515, 478, 188, 425] | loss: 0.07939648628234863\n",
      "epoch: 0 | step: 38 | count: [705, 282, 566, 275, 389] | prediction: [343, 381, 414, 478, 454] | loss: 0.06150476634502411\n",
      "epoch: 0 | step: 39 | count: [481, 269, 343, 266, 103] | prediction: [325, 343, 353, 385, 316] | loss: 0.04098447039723396\n",
      "epoch: 0 | step: 40 | count: [282, 224, 108, 551, 323] | prediction: [282, 344, 267, 376, 404] | loss: 0.04096958413720131\n",
      "epoch: 0 | step: 41 | count: [78, 673, 132, 686, 212] | prediction: [362, 260, 261, 255, 309] | loss: 0.09335413575172424\n",
      "epoch: 0 | step: 42 | count: [430, 1130, 190, 503, 99] | prediction: [345, 234, 269, 385, 282] | loss: 0.1219441294670105\n",
      "epoch: 0 | step: 43 | count: [9, 286, 128, 690, 250] | prediction: [179, 257, 233, 272, 238] | loss: 0.09578154981136322\n",
      "epoch: 0 | step: 44 | count: [179, 701, 522, 243, 746] | prediction: [408, 419, 581, 322, 386] | loss: 0.08977682888507843\n",
      "epoch: 0 | step: 45 | count: [266, 248, 83, 230, 159] | prediction: [337, 379, 227, 277, 433] | loss: 0.03233179450035095\n",
      "epoch: 0 | step: 46 | count: [285, 144, 74, 119, 203] | prediction: [294, 431, 312, 311, 318] | loss: 0.024665100499987602\n",
      "epoch: 0 | step: 47 | count: [84, 1286, 167, 238, 192] | prediction: [307, 347, 319, 349, 342] | loss: 0.0822739526629448\n",
      "epoch: 0 | step: 48 | count: [177, 398, 376, 68, 111] | prediction: [303, 384, 398, 366, 407] | loss: 0.04812537133693695\n",
      "epoch: 0 | step: 49 | count: [106, 513, 255, 229, 957] | prediction: [241, 257, 319, 452, 428] | loss: 0.09225111454725266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([5, 562, 768])) that is different to the input size (torch.Size([5, 1, 562, 768])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | step: 50 | count: [493, 296, 297, 186, 387] | prediction: [309, 432, 311, 340, 381] | loss: 0.047933731228113174\n",
      "epoch: 0 | step: 51 | count: [325, 326, 826, 305, 526] | prediction: [280, 238, 309, 288, 444] | loss: 0.07453327625989914\n",
      "epoch: 0 | step: 52 | count: [185, 528, 477, 1130, 148] | prediction: [356, 360, 377, 472, 307] | loss: 0.10074475407600403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([5, 521, 768])) that is different to the input size (torch.Size([5, 1, 521, 768])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | step: 53 | count: [695, 1303, 152, 969, 564] | prediction: [455, 204, 313, 399, 314] | loss: 0.302240788936615\n",
      "epoch: 0 | step: 54 | count: [191, 347, 149, 637, 0] | prediction: [207, 159, 261, 196, 99] | loss: 0.07018369436264038\n",
      "epoch: 0 | step: 55 | count: [325, 201, 1945, 174, 57] | prediction: [417, 414, 529, 319, 340] | loss: 0.1681775003671646\n",
      "epoch: 0 | step: 56 | count: [130, 304, 25, 71, 23] | prediction: [263, 334, 185, 211, 235] | loss: 0.026730848476290703\n",
      "epoch: 0 | step: 57 | count: [183, 127, 301, 70, 684] | prediction: [427, 384, 345, 379, 561] | loss: 0.05325204133987427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([5, 522, 768])) that is different to the input size (torch.Size([5, 1, 522, 768])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | step: 58 | count: [89, 569, 83, 108, 100] | prediction: [446, 438, 242, 393, 336] | loss: 0.03392822667956352\n",
      "epoch: 0 | step: 59 | count: [533, 603, 228, 268, 323] | prediction: [656, 400, 524, 498, 460] | loss: 0.06707252562046051\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-91e3df2040b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mgt_trainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer_GT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloading_data_GT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mgt_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-93-0b01a657643f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m# validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_frequency\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"my_model_weigths_GT.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-93-0b01a657643f>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mmses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAverageMeter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mvi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Launch GT Train !\n",
    "lr = 1e-5 \n",
    "\n",
    "net = CSRNet().to(device)\n",
    "loss = nn.MSELoss().to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=1e-4)\n",
    "# optimizer = optim.SGD(self.net.parameters(), cfg.LR, momentum=0.95,weight_decay=5e-4)  \n",
    "\n",
    "gt_trainer = Trainer_GT(loading_data_GT, net, loss, optimizer)\n",
    "gt_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models parameters for Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# image = Image.open('../ProcessedData/SHHA/train/IMG_1.jpg')\n",
    "# trans1 = transforms.ToTensor()\n",
    "# img = trans1(image).to(device)\n",
    "# img = img.unsqueeze(0)\n",
    "\n",
    "# write to tensorboard\n",
    "\n",
    "# writer.add_graph(model, img)\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_epoch = 1\n",
    "val_start = 0\n",
    "save_dir = \"\"\n",
    "\n",
    "class Trainer_Bayes():\n",
    "    def __init__(self, dataloader, net, loss, optimizer,  validation_frequency=1, max_epoch=100):\n",
    "        self.train_loader, self.val_loader, self.test_loader = dataloader()\n",
    "        self.net = net\n",
    "        self.loss = loss \n",
    "        self.optimizer = optimizer\n",
    "        self.best_mae = 1e20\n",
    "        self.best_mse = 1e20\n",
    "        self.epoch = 0\n",
    "        self.validation_frequency = validation_frequency\n",
    "        self.max_epoch = max_epoch\n",
    "        \n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(0, self.max_epoch):\n",
    "            self.epoch = epoch\n",
    "\n",
    "            # training    \n",
    "            self.train_epoch()\n",
    "\n",
    "            # validation\n",
    "            if epoch % self.validation_frequency == 0:\n",
    "                self.validate()\n",
    "                \n",
    "        print(f'Train finished | best_mse: {self.best_mse} | best_mae: {self.bast_mae})\n",
    "                \n",
    "\n",
    "    def train_epoch(self):\n",
    "        self.net.train()  # Set model to training mode\n",
    "        # running_loss = 0.0\n",
    "        # Iterate over data.\n",
    "        for step, (inputs, points, targets, st_sizes) in enumerate(self.train_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            st_sizes = st_sizes.to(device)\n",
    "            gd_count = np.array([len(p) for p in points], dtype=np.float32)\n",
    "            points = [p.to(device) for p in points]\n",
    "            targets = [t.to(device) for t in targets]\n",
    "            \n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = self.net(inputs)\n",
    "                prob_list = post_prob(points, st_sizes)\n",
    "                loss = self.loss(prob_list, targets, outputs)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                N = inputs.size(0) # batch size\n",
    "                pre_count = torch.sum(outputs.view(N, -1), dim=1).detach().cpu().numpy()\n",
    "                res = pre_count - gd_count\n",
    "\n",
    "                print(f'epoch: {self.epoch} | step: {step} | gd_count: {gd_count} | prediction: {pre_count} | loss: {loss}')\n",
    "\n",
    "                # running_loss += loss.item()\n",
    "                # if step % 2 == 1:\n",
    "                #   writer.add_scalar('training loss vgg',\n",
    "                #               running_loss / 2,\n",
    "                #               self.epoch * len(self.train_loader) + step)\n",
    "                #   running_loss = 0.0\n",
    "\n",
    "    def validate(self):\n",
    "        epoch_start = time.time()\n",
    "        self.net.eval()  # Set model to evaluate mode\n",
    "        epoch_res = []\n",
    "\n",
    "        # Iterate over data.\n",
    "        for inputs, count, name in self.val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            # inputs are images with different sizes\n",
    "            assert inputs.size(0) == 1, 'the batch size should equal to 1 in validation mode'\n",
    "            with torch.set_grad_enabled(False):\n",
    "                outputs = self.net(inputs)\n",
    "                res = count[0].item() - torch.sum(outputs).item()\n",
    "                epoch_res.append(res)\n",
    "\n",
    "\n",
    "        epoch_res = np.array(epoch_res)\n",
    "        mse = np.sqrt(np.mean(np.square(epoch_res)))\n",
    "        mae = np.mean(np.abs(epoch_res))\n",
    "\n",
    "        # ...log the running loss\n",
    "        writer.add_scalar('val MAE vgg',\n",
    "                            mae,\n",
    "                            self.epoch * len(self.val_loader))\n",
    "        writer.add_scalar('val MSE vgg',\n",
    "                        mse,\n",
    "                        self.epoch * len(self.val_loader))\n",
    "\n",
    "        print('Epoch {} Val, MSE: {:.2f} MAE: {:.2f}, Cost {:.1f} sec'\n",
    "                     .format(self.epoch, mse, mae, time.time()-epoch_start))\n",
    "\n",
    "        if (2.0 * mse + mae) < (2.0 * self.best_mse + self.best_mae):\n",
    "            self.best_mse = mse\n",
    "            self.best_mae = mae\n",
    "            print(\"save best mse {:.2f} mae {:.2f} model epoch {}\".format(self.best_mse,\n",
    "                                                                            self.best_mae,\n",
    "                                                                                 self.epoch))\n",
    "            torch.save(self.net.state_dict(), os.path.join(save_dir, 'best_model_vgg.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | step: 0 | gd_count: [ 26. 165.  42.  39. 191.] | prediction: [36.423897 40.730583 36.419674 41.389618 20.366959] | loss: 76.236572265625\n",
      "epoch: 0 | step: 1 | gd_count: [ 10. 434.  73. 119.  63.] | prediction: [24.99488  44.843002 29.235348 52.549484 39.237038] | loss: 114.30181884765625\n",
      "epoch: 0 | step: 2 | gd_count: [37. 23. 14. 71. 69.] | prediction: [38.385216 32.69168  28.70775  50.60098  48.23176 ] | loss: 34.03035354614258\n",
      "epoch: 0 | step: 3 | gd_count: [136.  53.  99.  41.  59.] | prediction: [37.123306 29.79672  32.597652 32.10839  30.59602 ] | loss: 51.994483947753906\n",
      "epoch: 0 | step: 4 | gd_count: [  0.  41.  18. 157.  31.] | prediction: [43.15415  34.915127 23.583668 52.186546 36.662582] | loss: 38.72573471069336\n",
      "epoch: 0 | step: 5 | gd_count: [33. 58. 29. 13. 10.] | prediction: [31.520376 53.447353 38.144714 27.577461 18.449669] | loss: 16.662334442138672\n",
      "epoch: 0 | step: 6 | gd_count: [ 26.  22.  43.  87. 110.] | prediction: [30.84146  24.148678 36.868137 31.855232 54.040333] | loss: 29.527088165283203\n",
      "epoch: 0 | step: 7 | gd_count: [131.  40.  11.  50.  10.] | prediction: [40.280914 30.173296 20.532896 30.515278 30.767832] | loss: 35.517547607421875\n",
      "epoch: 0 | step: 8 | gd_count: [107. 217.  90.   3.  12.] | prediction: [48.374306 45.16922  53.267853 17.320019 28.126286] | loss: 56.379913330078125\n",
      "epoch: 0 | step: 9 | gd_count: [  3. 123.  75.  14.   8.] | prediction: [15.155836 58.15878  60.544514 25.02589  23.709915] | loss: 28.24287986755371\n",
      "epoch: 0 | step: 10 | gd_count: [178.  27.  50.  73.  37.] | prediction: [69.58469  25.563753 25.011131 39.357056 25.773342] | loss: 42.591880798339844\n",
      "epoch: 0 | step: 11 | gd_count: [12. 89. 41. 31. 37.] | prediction: [20.551348 45.89944  30.09977  36.00418  29.987366] | loss: 21.995182037353516\n",
      "epoch: 0 | step: 12 | gd_count: [74. 47. 13. 63. 20.] | prediction: [67.937744 72.34407  22.529612 55.365585 28.003128] | loss: 22.809823989868164\n",
      "epoch: 0 | step: 13 | gd_count: [10. 86. 25.  2. 84.] | prediction: [22.110664 50.127045 31.237183 19.34687  62.202675] | loss: 22.829076766967773\n",
      "epoch: 0 | step: 14 | gd_count: [ 30. 471.  18.  39.  64.] | prediction: [30.0813   77.140625 28.713526 48.00978  51.958977] | loss: 93.98084259033203\n",
      "epoch: 0 | step: 15 | gd_count: [125.  22.  96.  34.  36.] | prediction: [83.66754  36.344955 84.37574  32.157513 43.03869 ] | loss: 27.9473819732666\n",
      "epoch: 0 | step: 16 | gd_count: [ 44.  50. 334. 176.  77.] | prediction: [ 58.84795   48.29146  100.169174 102.19766   50.26921 ] | loss: 79.6752700805664\n",
      "epoch: 0 | step: 17 | gd_count: [ 39.  44. 154.  69.   9.] | prediction: [ 40.902214  46.068977 108.181816  62.977737  27.399317] | loss: 27.934850692749023\n",
      "epoch: 0 | step: 18 | gd_count: [132.  47. 125.  26.  24.] | prediction: [64.09638  38.978622 38.576614 27.556747 18.637482] | loss: 38.506317138671875\n",
      "epoch: 0 | step: 19 | gd_count: [ 37. 109.  31.   4. 145.] | prediction: [ 41.598953  73.20926   25.732306  25.236813 120.342964] | loss: 38.496620178222656\n",
      "epoch: 0 | step: 20 | gd_count: [100.  14. 156. 234.  45.] | prediction: [ 77.127884  21.754059 105.823456 109.069534  46.254753] | loss: 52.62225341796875\n",
      "epoch: 0 | step: 21 | gd_count: [ 19.  72.  23. 189. 120.] | prediction: [ 16.54539   57.19063   19.790108 124.72287  106.82258 ] | loss: 35.1423454284668\n",
      "epoch: 0 | step: 22 | gd_count: [ 14.  73. 141.  52. 142.] | prediction: [ 21.306858  83.25554   93.056564  19.178654 146.79462 ] | loss: 43.27394485473633\n",
      "epoch: 0 | step: 23 | gd_count: [26. 17. 94. 31. 86.] | prediction: [23.825563 16.160458 56.29235  27.15949  63.91257 ] | loss: 21.575748443603516\n",
      "epoch: 0 | step: 24 | gd_count: [  1.   1.  24.  36. 125.] | prediction: [12.574663 14.701305 34.640625 62.047424 94.08714 ] | loss: 26.652978897094727\n",
      "epoch: 0 | step: 25 | gd_count: [ 38.  50.   8.  25. 447.] | prediction: [ 51.16713   52.886696  14.594607  13.894946 174.28009 ] | loss: 68.75096130371094\n",
      "epoch: 0 | step: 26 | gd_count: [49. 94.  6. 76. 81.] | prediction: [59.345207 56.148224 13.713758 41.808167 41.520813] | loss: 30.855682373046875\n",
      "epoch: 0 | step: 27 | gd_count: [252.   9. 183.  55.   6.] | prediction: [126.81179    9.715306 116.6836    27.453197   8.980427] | loss: 50.52094650268555\n",
      "epoch: 0 | step: 28 | gd_count: [23. 11. 31.  0. 28.] | prediction: [15.991703 14.646797 25.589165 16.534292 24.598612] | loss: 12.227113723754883\n",
      "epoch: 0 | step: 29 | gd_count: [  3.  24.  18.  20. 400.] | prediction: [  8.765898  29.542435  14.361145  21.353323 130.51057 ] | loss: 61.95052719116211\n",
      "epoch: 0 | step: 30 | gd_count: [58.  0. 35. 98. 11.] | prediction: [ 88.81305   15.400672  41.41835  103.44071   14.970625] | loss: 24.97294807434082\n",
      "epoch: 0 | step: 31 | gd_count: [526.   8. 127.  25. 325.] | prediction: [488.4804    11.27137   95.94241   30.678246  73.33548 ] | loss: 109.35357666015625\n",
      "epoch: 0 | step: 32 | gd_count: [  3.  28.  62. 157.  65.] | prediction: [  6.4863434  19.66109    41.30047   150.87546    63.421795 ] | loss: 26.727216720581055\n",
      "epoch: 0 | step: 33 | gd_count: [121.  14.   7.  64.  15.] | prediction: [113.89316    14.282507   25.194302   39.20346    14.2427845] | loss: 18.35063362121582\n",
      "epoch: 0 | step: 34 | gd_count: [ 31.  24.  12.   5. 190.] | prediction: [17.813145  29.074549  16.59766   14.5410795 80.67829  ] | loss: 29.320205688476562\n",
      "epoch: 0 | step: 35 | gd_count: [33. 97.  0. 32. 36.] | prediction: [17.187355 69.70667  12.638064 43.7725   20.639637] | loss: 21.74814796447754\n",
      "epoch: 0 | step: 36 | gd_count: [54. 13. 26. 40. 36.] | prediction: [38.601788 12.610954 20.167612 39.346947 23.92059 ] | loss: 12.475542068481445\n",
      "epoch: 0 | step: 37 | gd_count: [23. 46. 26.  4. 35.] | prediction: [25.76067  29.806862 22.130745 13.636621 16.664406] | loss: 15.672804832458496\n",
      "epoch: 0 | step: 38 | gd_count: [28. 93. 42. 53. 41.] | prediction: [21.26717  62.180344 24.488026 16.900457 28.132908] | loss: 20.605165481567383\n",
      "epoch: 0 | step: 39 | gd_count: [18. 87. 25. 30. 36.] | prediction: [19.590178 50.440186 19.996162 18.604263 16.749187] | loss: 21.50078582763672\n",
      "epoch: 0 | step: 40 | gd_count: [ 16.   9. 419.  13.   0.] | prediction: [14.141629 17.611732 68.34738  16.148361 13.670744] | loss: 74.82889556884766\n",
      "epoch: 0 | step: 41 | gd_count: [ 21.  93. 209. 196.  65.] | prediction: [ 30.209085  61.599823 126.28604   91.48294   30.322994] | loss: 56.234291076660156\n",
      "epoch: 0 | step: 42 | gd_count: [ 81.  10.  57. 115.  36.] | prediction: [74.53024  15.267433 53.795296 87.77377  28.452412] | loss: 21.804380416870117\n",
      "epoch: 0 | step: 43 | gd_count: [ 93.  17.  48. 133.  45.] | prediction: [111.08962   23.543497  44.11003  128.89688   31.311638] | loss: 31.533353805541992\n",
      "epoch: 0 | step: 44 | gd_count: [  8.  26. 121.  90.  37.] | prediction: [  7.049003  29.706577  97.41162  130.20172   35.703217] | loss: 24.411014556884766\n",
      "epoch: 0 | step: 45 | gd_count: [ 73.  43.   2. 146. 205.] | prediction: [ 19.640554   31.245022    7.7919116 132.81125    60.393974 ] | loss: 55.30836868286133\n",
      "epoch: 0 | step: 46 | gd_count: [ 1. 26. 96. 71. 26.] | prediction: [ 15.670077  26.124992 114.24777   41.102615  18.34093 ] | loss: 18.87818145751953\n",
      "epoch: 0 | step: 47 | gd_count: [ 45. 135.  12.  16.] | prediction: [ 42.89927  119.78989   11.982665  17.513182] | loss: 17.908138275146484\n",
      "Epoch 0 Val, MSE: 307.90 MAE: 187.56, Cost 6.6 sec\n",
      "save best mse 307.90 mae 187.56 model epoch 0\n",
      "epoch: 1 | step: 0 | gd_count: [ 18.  49.  61. 142. 140.] | prediction: [10.130787 24.33664  66.39801  93.34425  91.04144 ] | loss: 31.097335815429688\n",
      "epoch: 1 | step: 1 | gd_count: [  5.  91.  12. 168.  34.] | prediction: [11.316274 40.874565 12.282936 88.125275 18.474586] | loss: 31.61187744140625\n",
      "epoch: 1 | step: 2 | gd_count: [ 24. 264.   1.  47.  56.] | prediction: [ 30.057188 127.097015   8.168021  32.16562   48.843533] | loss: 42.09324645996094\n",
      "epoch: 1 | step: 3 | gd_count: [ 23. 140.  49.  10.   9.] | prediction: [24.02251  85.82937  35.569427  7.095131  8.586622] | loss: 18.58614730834961\n",
      "epoch: 1 | step: 4 | gd_count: [ 43.   3. 185.  68. 212.] | prediction: [ 36.955574  19.595554  89.1164    36.803535 105.622955] | loss: 59.877540588378906\n",
      "epoch: 1 | step: 5 | gd_count: [35. 60. 19.  7. 28.] | prediction: [16.04262   52.817745  15.307632   8.0297785 28.559338 ] | loss: 10.753181457519531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | step: 6 | gd_count: [ 9.  5. 60. 30. 32.] | prediction: [13.277039 10.364214 36.015953 17.872883 30.603231] | loss: 10.792521476745605\n",
      "epoch: 1 | step: 7 | gd_count: [  0. 136.  13.  43. 117.] | prediction: [12.016462 87.62645   9.829139 22.4534   62.142426] | loss: 27.37335205078125\n",
      "epoch: 1 | step: 8 | gd_count: [48. 52. 95. 12. 32.] | prediction: [32.62148  38.836685 66.38014  12.617369 36.335808] | loss: 15.811123847961426\n",
      "epoch: 1 | step: 9 | gd_count: [ 70.  22.  37.  73. 239.] | prediction: [ 70.12067   19.894981  37.865753  77.31476  103.874466] | loss: 42.35724639892578\n",
      "epoch: 1 | step: 10 | gd_count: [ 64.   8.  81.  91. 109.] | prediction: [31.145962   6.7214155 70.45787   61.05384   78.978584 ] | loss: 29.08650779724121\n",
      "epoch: 1 | step: 11 | gd_count: [62. 84. 66. 62. 47.] | prediction: [165.28787   71.13586   93.53144   46.03423   49.518402] | loss: 44.87816619873047\n",
      "epoch: 1 | step: 12 | gd_count: [31. 76. 76. 72. 15.] | prediction: [24.06876  48.332756 84.551956 69.308014 12.329338] | loss: 20.040660858154297\n",
      "epoch: 1 | step: 13 | gd_count: [62.  9. 65.  2. 69.] | prediction: [47.19413  16.773825 53.316017  7.624811 77.26519 ] | loss: 18.309240341186523\n",
      "epoch: 1 | step: 14 | gd_count: [528.  72.  70.  82.  13.] | prediction: [238.55385   49.496735  44.73887   66.6039    21.921131] | loss: 82.41333770751953\n",
      "epoch: 1 | step: 15 | gd_count: [ 21. 191.  52.  12.   3.] | prediction: [ 18.904633 116.70069   49.413216  40.424736   9.734592] | loss: 27.43500328063965\n",
      "epoch: 1 | step: 16 | gd_count: [138.   6. 144.  46.  46.] | prediction: [ 84.26976    9.775904 107.774734  27.803902  35.850075] | loss: 30.389951705932617\n",
      "epoch: 1 | step: 17 | gd_count: [114.  76.  12. 334.  11.] | prediction: [ 77.10368   53.292763  13.477234 203.85712    8.756018] | loss: 47.508567810058594\n",
      "epoch: 1 | step: 18 | gd_count: [ 27.  87.  37. 143.   6.] | prediction: [22.500774 79.64297  18.516415 90.4138    9.806005] | loss: 22.132810592651367\n",
      "epoch: 1 | step: 19 | gd_count: [103.  14.  14.  28.  49.] | prediction: [63.695488 15.776131 12.551516 27.338099 36.465164] | loss: 16.95427894592285\n",
      "epoch: 1 | step: 20 | gd_count: [31.  5. 14. 79.  0.] | prediction: [28.959068  9.654655 21.597197 88.72322  11.795146] | loss: 14.259370803833008\n",
      "epoch: 1 | step: 21 | gd_count: [117.  17.  91.  26.  77.] | prediction: [112.514755  15.277862  70.1537    27.962536  54.918915] | loss: 23.616376876831055\n",
      "epoch: 1 | step: 22 | gd_count: [187. 262.  50.  16.  35.] | prediction: [159.7541   211.49141   43.982468  10.657984  33.477737] | loss: 48.704044342041016\n",
      "epoch: 1 | step: 23 | gd_count: [  1.  12.  11. 134.  14.] | prediction: [  6.761904  12.516624   9.458626 159.80971   17.324097] | loss: 15.206751823425293\n",
      "epoch: 1 | step: 24 | gd_count: [  0. 232.  68.  31.  11.] | prediction: [  7.031418 197.93181   52.706966  26.33512    9.28124 ] | loss: 29.956327438354492\n",
      "epoch: 1 | step: 25 | gd_count: [723. 166.  29.  75.  39.] | prediction: [273.2025   114.240005  20.395992  66.850586  40.022804] | loss: 120.60689544677734\n",
      "epoch: 1 | step: 26 | gd_count: [ 88.  38. 123.   5.  67.] | prediction: [ 44.773155  30.507275 141.84808   11.584337  47.162575] | loss: 27.48152732849121\n",
      "epoch: 1 | step: 27 | gd_count: [ 55.  18. 282.  87. 175.] | prediction: [ 34.884464  12.875807 179.52518   34.764748  88.365524] | loss: 52.893001556396484\n",
      "epoch: 1 | step: 28 | gd_count: [252.   3. 205.  13.   0.] | prediction: [225.28358     7.6091204  54.83184    12.482685    9.913678 ] | loss: 51.033843994140625\n",
      "epoch: 1 | step: 29 | gd_count: [22. 16.  2.  3. 30.] | prediction: [17.789967  16.411442   7.1647973  6.632028  18.817385 ] | loss: 6.6851935386657715\n",
      "epoch: 1 | step: 30 | gd_count: [85. 67. 97. 52.  5.] | prediction: [77.49159  48.9142   58.586    47.64859  14.558836] | loss: 20.045665740966797\n",
      "epoch: 1 | step: 31 | gd_count: [20. 52. 65. 38. 83.] | prediction: [11.175973 52.248405 80.315155 41.47792  54.362823] | loss: 21.270309448242188\n",
      "epoch: 1 | step: 32 | gd_count: [13. 10.  0. 27. 29.] | prediction: [15.205203 12.047028  7.996755 32.435463 38.44261 ] | loss: 8.845848083496094\n",
      "epoch: 1 | step: 33 | gd_count: [  9.  38. 221. 284.  81.] | prediction: [ 14.657018  13.739634 178.37326  162.7973    61.97865 ] | loss: 56.00395584106445\n",
      "epoch: 1 | step: 34 | gd_count: [ 97. 206. 207. 127.  55.] | prediction: [145.27411  135.47485  160.23761  122.853806  51.279957] | loss: 59.27693557739258\n",
      "epoch: 1 | step: 35 | gd_count: [28. 94.  0. 60.  8.] | prediction: [31.76717   72.04416   10.03438   50.881386   7.6547937] | loss: 14.596781730651855\n",
      "epoch: 1 | step: 36 | gd_count: [ 78.  29. 114. 131.  19.] | prediction: [63.27121  28.314186 92.47662  57.53499  13.706724] | loss: 29.494373321533203\n",
      "epoch: 1 | step: 37 | gd_count: [343.  11.  23. 111. 130.] | prediction: [178.35788   12.545519  22.61282   83.10921  154.60623 ] | loss: 59.0041618347168\n",
      "epoch: 1 | step: 38 | gd_count: [ 57. 305.   1.  50.  59.] | prediction: [ 50.337914 311.64807    8.15181   38.972336  43.536224] | loss: 34.651065826416016\n",
      "epoch: 1 | step: 39 | gd_count: [  5.   7.  47.  59. 122.] | prediction: [ 7.5808077 10.649518  34.71763   48.153057  92.29532  ] | loss: 15.822624206542969\n",
      "epoch: 1 | step: 40 | gd_count: [ 98.  43.  45. 103.  92.] | prediction: [65.07072  24.045055 27.456705 70.79643  65.774826] | loss: 28.93313980102539\n",
      "epoch: 1 | step: 41 | gd_count: [ 9. 31.  0. 10. 61.] | prediction: [11.145246 25.27238  15.514158 10.035251 53.82035 ] | loss: 10.333927154541016\n",
      "epoch: 1 | step: 42 | gd_count: [  0. 112. 142.  52.  55.] | prediction: [11.488094 78.88118  99.57597  32.527245 39.646484] | loss: 29.514184951782227\n",
      "epoch: 1 | step: 43 | gd_count: [  6.  87.  22. 168.  10.] | prediction: [  9.132791  84.30316   24.97499  103.76209   17.160149] | loss: 25.55727195739746\n",
      "epoch: 1 | step: 44 | gd_count: [ 20.  11. 315.  56.   0.] | prediction: [ 22.457561  16.891512 222.07097   72.99711    9.699641] | loss: 32.78144836425781\n",
      "epoch: 1 | step: 45 | gd_count: [ 56.  12. 145. 117. 209.] | prediction: [ 55.046593  11.699151 100.22072  127.30994  150.98605 ] | loss: 42.07111740112305\n",
      "epoch: 1 | step: 46 | gd_count: [29. 37.  8. 43.  6.] | prediction: [33.948463 41.568027  8.769247 45.4013    8.473656] | loss: 9.286158561706543\n",
      "epoch: 1 | step: 47 | gd_count: [ 50. 200.  32. 425.] | prediction: [ 39.09874  174.10165   35.558426 153.3438  ] | loss: 97.58029174804688\n",
      "Epoch 1 Val, MSE: 238.21 MAE: 129.27, Cost 6.7 sec\n",
      "save best mse 238.21 mae 129.27 model epoch 1\n",
      "epoch: 2 | step: 0 | gd_count: [  2.  34. 312. 166.  10.] | prediction: [  6.7970405  31.838593  119.086075   39.568634    8.777304 ] | loss: 66.9336929321289\n",
      "epoch: 2 | step: 1 | gd_count: [23.  8. 89. 37. 29.] | prediction: [15.603296  9.663335 88.556046 27.231186 17.559158] | loss: 14.644268989562988\n",
      "epoch: 2 | step: 2 | gd_count: [68. 91. 28. 29. 35.] | prediction: [41.632698 78.88614  32.385593 18.100718 39.056183] | loss: 15.40632152557373\n",
      "epoch: 2 | step: 3 | gd_count: [ 32. 269. 246.  30.  17.] | prediction: [ 16.539299 227.6311   167.57169   22.006056  14.120552] | loss: 48.305145263671875\n",
      "epoch: 2 | step: 4 | gd_count: [29.  2. 48.  6. 67.] | prediction: [25.801962  7.005436 41.103134  7.739137 78.03204 ] | loss: 12.437095642089844\n",
      "epoch: 2 | step: 5 | gd_count: [19. 30.  2. 41. 12.] | prediction: [12.762986  30.276394   4.340438  25.199512  11.0552635] | loss: 8.182438850402832\n",
      "epoch: 2 | step: 6 | gd_count: [100.   8.   8.  57.  52.] | prediction: [75.45467    6.7041736  7.979247  42.039944  43.959984 ] | loss: 14.752174377441406\n",
      "epoch: 2 | step: 7 | gd_count: [ 24. 222.  51.  28.  58.] | prediction: [ 17.233881 193.26176   43.296906  19.249363  40.585716] | loss: 26.947357177734375\n",
      "epoch: 2 | step: 8 | gd_count: [  7. 158.  47. 129.  87.] | prediction: [ 26.570778 141.14241   32.4635    73.467125  79.89157 ] | loss: 41.58228302001953\n",
      "epoch: 2 | step: 9 | gd_count: [76.  0. 25. 45. 11.] | prediction: [40.070618  8.11386  18.534836 28.429153 12.886171] | loss: 13.852572441101074\n",
      "epoch: 2 | step: 10 | gd_count: [ 19. 112. 119.  13. 104.] | prediction: [ 16.476952  67.83902   83.04979   18.764067 114.27121 ] | loss: 26.6031551361084\n",
      "epoch: 2 | step: 11 | gd_count: [19. 97. 12. 22. 18.] | prediction: [10.820485  60.187576   7.0612383 14.967752  21.262762 ] | loss: 11.208014488220215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 | step: 12 | gd_count: [ 0. 83. 40.  2. 45.] | prediction: [ 4.6239   59.375786 19.483543  9.33033  26.645805] | loss: 14.38024616241455\n",
      "epoch: 2 | step: 13 | gd_count: [ 69.   4. 179. 190.  94.] | prediction: [ 55.54917    6.800326 155.90295  168.2337    89.121254] | loss: 34.333866119384766\n",
      "epoch: 2 | step: 14 | gd_count: [21. 15. 12. 20. 30.] | prediction: [27.156    19.150421 11.761578  9.015396 33.710014] | loss: 8.38090991973877\n",
      "epoch: 2 | step: 15 | gd_count: [65. 56. 28. 80. 72.] | prediction: [54.270115 54.350555 31.174852 53.39751  67.70145 ] | loss: 19.93996810913086\n",
      "epoch: 2 | step: 16 | gd_count: [ 63. 147.  43.  46. 314.] | prediction: [ 46.698597 103.59923   29.996552  58.60714  276.2155  ] | loss: 45.698177337646484\n",
      "epoch: 2 | step: 17 | gd_count: [ 12.  45. 109. 581.  41.] | prediction: [  6.248576  37.05198   85.98068  407.07516   29.79794 ] | loss: 57.727508544921875\n",
      "epoch: 2 | step: 18 | gd_count: [  8.  10. 141. 190.  57.] | prediction: [  7.7543097  12.220383   42.747555  130.74313    38.817924 ] | loss: 36.03125\n",
      "epoch: 2 | step: 19 | gd_count: [ 21.  36.  52. 274.  24.] | prediction: [ 26.579987  32.83091   39.056572 238.73038   18.76623 ] | loss: 25.62076759338379\n",
      "epoch: 2 | step: 20 | gd_count: [  7.  76.  15.   8. 452.] | prediction: [  6.478163  86.42157   10.644281   8.202682 286.43216 ] | loss: 49.333255767822266\n",
      "epoch: 2 | step: 21 | gd_count: [ 27.  51.  36. 193. 211.] | prediction: [ 39.435833  43.987034  20.602913 181.07144  161.52274 ] | loss: 39.47600173950195\n",
      "epoch: 2 | step: 22 | gd_count: [ 64. 157. 122.  86.  34.] | prediction: [ 65.478355 151.9331   100.99261   90.225716  31.847046] | loss: 32.48983383178711\n",
      "epoch: 2 | step: 23 | gd_count: [13. 68.  0. 79. 55.] | prediction: [ 9.867381 60.952724  9.595085 85.266556 51.606842] | loss: 18.478418350219727\n",
      "epoch: 2 | step: 24 | gd_count: [110. 281.  90.   2.  14.] | prediction: [131.87035   89.49451   75.43848    6.989195   8.178549] | loss: 55.29696273803711\n",
      "epoch: 2 | step: 25 | gd_count: [304.  19.  78.   4. 269.] | prediction: [205.40648    11.553158   44.53467     6.1293135 152.11057  ] | loss: 56.17608642578125\n",
      "epoch: 2 | step: 26 | gd_count: [ 13.   1.  27.  45. 156.] | prediction: [ 7.7788296  5.426357  15.263159  59.868065  88.81409  ] | loss: 23.1612491607666\n",
      "epoch: 2 | step: 27 | gd_count: [  8. 101.  30. 542.  28.] | prediction: [  6.4006934  70.4299     17.897053  239.55057    18.052837 ] | loss: 74.01126098632812\n",
      "epoch: 2 | step: 28 | gd_count: [101. 314. 128.   7.  10.] | prediction: [ 52.924828 151.60796  102.636734   5.720264  11.113358] | loss: 48.315059661865234\n",
      "epoch: 2 | step: 29 | gd_count: [256.  49. 187.  34.  55.] | prediction: [134.59589   32.902943 194.4834    31.613846  60.416405] | loss: 55.293243408203125\n",
      "epoch: 2 | step: 30 | gd_count: [195.  49.  21.  54. 107.] | prediction: [288.08313   40.525276  19.3135    27.807718 124.083595] | loss: 45.86849594116211\n",
      "epoch: 2 | step: 31 | gd_count: [14. 89.  2.  8. 35.] | prediction: [11.354608  55.646698   5.3465347  5.721215  19.200047 ] | loss: 13.200248718261719\n",
      "epoch: 2 | step: 32 | gd_count: [ 31. 135.  24.  48. 152.] | prediction: [ 24.50291  115.64698   20.604015  30.856632 128.66171 ] | loss: 26.078027725219727\n",
      "epoch: 2 | step: 33 | gd_count: [413.  62.  48.  43.  54.] | prediction: [146.64716   60.281242  24.835495  49.12471   51.269253] | loss: 71.18836975097656\n",
      "epoch: 2 | step: 34 | gd_count: [ 28.  81.   0. 125.  11.] | prediction: [ 45.807533  75.813095  38.21692  114.30858    8.883326] | loss: 26.549955368041992\n",
      "epoch: 2 | step: 35 | gd_count: [123.  47.  15. 155.  99.] | prediction: [ 77.824234  29.502884  13.884836 109.0227    87.68106 ] | loss: 29.939727783203125\n",
      "epoch: 2 | step: 36 | gd_count: [40.  0. 18. 13. 36.] | prediction: [33.196503  9.21298  16.80422  13.997459 25.890442] | loss: 7.470468044281006\n",
      "epoch: 2 | step: 37 | gd_count: [42. 69. 22. 24. 63.] | prediction: [28.481722 67.639145 24.092445 21.741674 51.270187] | loss: 15.715837478637695\n",
      "epoch: 2 | step: 38 | gd_count: [ 48. 267.  41.  74.  32.] | prediction: [ 44.239    153.87782   30.353348  44.06041   29.371744] | loss: 41.0169792175293\n",
      "epoch: 2 | step: 39 | gd_count: [ 99.  78.  11. 126.  19.] | prediction: [70.273155 62.00425   9.684728 91.28883  13.040894] | loss: 20.425901412963867\n",
      "epoch: 2 | step: 40 | gd_count: [ 36.  99. 108.  89.  71.] | prediction: [28.494337 63.144825 77.72977  78.93593  48.719948] | loss: 26.891992568969727\n",
      "epoch: 2 | step: 41 | gd_count: [ 33.  14.   1. 173. 449.] | prediction: [ 38.71189    13.675194    5.6314335 100.31673   155.18831  ] | loss: 81.3706283569336\n",
      "epoch: 2 | step: 42 | gd_count: [83. 35. 20. 52. 66.] | prediction: [87.22908  30.310598 12.659015 43.146034 42.220764] | loss: 16.584230422973633\n",
      "epoch: 2 | step: 43 | gd_count: [ 34. 123.   2.  40. 134.] | prediction: [ 52.56445   191.29193     5.0116916  37.27323   104.89465  ] | loss: 31.502553939819336\n",
      "epoch: 2 | step: 44 | gd_count: [25. 74. 78. 10. 60.] | prediction: [ 24.003197 132.22835   82.854065   7.990961  67.61949 ] | loss: 25.613178253173828\n",
      "epoch: 2 | step: 45 | gd_count: [144.  43.   0. 519. 157.] | prediction: [107.59972    47.222847    5.7755737 360.5268    124.24721  ] | loss: 77.90211486816406\n",
      "epoch: 2 | step: 46 | gd_count: [18.  4. 44. 64. 60.] | prediction: [18.566784   6.3841934 38.76671   55.902702  52.54994  ] | loss: 10.452173233032227\n",
      "epoch: 2 | step: 47 | gd_count: [19.  0. 77. 51.] | prediction: [20.419827   3.8727176 56.905594  54.716713 ] | loss: 11.34145736694336\n",
      "Epoch 2 Val, MSE: 278.74 MAE: 182.72, Cost 6.8 sec\n"
     ]
    }
   ],
   "source": [
    "lr = 0.00001\n",
    "weight_decay = 0.9\n",
    "sigma = 0.1\n",
    "use_background = False\n",
    "background_ratio = 1\n",
    "\n",
    "net = vgg19()\n",
    "net.to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "post_prob = Post_Prob(sigma,\n",
    "                           crop_size,\n",
    "                           downsample_ratio,\n",
    "                           background_ratio,\n",
    "                           use_background,\n",
    "                           device)\n",
    "loss = Bay_Loss(use_background, device)\n",
    "\n",
    "bayes_trainer = Trainer_Bayes(loading_data_Bayes, net, loss, optimizer, max_epoch=3)\n",
    "bayes_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
