{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import models,transforms,datasets\n",
    "from torch.utils import data\n",
    "%matplotlib inline\n",
    "import torchvision.transforms as standard_transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from PIL import Image, ImageOps, ImageFilter\n",
    "import numbers\n",
    "import pdb\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as F2\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from glob import glob\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch import optim\n",
    "from torch.nn import Module\n",
    "import time\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code can be run both on laptop and on GCP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using gpu: False \n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Using gpu: %s ' % torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unzip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "We use 2 nn, a CSRNet and a VGG19 extented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG19 extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = ['vgg19']\n",
    "model_urls = {\n",
    "    'vgg19': 'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth',\n",
    "}\n",
    "\n",
    "class VGGExtended(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super(VGGExtended, self).__init__()\n",
    "        self.features = features\n",
    "        self.reg_layer = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = F.upsample_bilinear(x, scale_factor=2)\n",
    "        x = self.reg_layer(x)\n",
    "        return torch.abs(x)\n",
    "\n",
    "\n",
    "def make_layers(cfg, in_channels = 3,batch_norm=False,dilation = False):\n",
    "    if dilation:\n",
    "        d_rate = 2\n",
    "    else:\n",
    "        d_rate = 1\n",
    "    layers = []\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=d_rate,dilation = d_rate)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "cfg = {\n",
    "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512]\n",
    "}\n",
    "\n",
    "def vgg19():\n",
    "    \"\"\"VGG 19-layer model (configuration \"E\")\n",
    "        model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = VGGExtended(make_layers(cfg['E']))\n",
    "    model.load_state_dict(model_zoo.load_url(model_urls['vgg19']), strict=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSRNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSRNet(nn.Module):\n",
    "    def __init__(self, load_weights=False):\n",
    "        super(CSRNet, self).__init__()\n",
    "        self.seen = 0\n",
    "        self.frontend_feat = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512]\n",
    "        self.backend_feat  = [512, 512, 512,256,128,64]\n",
    "        self.frontend = make_layers(self.frontend_feat)\n",
    "        self.backend = make_layers(self.backend_feat,in_channels = 512,dilation = True)\n",
    "        self.output_layer = nn.Conv2d(64, 1, kernel_size=1)\n",
    "        if not load_weights:\n",
    "            mod = models.vgg16(pretrained = True)\n",
    "            self._initialize_weights()\n",
    "            self.frontend.load_state_dict(mod.features[0:23].state_dict())\n",
    "            \n",
    "    def forward(self,x):\n",
    "        size = x.size()\n",
    "        x = self.frontend(x)\n",
    "        x = self.backend(x)\n",
    "        x = self.output_layer(x)\n",
    "        x = F.upsample(x, size = size[2:])\n",
    "        return x\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight, std=0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_cropBayes(im_h, im_w, crop_h, crop_w):\n",
    "    res_h = im_h - crop_h\n",
    "    res_w = im_w - crop_w\n",
    "    i = random.randint(0, res_h)\n",
    "    j = random.randint(0, res_w)\n",
    "    return i, j, crop_h, crop_w\n",
    "\n",
    "\n",
    "def cal_innner_area(c_left, c_up, c_right, c_down, bbox):\n",
    "    inner_left = np.maximum(c_left, bbox[:, 0])\n",
    "    inner_up = np.maximum(c_up, bbox[:, 1])\n",
    "    inner_right = np.minimum(c_right, bbox[:, 2])\n",
    "    inner_down = np.minimum(c_down, bbox[:, 3])\n",
    "    inner_area = np.maximum(inner_right-inner_left, 0.0) * np.maximum(inner_down-inner_up, 0.0)\n",
    "    return inner_area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ground Truth dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTDataset(data.Dataset):\n",
    "    def __init__(self, data_path, mode, main_transform=None, img_transform=None, gt_transform=None):\n",
    "        self.img_path = data_path + '/img'\n",
    "        self.gt_path = data_path + '/den'\n",
    "        self.data_files = [filename for filename in os.listdir(self.img_path) \\\n",
    "                           if os.path.isfile(os.path.join(self.img_path,filename))]\n",
    "        self.num_samples = len(self.data_files) \n",
    "        self.main_transform=main_transform  \n",
    "        self.img_transform = img_transform\n",
    "        self.gt_transform = gt_transform     \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        fname = self.data_files[index]\n",
    "        img, den = self.read_image_and_gt(fname)      \n",
    "        if self.main_transform is not None:\n",
    "            img, den = self.main_transform(img,den) \n",
    "        if self.img_transform is not None:\n",
    "            img = self.img_transform(img)         \n",
    "        if self.gt_transform is not None:\n",
    "            den = self.gt_transform(den)               \n",
    "        return img, den\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def read_image_and_gt(self,fname):\n",
    "        img = Image.open(os.path.join(self.img_path,fname))\n",
    "        if img.mode == 'L':\n",
    "            img = img.convert('RGB')\n",
    "\n",
    "        den = pd.read_csv(os.path.join(self.gt_path,os.path.splitext(fname)[0] + '.csv'), sep=',',header=None).values\n",
    "        \n",
    "        den = den.astype(np.float32, copy=False)    \n",
    "        den = Image.fromarray(den)  \n",
    "        return img, den    \n",
    "\n",
    "    def get_num_samples(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayes method Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesDataset(data.Dataset):\n",
    "    def __init__(self, root_path, crop_size,\n",
    "                 downsample_ratio, is_gray=False,\n",
    "                 method='train'):\n",
    "\n",
    "        self.root_path = root_path\n",
    "        self.im_list = sorted(glob(os.path.join(self.root_path, '*.jpg')))\n",
    "        if method not in ['train', 'val']:\n",
    "            raise Exception(\"not implement\")\n",
    "        self.method = method\n",
    "\n",
    "        self.c_size = crop_size\n",
    "        self.d_ratio = downsample_ratio\n",
    "        assert self.c_size % self.d_ratio == 0\n",
    "        self.dc_size = self.c_size // self.d_ratio\n",
    "\n",
    "        if is_gray:\n",
    "            self.trans = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "            ])\n",
    "        else:\n",
    "            self.trans = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # Pour CSRNet à vérifier si besoin de modif\n",
    "            ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.im_list)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        img_path = self.im_list[item]\n",
    "        gd_path = img_path.replace('jpg', 'npy')\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        if self.method == 'train':\n",
    "            keypoints = np.load(gd_path)\n",
    "            return self.train_transform(img, keypoints)\n",
    "        elif self.method == 'val':\n",
    "            keypoints = np.load(gd_path)\n",
    "            img = self.trans(img)\n",
    "            name = os.path.basename(img_path).split('.')[0]\n",
    "            return img, len(keypoints), name\n",
    "\n",
    "    def train_transform(self, img, keypoints):\n",
    "        \"\"\"random crop image patch and find people in it\"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        Les keypoints correspondent aux coordonnées des têtes\n",
    "        MAIS une troisième coordonnée a été calculée lors du preprocessing des données,\n",
    "        elle correspont à \"dis\" et semble important pour calculer pas mal de choses\n",
    "        \"\"\"\n",
    "        \n",
    "        wd, ht = img.size\n",
    "        st_size = min(wd, ht)\n",
    "        assert st_size >= self.c_size\n",
    "        assert len(keypoints) > 0\n",
    "        i, j, h, w = random_cropBayes(ht, wd, self.c_size, self.c_size)\n",
    "        img = F2.crop(img, i, j, h, w)\n",
    "        \n",
    "        nearest_dis = np.clip(keypoints[:, 2], 4.0, 128.0)\n",
    "       \n",
    "        points_left_up = keypoints[:, :2] - nearest_dis[:, None] / 2.0\n",
    "        points_right_down = keypoints[:, :2] + nearest_dis[:, None] / 2.0\n",
    "        bbox = np.concatenate((points_left_up, points_right_down), axis=1)\n",
    "        inner_area = cal_innner_area(j, i, j+w, i+h, bbox)\n",
    "        origin_area = nearest_dis * nearest_dis\n",
    "        ratio = np.clip(1.0 * inner_area / origin_area, 0.0, 1.0)\n",
    "        mask = (ratio >= 0.3)\n",
    "\n",
    "        target = ratio[mask]\n",
    "        keypoints = keypoints[mask]\n",
    "        keypoints = keypoints[:, :2] - [j, i]  # change coodinate\n",
    "        if len(keypoints) > 0:\n",
    "            if random.random() > 0.5:\n",
    "                img = F2.hflip(img)\n",
    "                keypoints[:, 0] = w - keypoints[:, 0]\n",
    "        else:\n",
    "            if random.random() > 0.5:\n",
    "                img = F2.hflip(img)\n",
    "        return self.trans(img), torch.from_numpy(keypoints.copy()).float(), \\\n",
    "               torch.from_numpy(target.copy()).float(), st_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CSRNet\n",
    "TRAIN_SIZE = (576,768)\n",
    "LABEL_FACTOR = 1\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "def get_min_size(batch):\n",
    "\n",
    "    min_ht = TRAIN_SIZE[0]\n",
    "    min_wd = TRAIN_SIZE[1]\n",
    "\n",
    "    for i_sample in batch:\n",
    "        \n",
    "        _,ht,wd = i_sample.shape\n",
    "        if ht<min_ht:\n",
    "            min_ht = ht\n",
    "        if wd<min_wd:\n",
    "            min_wd = wd\n",
    "    return min_ht,min_wd\n",
    "\n",
    "def random_crop_GT(img,den,dst_size):\n",
    "    # dst_size: ht, wd\n",
    "\n",
    "    _,ts_hd,ts_wd = img.shape\n",
    "\n",
    "    x1 = random.randint(0, ts_wd - dst_size[1])//LABEL_FACTOR*LABEL_FACTOR\n",
    "    y1 = random.randint(0, ts_hd - dst_size[0])//LABEL_FACTOR*LABEL_FACTOR\n",
    "    x2 = x1 + dst_size[1]\n",
    "    y2 = y1 + dst_size[0]\n",
    "\n",
    "    label_x1 = x1//LABEL_FACTOR\n",
    "    label_y1 = y1//LABEL_FACTOR\n",
    "    label_x2 = x2//LABEL_FACTOR\n",
    "    label_y2 = y2//LABEL_FACTOR\n",
    "\n",
    "    return img[:,y1:y2,x1:x2], den[label_y1:label_y2,label_x1:label_x2]\n",
    "\n",
    "\n",
    "\n",
    "def share_memory(batch):\n",
    "    out = None\n",
    "    if False:\n",
    "        # If we're in a background process, concatenate directly into a\n",
    "        # shared memory tensor to avoid an extra copy\n",
    "        numel = sum([x.numel() for x in batch])\n",
    "        storage = batch[0].storage()._new_shared(numel)\n",
    "        out = batch[0].new(storage)\n",
    "    return out\n",
    "\n",
    "\n",
    "def GT_collate(batch):\n",
    "    # @GJY \n",
    "    r\"\"\"Puts each data field into a tensor with outer dimension batch size\"\"\"\n",
    "\n",
    "    transposed = list(zip(*batch)) # imgs and dens\n",
    "    imgs, dens = [transposed[0],transposed[1]]\n",
    "\n",
    "\n",
    "    error_msg = \"batch must contain tensors; found {}\"\n",
    "    if isinstance(imgs[0], torch.Tensor) and isinstance(dens[0], torch.Tensor):\n",
    "        \n",
    "        min_ht, min_wd = get_min_size(imgs)\n",
    "\n",
    "        # print min_ht, min_wd\n",
    "\n",
    "        # pdb.set_trace()\n",
    "        \n",
    "        cropped_imgs = []\n",
    "        cropped_dens = []\n",
    "        for i_sample in range(len(batch)):\n",
    "            _img, _den = random_crop_GT(imgs[i_sample],dens[i_sample],[min_ht,min_wd])\n",
    "            cropped_imgs.append(_img)\n",
    "            cropped_dens.append(_den)\n",
    "\n",
    "\n",
    "        cropped_imgs = torch.stack(cropped_imgs, 0, out=share_memory(cropped_imgs))\n",
    "        cropped_dens = torch.stack(cropped_dens, 0, out=share_memory(cropped_dens))\n",
    "\n",
    "        return [cropped_imgs,cropped_dens]\n",
    "\n",
    "    raise TypeError((error_msg.format(type(batch[0]))))\n",
    "\n",
    "\n",
    "def loading_data_GT(batch_size=5, num_workers=8):\n",
    "    mean_std = ([0.410824894905, 0.370634973049, 0.359682112932], [0.278580576181, 0.26925137639, 0.27156367898])\n",
    "    log_para = 100.\n",
    "    factor = 1\n",
    "    # DATA_PATH = \"/home/simon/Bureau/framework-crowd-counting/ProcessedData/shanghaitech_part_A\"\n",
    "    DATA_PATH = \"data/gt\"\n",
    "    \n",
    "    \n",
    "    train_main_transform = Compose([\n",
    "        RandomHorizontallyFlip()\n",
    "    ])\n",
    "    img_transform = standard_transforms.Compose([\n",
    "        standard_transforms.ToTensor(),\n",
    "        standard_transforms.Normalize(*mean_std)\n",
    "    ])\n",
    "    gt_transform = standard_transforms.Compose([\n",
    "        GTScaleDown(factor),\n",
    "        LabelNormalize(log_para)\n",
    "    ])\n",
    "\n",
    "    train_set = GTDataset(DATA_PATH+'/train', 'train',main_transform=train_main_transform, img_transform=img_transform, gt_transform=gt_transform)\n",
    "    train_loader =None\n",
    "    if batch_size == 1:\n",
    "        train_loader = DataLoader(train_set, batch_size=1, shuffle=True, drop_last=True)\n",
    "    elif batch_size > 1:\n",
    "        train_loader = DataLoader(train_set, batch_size=batch_size, num_workers=num_workers, collate_fn=GT_collate, shuffle=True, drop_last=True)\n",
    "    \n",
    "    \n",
    "    # il va falloir nous faire un val pour les données GT.\n",
    "    # val_set = GTDataset(DATA_PATH+'/val', 'val', main_transform=None, img_transform=img_transform, gt_transform=gt_transform)\n",
    "    # val_loader = DataLoader(val_set, batch_size=VAL_BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True, drop_last=False)\n",
    "    val_loader = None\n",
    "    \n",
    "    test_set = GTDataset(DATA_PATH+'/test', 'test', main_transform=None, img_transform=img_transform, gt_transform=gt_transform)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, num_workers=1, shuffle=True, drop_last=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bayes\n",
    "downsample_ratio = 8 # Mettre à 8 pour le réseau du répo (à 1 pour CSRNet puisque on ne modifie pas la dim avec le réseau)\n",
    "data_dir = \"data/bayes\"\n",
    "#data_dir = \"/home/simon/Bureau/framework-crowd-counting/processed_data_bcc/SHHA\"\n",
    "#data_dir = \"/Users/VictoRambaud/dev/crowd_counting2/ProcessedData/SHHA\"\n",
    "crop_size = 256\n",
    "is_gray = False\n",
    "\n",
    "def train_collate(batch):\n",
    "    transposed_batch = list(zip(*batch))\n",
    "    images = torch.stack(transposed_batch[0], 0)\n",
    "    points = transposed_batch[1]  # the number of points is not fixed, keep it as a list of tensor\n",
    "    targets = transposed_batch[2]\n",
    "    st_sizes = torch.FloatTensor(transposed_batch[3])\n",
    "    return images, points, targets, st_sizes\n",
    "\n",
    "\n",
    "def loading_data_Bayes(batch_size = 5, num_workers = 8):\n",
    "    datasets_bayes = {x: BayesDataset(os.path.join(data_dir, x),\n",
    "                              crop_size,\n",
    "                              downsample_ratio,\n",
    "                              is_gray, x) for x in ['train', 'val']}\n",
    "\n",
    "    dataloaders_bayes = {x: DataLoader(datasets_bayes[x],\n",
    "                                collate_fn=(train_collate if x == 'train' else default_collate),\n",
    "                                batch_size=(batch_size if x == 'train' else 1),\n",
    "                                shuffle=(True if x == 'train' else False),\n",
    "                                num_workers=num_workers,\n",
    "                                pin_memory=(True if x == 'train' else False))\n",
    "                                for x in ['train', 'val']}\n",
    "    \n",
    "    dataloaders_bayes_test = \"To do\"\n",
    "    \n",
    "    return dataloaders_bayes[\"train\"], dataloaders_bayes[\"val\"], dataloaders_bayes_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x7f4c5251ca20>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7f4c5251c630>,\n",
       " 'To do')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loading_data_Bayes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes : computing losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post_Prob(Module):\n",
    "    def __init__(self, sigma, c_size, stride, background_ratio, use_background, device):\n",
    "        super(Post_Prob, self).__init__()\n",
    "        assert c_size % stride == 0\n",
    "\n",
    "        self.sigma = sigma\n",
    "        self.bg_ratio = background_ratio\n",
    "        self.device = device\n",
    "        # coordinate is same to image space, set to constant since crop size is same\n",
    "        self.cood = torch.arange(0, c_size, step=stride,\n",
    "                                 dtype=torch.float32, device=device) + stride / 2\n",
    "        self.cood.unsqueeze_(0)\n",
    "        self.softmax = torch.nn.Softmax(dim=0)\n",
    "        self.use_bg = use_background\n",
    "\n",
    "    def forward(self, points, st_sizes):\n",
    "        num_points_per_image = [len(points_per_image) for points_per_image in points]\n",
    "        all_points = torch.cat(points, dim=0)\n",
    "\n",
    "        if len(all_points) > 0:\n",
    "            x = all_points[:, 0].unsqueeze_(1)\n",
    "            y = all_points[:, 1].unsqueeze_(1)\n",
    "            x_dis = -2 * torch.matmul(x, self.cood) + x * x + self.cood * self.cood\n",
    "            y_dis = -2 * torch.matmul(y, self.cood) + y * y + self.cood * self.cood\n",
    "            y_dis.unsqueeze_(2)\n",
    "            x_dis.unsqueeze_(1)\n",
    "            dis = y_dis + x_dis\n",
    "            dis = dis.view((dis.size(0), -1))\n",
    "\n",
    "            dis_list = torch.split(dis, num_points_per_image)\n",
    "            prob_list = []\n",
    "            for dis, st_size in zip(dis_list, st_sizes):\n",
    "                if len(dis) > 0:\n",
    "                    if self.use_bg:\n",
    "                        min_dis = torch.clamp(torch.min(dis, dim=0, keepdim=True)[0], min=0.0)\n",
    "                        d = st_size * self.bg_ratio\n",
    "                        bg_dis = (d - torch.sqrt(min_dis))**2\n",
    "                        dis = torch.cat([dis, bg_dis], 0)  # concatenate background distance to the last\n",
    "                    dis = -dis / (2.0 * self.sigma ** 2)\n",
    "                    prob = self.softmax(dis)\n",
    "                else:\n",
    "                    prob = None\n",
    "                prob_list.append(prob)\n",
    "        else:\n",
    "            prob_list = []\n",
    "            for _ in range(len(points)):\n",
    "                prob_list.append(None)\n",
    "        return prob_list\n",
    "    \n",
    "    \n",
    "class Bay_Loss(Module):\n",
    "    def __init__(self, use_background, device):\n",
    "        super(Bay_Loss, self).__init__()\n",
    "        self.device = device\n",
    "        self.use_bg = use_background\n",
    "\n",
    "    def forward(self, prob_list, target_list, pre_density):\n",
    "        loss = 0\n",
    "        \n",
    "        \"\"\"\n",
    "            - prob list semble être la listes des p(yn|xm) ie la contribution du pixel xm sur la n-ieme tête\n",
    "            (les lignes de cette matrice sont de taille 4096 = 64*64)\n",
    "            - pre density est la prédiction de la densité (sortie du réseau) - de taille 64x64 ici\n",
    "            - target list a pour longueur le nombre de têtes - correspond aux E[cn] \"réel\" (le calcul reste un mystère)\n",
    "            - On obtient les E[cn] estimées grâce à un produit terme à terme de prob_list et pre_density\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        for idx, prob in enumerate(prob_list):  # iterative through each sample\n",
    "            if prob is None:  # image contains no annotation points\n",
    "                pre_count = torch.sum(pre_density[idx])\n",
    "                target = torch.zeros((1,), dtype=torch.float32, device=self.device)\n",
    "            else:\n",
    "                N = len(prob)\n",
    "                if self.use_bg:\n",
    "                    target = torch.zeros((N,), dtype=torch.float32, device=self.device)\n",
    "                    target[:-1] = target_list[idx]\n",
    "                else:\n",
    "                    target = target_list[idx]\n",
    "                pre_count = torch.sum(pre_density[idx].view((1, -1)) * prob, dim=1)  # flatten into vector\n",
    "            \n",
    "            loss += torch.sum(torch.abs(target - pre_count))\n",
    "        loss = loss / len(prob_list)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this one should work for both architectures\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = 1.0 * self.sum / self.count\n",
    "\n",
    "    def get_avg(self):\n",
    "        return self.avg\n",
    "\n",
    "    def get_count(self):\n",
    "        return self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================img tranforms============================\n",
    "\n",
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, mask, bbx=None):\n",
    "        if bbx is None:\n",
    "            for t in self.transforms:\n",
    "                img, mask = t(img, mask)\n",
    "            return img, mask\n",
    "        for t in self.transforms:\n",
    "            img, mask, bbx = t(img, mask, bbx)\n",
    "        return img, mask, bbx\n",
    "\n",
    "class RandomHorizontallyFlip(object):\n",
    "    def __call__(self, img, mask, bbx=None):\n",
    "        if random.random() < 0.5:\n",
    "            if bbx is None:\n",
    "                return img.transpose(Image.FLIP_LEFT_RIGHT), mask.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            w, h = img.size\n",
    "            xmin = w - bbx[:,3]\n",
    "            xmax = w - bbx[:,1]\n",
    "            bbx[:,1] = xmin\n",
    "            bbx[:,3] = xmax\n",
    "            return img.transpose(Image.FLIP_LEFT_RIGHT), mask.transpose(Image.FLIP_LEFT_RIGHT), bbx\n",
    "        if bbx is None:\n",
    "            return img, mask\n",
    "        return img, mask, bbx\n",
    "\n",
    "\n",
    "\n",
    "# ===============================label tranforms============================\n",
    "\n",
    "class DeNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "        return tensor\n",
    "\n",
    "\n",
    "class LabelNormalize(object):\n",
    "    def __init__(self, para):\n",
    "        self.para = para\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        # tensor = 1./(tensor+self.para).log()\n",
    "        tensor = torch.from_numpy(np.array(tensor))\n",
    "        tensor = tensor*self.para\n",
    "        return tensor\n",
    "\n",
    "    \n",
    "class GTScaleDown(object):\n",
    "    def __init__(self, factor=8):\n",
    "        self.factor = factor\n",
    "\n",
    "    def __call__(self, img):\n",
    "        w, h = img.size\n",
    "        if self.factor==1:\n",
    "            return img\n",
    "        tmp = np.array(img.resize((w//self.factor, h//self.factor), Image.BICUBIC))*self.factor*self.factor\n",
    "        img = Image.fromarray(tmp)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer CSRNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRINT_FREQ = 1\n",
    "LOG_PARA = 100. # C'est quoi ce LOG_PARA ??\n",
    "seed = 1\n",
    "\n",
    "\n",
    "class Trainer_GT():\n",
    "    def __init__(self, dataloader, net, loss, optimizer, validation_frequency=1, max_epoch=100):\n",
    "        self.train_loader, self.val_loader, self.test_loader = dataloader()\n",
    "        self.net = net\n",
    "        self.loss = loss \n",
    "        self.optimizer = optimizer\n",
    "        self.best_mae = 1e20\n",
    "        self.best_mse = 1e20\n",
    "        self.epoch = 0\n",
    "        self.validation_frequency = validation_frequency\n",
    "        self.max_epoch = max_epoch\n",
    "\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(0, self.max_epoch):\n",
    "            self.epoch = epoch\n",
    "            # si on veut un lr sheduler il faut le mettre là\n",
    "                \n",
    "            # training    \n",
    "            self.train_epoch()\n",
    "\n",
    "            # validation\n",
    "            if epoch%self.validation_frequency==0:\n",
    "                self.validate()\n",
    "\n",
    "\n",
    "    def train_epoch(self): # training for all datasets\n",
    "        self.net.train()\n",
    "        \n",
    "        for step, data in enumerate(self.train_loader, 0):\n",
    "            img, gt_map = data\n",
    "            img = Variable(img).to(device)\n",
    "            gt_map = Variable(gt_map).to(device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            # pred_map = self.net(img, gt_map)\n",
    "            \n",
    "            pred_density_map = self.net(img)\n",
    "            loss = self.loss(pred_density_map, gt_map)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            gt_count = [int(gt_map[i].sum().data / LOG_PARA) for i in range(gt_map.size()[0])]\n",
    "            pre_count = [int(pred_density_map[i].sum().data/LOG_PARA) for i in range(pred_density_map.size()[0])]\n",
    "            \n",
    "            print(f'epoch: {self.epoch} | step: {step} | count: {gt_count} | prediction: {pre_count} | loss: {loss}') \n",
    "\n",
    "\n",
    "    def validate(self):\n",
    "\n",
    "        self.net.eval()\n",
    "        epoch_res = []\n",
    "\n",
    "        for vi, data in enumerate(self.val_loader, 0):\n",
    "            img, gt_map = data\n",
    "\n",
    "            with torch.no_grad():\n",
    "                img = Variable(img).to(device)\n",
    "                gt_map = Variable(gt_map).to(device)\n",
    "\n",
    "                pred_map = self.net.forward(img,gt_map)\n",
    "\n",
    "                pred_map = pred_map.data.cpu().numpy()\n",
    "                gt_map = gt_map.data.cpu().numpy()\n",
    "\n",
    "                for i_img in range(pred_map.shape[0]):\n",
    "                \n",
    "                    pred_cnt = np.sum(pred_map[i_img])/LOG_PARA\n",
    "                    gt_count = np.sum(gt_map[i_img])/LOG_PARA\n",
    "                    res = gt_count - pred_cnt\n",
    "                    \n",
    "                    epoch_res.append(res)\n",
    "\n",
    "\n",
    "        epoch_res = np.array(epoch_res)\n",
    "        mse = np.sqrt(np.mean(np.square(epoch_res)))\n",
    "        mae = np.mean(np.abs(epoch_res))\n",
    "        \n",
    "        print('Epoch {} Val, MSE: {:.2f} MAE: {:.2f}, Cost {:.1f} sec'\n",
    "                     .format(self.epoch, mse, mae, time.time()-epoch_start))\n",
    "\n",
    "        if (2.0 * mse + mae) < (2.0 * self.best_mse + self.best_mae):\n",
    "            self.best_mse = mse\n",
    "            self.best_mae = mae\n",
    "            print(\"save best mse {:.2f} mae {:.2f} model epoch {}\".format(self.best_mse,\n",
    "                                                                            self.best_mae,\n",
    "                                                                                 self.epoch))\n",
    "            torch.save(self.net.state_dict(), os.path.join(save_dir, 'best_model.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-8:\n",
      "Process Process-1:\n",
      "Process Process-4:\n",
      "Process Process-7:\n",
      "Process Process-5:\n",
      "Process Process-3:\n",
      "Process Process-6:\n",
      "Process Process-2:\n",
      "Exception ignored in: <function WeakValueDictionary.__init__.<locals>.remove at 0x7f4c5bbf2598>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/simon/anaconda3/lib/python3.7/weakref.py\", line 109, in remove\n",
      "    def remove(wr, selfref=ref(self), _atomic_removal=_remove_dead_weakref):\n",
      "KeyboardInterrupt\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/simon/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-16-91e3df2040b9>\", line 10, in <module>\n",
      "    gt_trainer.train()\n",
      "  File \"<ipython-input-15-517e14f8bd06>\", line 26, in train\n",
      "    self.train_epoch()\n",
      "  File \"<ipython-input-15-517e14f8bd06>\", line 44, in train_epoch\n",
      "    pred_density_map = self.net(img)\n",
      "  File \"/home/simon/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"<ipython-input-4-9655267abf97>\", line 17, in forward\n",
      "    x = self.frontend(x)\n",
      "  File \"/home/simon/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/simon/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\", line 92, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/simon/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/simon/anaconda3/lib/python3.7/site-packages/torch/nn/modules/pooling.py\", line 141, in forward\n",
      "    self.return_indices)\n",
      "  File \"/home/simon/anaconda3/lib/python3.7/site-packages/torch/_jit_internal.py\", line 138, in fn\n",
      "    return if_false(*args, **kwargs)\n",
      "  File \"/home/simon/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\", line 488, in _max_pool2d\n",
      "    input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/simon/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/simon/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/simon/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/simon/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/simon/anaconda3/lib/python3.7/inspect.py\", line 1495, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/simon/anaconda3/lib/python3.7/inspect.py\", line 1453, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/simon/anaconda3/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/simon/anaconda3/lib/python3.7/inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/home/simon/anaconda3/lib/python3.7/inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/home/simon/anaconda3/lib/python3.7/inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/home/simon/anaconda3/lib/python3.7/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Launch GT Train !\n",
    "lr = 1e-5 \n",
    "\n",
    "net = CSRNet().to(device)\n",
    "loss = nn.MSELoss().to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=1e-4)\n",
    "# optimizer = optim.SGD(self.net.parameters(), cfg.LR, momentum=0.95,weight_decay=5e-4)  \n",
    "\n",
    "gt_trainer = Trainer_GT(loading_data_GT, net, loss, optimizer)\n",
    "gt_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models parameters for Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# image = Image.open('../ProcessedData/SHHA/train/IMG_1.jpg')\n",
    "# trans1 = transforms.ToTensor()\n",
    "# img = trans1(image).to(device)\n",
    "# img = img.unsqueeze(0)\n",
    "\n",
    "# write to tensorboard\n",
    "\n",
    "# writer.add_graph(model, img)\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_epoch = 1\n",
    "val_start = 0\n",
    "save_dir = \"\"\n",
    "\n",
    "class Trainer_Bayes():\n",
    "    def __init__(self, dataloader, net, loss, optimizer,  validation_frequency=1, max_epoch=100):\n",
    "        self.train_loader, self.val_loader, self.test_loader = dataloader()\n",
    "        self.net = net\n",
    "        self.loss = loss \n",
    "        self.optimizer = optimizer\n",
    "        self.best_mae = 1e20\n",
    "        self.best_mse = 1e20\n",
    "        self.epoch = 0\n",
    "        self.validation_frequency = validation_frequency\n",
    "        self.max_epoch = max_epoch\n",
    "        \n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(0, self.max_epoch):\n",
    "            self.epoch = epoch\n",
    "\n",
    "            # training    \n",
    "            self.train_epoch()\n",
    "\n",
    "            # validation\n",
    "            if epoch % self.validation_frequency == 0:\n",
    "                self.validate()\n",
    "                \n",
    "        print(f'Train finished | best_mse: {self.best_mse} | best_mae: {self.bast_mae}')\n",
    "                \n",
    "\n",
    "    def train_epoch(self):\n",
    "        self.net.train()  # Set model to training mode\n",
    "        # running_loss = 0.0\n",
    "        # Iterate over data.\n",
    "        for step, (inputs, points, targets, st_sizes) in enumerate(self.train_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            st_sizes = st_sizes.to(device)\n",
    "            gd_count = np.array([len(p) for p in points], dtype=np.float32)\n",
    "            points = [p.to(device) for p in points]\n",
    "            targets = [t.to(device) for t in targets]\n",
    "            \n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = self.net(inputs)\n",
    "                prob_list = post_prob(points, st_sizes)\n",
    "                loss = self.loss(prob_list, targets, outputs)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                N = inputs.size(0) # batch size\n",
    "                pre_count = torch.sum(outputs.view(N, -1), dim=1).detach().cpu().numpy()\n",
    "                res = pre_count - gd_count\n",
    "\n",
    "                print(f'epoch: {self.epoch} | step: {step} | gd_count: {gd_count} | prediction: {pre_count} | loss: {loss}')\n",
    "\n",
    "                # running_loss += loss.item()\n",
    "                # if step % 2 == 1:\n",
    "                #   writer.add_scalar('training loss vgg',\n",
    "                #               running_loss / 2,\n",
    "                #               self.epoch * len(self.train_loader) + step)\n",
    "                #   running_loss = 0.0\n",
    "\n",
    "    def validate(self):\n",
    "        epoch_start = time.time()\n",
    "        self.net.eval()  # Set model to evaluate mode\n",
    "        epoch_res = []\n",
    "\n",
    "        # Iterate over data.\n",
    "        for inputs, count, name in self.val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            # inputs are images with different sizes\n",
    "            assert inputs.size(0) == 1, 'the batch size should equal to 1 in validation mode'\n",
    "            with torch.set_grad_enabled(False):\n",
    "                outputs = self.net(inputs)\n",
    "                res = count[0].item() - torch.sum(outputs).item()\n",
    "                epoch_res.append(res)\n",
    "\n",
    "\n",
    "        epoch_res = np.array(epoch_res)\n",
    "        mse = np.sqrt(np.mean(np.square(epoch_res)))\n",
    "        mae = np.mean(np.abs(epoch_res))\n",
    "\n",
    "        # ...log the running loss\n",
    "        writer.add_scalar('val MAE vgg',\n",
    "                            mae,\n",
    "                            self.epoch * len(self.val_loader))\n",
    "        writer.add_scalar('val MSE vgg',\n",
    "                        mse,\n",
    "                        self.epoch * len(self.val_loader))\n",
    "\n",
    "        print('Epoch {} Val, MSE: {:.2f} MAE: {:.2f}, Cost {:.1f} sec'\n",
    "                     .format(self.epoch, mse, mae, time.time()-epoch_start))\n",
    "\n",
    "        if (2.0 * mse + mae) < (2.0 * self.best_mse + self.best_mae):\n",
    "            self.best_mse = mse\n",
    "            self.best_mae = mae\n",
    "            print(\"save best mse {:.2f} mae {:.2f} model epoch {}\".format(self.best_mse,\n",
    "                                                                            self.best_mae,\n",
    "                                                                                 self.epoch))\n",
    "            torch.save(self.net.state_dict(), os.path.join(save_dir, 'best_model_vgg.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simon/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:2577: UserWarning: nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | step: 0 | gd_count: [ 30. 250.  60.  54. 143.] | prediction: [96.55738  94.437836 90.3692   77.07837  83.38223 ] | loss: 86.152099609375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-8a5acdb3c6da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mbayes_trainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer_Bayes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloading_data_Bayes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mbayes_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-5ff621cc1776>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m# validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-5ff621cc1776>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr = 0.00001\n",
    "weight_decay = 0.9\n",
    "sigma = 0.1\n",
    "use_background = False\n",
    "background_ratio = 1\n",
    "\n",
    "net = vgg19()\n",
    "net.to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "post_prob = Post_Prob(sigma,\n",
    "                           crop_size,\n",
    "                           downsample_ratio,\n",
    "                           background_ratio,\n",
    "                           use_background,\n",
    "                           device)\n",
    "loss = Bay_Loss(use_background, device)\n",
    "\n",
    "bayes_trainer = Trainer_Bayes(loading_data_Bayes, net, loss, optimizer, max_epoch=3)\n",
    "bayes_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
